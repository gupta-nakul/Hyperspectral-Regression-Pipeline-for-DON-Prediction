{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c839cba-be36-4c06-927c-e5c1f50d352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3e1b7ed-1c2e-4449-b597-acaa467c6fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d872b126-9588-49c8-b413-7da9b5c42ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.fcn_model import FCNModel\n",
    "from src.models.cnn_model import CNNModel\n",
    "from src.models.transformer_model import TransformerModel\n",
    "from src.models.xgboost_model import XGBoostModel\n",
    "from src.data.feature_selection import pca_feature_selection\n",
    "from src.data.dataset_loader import DatasetLoader\n",
    "from src.utils.load_config import config\n",
    "\n",
    "from src.deployment.predictor import Predictor\n",
    "from src.evaluation.evaluate import evaluate_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "968d6f0f-7508-4fa4-96b5-9705107fa360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-15 21:59:26 [INFO] src.data.dataset_loader - Loaded dataset with shape: (500, 449)\n",
      "2025-03-15 21:59:26 [DEBUG] src.data.dataset_loader - Dataset head:\n",
      "                       0         1         2         3         4         5  \\\n",
      "hsi_id                                                                       \n",
      "imagoai_corn_0  0.416181  0.396844  0.408985  0.372865  0.385293  0.365390   \n",
      "imagoai_corn_1  0.415797  0.402956  0.402564  0.396014  0.397192  0.389634   \n",
      "imagoai_corn_2  0.389023  0.371206  0.373098  0.373872  0.361056  0.349709   \n",
      "imagoai_corn_3  0.468837  0.473255  0.462949  0.459335  0.461672  0.459824   \n",
      "imagoai_corn_4  0.483352  0.487274  0.469153  0.487648  0.464026  0.451152   \n",
      "\n",
      "                       6         7         8         9  ...       439  \\\n",
      "hsi_id                                                  ...             \n",
      "imagoai_corn_0  0.355226  0.343350  0.344837  0.361567  ...  0.710280   \n",
      "imagoai_corn_1  0.375671  0.363689  0.373883  0.359674  ...  0.684011   \n",
      "imagoai_corn_2  0.333882  0.330841  0.328925  0.323854  ...  0.683054   \n",
      "imagoai_corn_3  0.458194  0.427737  0.415360  0.413106  ...  0.742782   \n",
      "imagoai_corn_4  0.458229  0.440782  0.426193  0.430482  ...  0.770227   \n",
      "\n",
      "                     440       441       442       443       444       445  \\\n",
      "hsi_id                                                                       \n",
      "imagoai_corn_0  0.717482  0.715078  0.705379  0.696691  0.692793  0.711369   \n",
      "imagoai_corn_1  0.697271  0.701995  0.696077  0.701012  0.677418  0.696921   \n",
      "imagoai_corn_2  0.669286  0.663179  0.676165  0.676591  0.655951  0.658945   \n",
      "imagoai_corn_3  0.730801  0.736787  0.730044  0.751437  0.738497  0.742446   \n",
      "imagoai_corn_4  0.773013  0.761431  0.763488  0.762473  0.744012  0.775486   \n",
      "\n",
      "                     446       447  vomitoxin_ppb  \n",
      "hsi_id                                             \n",
      "imagoai_corn_0  0.697679  0.704520         1100.0  \n",
      "imagoai_corn_1  0.696544  0.689054         1000.0  \n",
      "imagoai_corn_2  0.670989  0.665176         1300.0  \n",
      "imagoai_corn_3  0.754657  0.733474         1300.0  \n",
      "imagoai_corn_4  0.760431  0.751988          220.0  \n",
      "\n",
      "[5 rows x 449 columns]\n",
      "2025-03-15 21:59:26 [DEBUG] src.data.dataset_loader - Missing values:\n",
      "0                0\n",
      "1                0\n",
      "2                0\n",
      "3                0\n",
      "4                0\n",
      "                ..\n",
      "444              0\n",
      "445              0\n",
      "446              0\n",
      "447              0\n",
      "vomitoxin_ppb    0\n",
      "Length: 449, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "loader = DatasetLoader(config[\"data\"][\"dataset_path\"])\n",
    "X, y = loader.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cb58d0d-2580-4bc5-8d4b-1f8494c413ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: reduce dimension via PCA or other\n",
    "# X = pca_feature_selection(X, n_components=config[\"data\"][\"selected_features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "365a19b6-2eb2-431d-a6d9-07dafdef1a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=config[\"training\"][\"validation_split\"], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "255661d8-4c5b-4c23-8ea8-e79d159b0914",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b91dc875-2465-473b-bc38-cb5079cf86e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000] | Train Loss: 153725537.5000 | Val Loss: 298977832.9600\n",
      "Epoch [2/1000] | Train Loss: 153682124.9600 | Val Loss: 298857511.6800\n",
      "Epoch [3/1000] | Train Loss: 153539373.1400 | Val Loss: 298473619.2000\n",
      "Epoch [4/1000] | Train Loss: 153157178.3600 | Val Loss: 297516590.0800\n",
      "Epoch [5/1000] | Train Loss: 152169475.1250 | Val Loss: 295491339.5200\n",
      "Epoch [6/1000] | Train Loss: 150609707.7200 | Val Loss: 292313451.5200\n",
      "Epoch [7/1000] | Train Loss: 148202749.3500 | Val Loss: 288196296.9600\n",
      "Epoch [8/1000] | Train Loss: 146247063.2400 | Val Loss: 284712061.4400\n",
      "Epoch [9/1000] | Train Loss: 144358845.1200 | Val Loss: 283100513.2800\n",
      "Epoch [10/1000] | Train Loss: 144636993.9200 | Val Loss: 282522670.0800\n",
      "Epoch [11/1000] | Train Loss: 145005535.5200 | Val Loss: 282956001.2800\n",
      "Epoch [12/1000] | Train Loss: 144227787.8400 | Val Loss: 282869916.1600\n",
      "Epoch [13/1000] | Train Loss: 144636200.1600 | Val Loss: 283283255.0400\n",
      "Epoch [14/1000] | Train Loss: 144237376.4000 | Val Loss: 283416414.7200\n",
      "Epoch [15/1000] | Train Loss: 144318051.1600 | Val Loss: 283480901.1200\n",
      "Epoch [16/1000] | Train Loss: 144498993.4600 | Val Loss: 283970974.7200\n",
      "Epoch [17/1000] | Train Loss: 144881580.4800 | Val Loss: 283959493.1200\n",
      "Epoch [18/1000] | Train Loss: 145079853.8400 | Val Loss: 284299340.8000\n",
      "Epoch [19/1000] | Train Loss: 144230692.7800 | Val Loss: 283818013.4400\n",
      "Epoch [20/1000] | Train Loss: 144816173.8600 | Val Loss: 283498438.4000\n",
      "Epoch [21/1000] | Train Loss: 144408388.4800 | Val Loss: 283508823.0400\n",
      "Epoch [22/1000] | Train Loss: 144232828.6400 | Val Loss: 283678883.8400\n",
      "Epoch [23/1000] | Train Loss: 144772578.2400 | Val Loss: 283168842.2400\n",
      "Epoch [24/1000] | Train Loss: 144520046.2400 | Val Loss: 282879687.6800\n",
      "Epoch [25/1000] | Train Loss: 144772770.9200 | Val Loss: 282637423.3600\n",
      "Epoch [26/1000] | Train Loss: 144662507.5200 | Val Loss: 282857781.7600\n",
      "Epoch [27/1000] | Train Loss: 144145242.1000 | Val Loss: 282829561.6000\n",
      "Epoch [28/1000] | Train Loss: 144417642.2400 | Val Loss: 283212138.2400\n",
      "Epoch [29/1000] | Train Loss: 144122946.2400 | Val Loss: 283516752.6400\n",
      "Epoch [30/1000] | Train Loss: 143765656.6000 | Val Loss: 282497703.6800\n",
      "Epoch [31/1000] | Train Loss: 144883491.2000 | Val Loss: 282355573.7600\n",
      "Epoch [32/1000] | Train Loss: 145231189.9200 | Val Loss: 282266403.8400\n",
      "Epoch [33/1000] | Train Loss: 145128794.4800 | Val Loss: 282598877.4400\n",
      "Epoch [34/1000] | Train Loss: 144436038.8000 | Val Loss: 283200256.0000\n",
      "Epoch [35/1000] | Train Loss: 144452608.7200 | Val Loss: 283668710.4000\n",
      "Epoch [36/1000] | Train Loss: 144316422.8800 | Val Loss: 283752757.7600\n",
      "Epoch [37/1000] | Train Loss: 144316354.6000 | Val Loss: 283734387.2000\n",
      "Epoch [38/1000] | Train Loss: 144706254.0000 | Val Loss: 283547407.3600\n",
      "Epoch [39/1000] | Train Loss: 144806559.3200 | Val Loss: 283479975.6800\n",
      "Epoch [40/1000] | Train Loss: 144262014.0000 | Val Loss: 283374612.4800\n",
      "Epoch [41/1000] | Train Loss: 144455136.6400 | Val Loss: 283304442.8800\n",
      "Epoch [42/1000] | Train Loss: 144365838.2400 | Val Loss: 282781381.1200\n",
      "Epoch [43/1000] | Train Loss: 144838502.7200 | Val Loss: 282520295.6800\n",
      "Epoch [44/1000] | Train Loss: 145007247.8400 | Val Loss: 282338812.1600\n",
      "Epoch [45/1000] | Train Loss: 144526936.0000 | Val Loss: 282249939.2000\n",
      "Epoch [46/1000] | Train Loss: 144751606.4000 | Val Loss: 282370229.7600\n",
      "Epoch [47/1000] | Train Loss: 144695731.4400 | Val Loss: 282753603.8400\n",
      "Epoch [48/1000] | Train Loss: 144652752.6600 | Val Loss: 283447333.1200\n",
      "Epoch [49/1000] | Train Loss: 144297319.8400 | Val Loss: 283664371.2000\n",
      "Epoch [50/1000] | Train Loss: 144369844.2400 | Val Loss: 283053657.6000\n",
      "Epoch [51/1000] | Train Loss: 144675569.2800 | Val Loss: 282751729.9200\n",
      "Epoch [52/1000] | Train Loss: 145198161.2800 | Val Loss: 282980782.0800\n",
      "Epoch [53/1000] | Train Loss: 145027674.7200 | Val Loss: 282815809.2800\n",
      "Epoch [54/1000] | Train Loss: 144255720.8400 | Val Loss: 282687114.2400\n",
      "Epoch [55/1000] | Train Loss: 144357528.1600 | Val Loss: 282691480.3200\n",
      "Epoch [56/1000] | Train Loss: 144016203.9000 | Val Loss: 282817681.9200\n",
      "Epoch [57/1000] | Train Loss: 144356521.4400 | Val Loss: 283023171.8400\n",
      "Epoch [58/1000] | Train Loss: 144412273.9800 | Val Loss: 282837386.2400\n",
      "Epoch [59/1000] | Train Loss: 144227500.3200 | Val Loss: 282867760.6400\n",
      "Epoch [60/1000] | Train Loss: 143920068.4000 | Val Loss: 282763139.8400\n",
      "Epoch [61/1000] | Train Loss: 144145226.2400 | Val Loss: 282776972.8000\n",
      "Epoch [62/1000] | Train Loss: 144493695.8400 | Val Loss: 282187136.0000\n",
      "Epoch [63/1000] | Train Loss: 144483629.7600 | Val Loss: 281806225.9200\n",
      "Epoch [64/1000] | Train Loss: 144796279.0000 | Val Loss: 282177468.1600\n",
      "Epoch [65/1000] | Train Loss: 144107644.6400 | Val Loss: 282734782.7200\n",
      "Epoch [66/1000] | Train Loss: 143924339.8200 | Val Loss: 283023461.1200\n",
      "Epoch [67/1000] | Train Loss: 144140562.4600 | Val Loss: 283234176.0000\n",
      "Epoch [68/1000] | Train Loss: 144019638.9400 | Val Loss: 283429872.6400\n",
      "Epoch [69/1000] | Train Loss: 144081049.6600 | Val Loss: 283569927.6800\n",
      "Epoch [70/1000] | Train Loss: 144773341.6800 | Val Loss: 283550438.4000\n",
      "Epoch [71/1000] | Train Loss: 144677555.4000 | Val Loss: 283574256.6400\n",
      "Epoch [72/1000] | Train Loss: 144199744.4000 | Val Loss: 283318280.9600\n",
      "Epoch [73/1000] | Train Loss: 144166198.4000 | Val Loss: 282596925.4400\n",
      "Epoch [74/1000] | Train Loss: 144189043.7600 | Val Loss: 282322968.3200\n",
      "Epoch [75/1000] | Train Loss: 144016368.8000 | Val Loss: 282510389.7600\n",
      "Epoch [76/1000] | Train Loss: 143761345.2000 | Val Loss: 282619399.6800\n",
      "Epoch [77/1000] | Train Loss: 144407011.4400 | Val Loss: 282777255.6800\n",
      "Epoch [78/1000] | Train Loss: 144283279.7400 | Val Loss: 282370739.2000\n",
      "Epoch [79/1000] | Train Loss: 144427319.5800 | Val Loss: 282487875.8400\n",
      "Epoch [80/1000] | Train Loss: 144340626.4000 | Val Loss: 282914754.5600\n",
      "Epoch [81/1000] | Train Loss: 143911219.1000 | Val Loss: 283087998.7200\n",
      "Epoch [82/1000] | Train Loss: 144320124.8400 | Val Loss: 283127658.2400\n",
      "Epoch [83/1000] | Train Loss: 144456407.3600 | Val Loss: 283151219.2000\n",
      "Epoch [84/1000] | Train Loss: 143840346.5600 | Val Loss: 282924537.6000\n",
      "Epoch [85/1000] | Train Loss: 143777160.4400 | Val Loss: 282137337.6000\n",
      "Epoch [86/1000] | Train Loss: 144152716.4400 | Val Loss: 281983786.2400\n",
      "Epoch [87/1000] | Train Loss: 144346371.0400 | Val Loss: 282221829.1200\n",
      "Epoch [88/1000] | Train Loss: 144161185.1000 | Val Loss: 282494760.9600\n",
      "Epoch [89/1000] | Train Loss: 144280026.2400 | Val Loss: 282684217.6000\n",
      "Epoch [90/1000] | Train Loss: 144196192.9200 | Val Loss: 282492792.3200\n",
      "Epoch [91/1000] | Train Loss: 144203480.7600 | Val Loss: 282429109.7600\n",
      "Epoch [92/1000] | Train Loss: 144360605.7600 | Val Loss: 282665544.9600\n",
      "Epoch [93/1000] | Train Loss: 144051241.0800 | Val Loss: 282184920.3200\n",
      "Epoch [94/1000] | Train Loss: 144754346.5200 | Val Loss: 282127756.8000\n",
      "Epoch [95/1000] | Train Loss: 143866797.7600 | Val Loss: 282225996.8000\n",
      "Epoch [96/1000] | Train Loss: 143862212.9600 | Val Loss: 282554223.3600\n",
      "Epoch [97/1000] | Train Loss: 143609352.8000 | Val Loss: 282732921.6000\n",
      "Epoch [98/1000] | Train Loss: 144301537.5200 | Val Loss: 282407147.5200\n",
      "Epoch [99/1000] | Train Loss: 143947896.3800 | Val Loss: 282427480.3200\n",
      "Epoch [100/1000] | Train Loss: 143784329.9200 | Val Loss: 282499942.4000\n",
      "Epoch [101/1000] | Train Loss: 143775382.7200 | Val Loss: 282542010.8800\n",
      "Epoch [102/1000] | Train Loss: 143977621.6000 | Val Loss: 282395348.4800\n",
      "Epoch [103/1000] | Train Loss: 144182536.6400 | Val Loss: 282264254.7200\n",
      "Epoch [104/1000] | Train Loss: 144165578.2000 | Val Loss: 282363816.9600\n",
      "Epoch [105/1000] | Train Loss: 143504983.3800 | Val Loss: 282327546.8800\n",
      "Epoch [106/1000] | Train Loss: 143906731.6000 | Val Loss: 282228014.0800\n",
      "Epoch [107/1000] | Train Loss: 143742092.7400 | Val Loss: 282169163.5200\n",
      "Epoch [108/1000] | Train Loss: 143936148.4800 | Val Loss: 282281779.2000\n",
      "Epoch [109/1000] | Train Loss: 144028924.0000 | Val Loss: 281681701.1200\n",
      "Epoch [110/1000] | Train Loss: 144001045.9200 | Val Loss: 281523690.2400\n",
      "Epoch [111/1000] | Train Loss: 144023071.0400 | Val Loss: 281750492.1600\n",
      "Epoch [112/1000] | Train Loss: 143825940.1200 | Val Loss: 282278694.4000\n",
      "Epoch [113/1000] | Train Loss: 143869254.5600 | Val Loss: 282301258.2400\n",
      "Epoch [114/1000] | Train Loss: 144450672.0000 | Val Loss: 281902534.4000\n",
      "Epoch [115/1000] | Train Loss: 143655648.2400 | Val Loss: 281533600.0000\n",
      "Epoch [116/1000] | Train Loss: 143789379.3200 | Val Loss: 281093361.9200\n",
      "Epoch [117/1000] | Train Loss: 143560604.0000 | Val Loss: 281138348.8000\n",
      "Epoch [118/1000] | Train Loss: 143876019.8400 | Val Loss: 281170920.9600\n",
      "Epoch [119/1000] | Train Loss: 143830578.5600 | Val Loss: 280858401.2800\n",
      "Epoch [120/1000] | Train Loss: 143608043.9800 | Val Loss: 281059219.2000\n",
      "Epoch [121/1000] | Train Loss: 143568549.4800 | Val Loss: 281618324.4800\n",
      "Epoch [122/1000] | Train Loss: 143940595.2200 | Val Loss: 282296741.1200\n",
      "Epoch [123/1000] | Train Loss: 143791405.6000 | Val Loss: 282594350.0800\n",
      "Epoch [124/1000] | Train Loss: 143949940.5000 | Val Loss: 281535084.8000\n",
      "Epoch [125/1000] | Train Loss: 143551553.7600 | Val Loss: 281186946.5600\n",
      "Epoch [126/1000] | Train Loss: 143988808.0800 | Val Loss: 281488083.2000\n",
      "Epoch [127/1000] | Train Loss: 143752729.7600 | Val Loss: 281652040.9600\n",
      "Epoch [128/1000] | Train Loss: 143448242.1600 | Val Loss: 281977182.7200\n",
      "Epoch [129/1000] | Train Loss: 143773079.1200 | Val Loss: 282130645.7600\n",
      "Epoch [130/1000] | Train Loss: 143435365.1200 | Val Loss: 281060727.0400\n",
      "Epoch [131/1000] | Train Loss: 142984162.7200 | Val Loss: 280844715.5200\n",
      "Epoch [132/1000] | Train Loss: 142928677.8000 | Val Loss: 280684467.2000\n",
      "Epoch [133/1000] | Train Loss: 143253085.8800 | Val Loss: 280947143.6800\n",
      "Epoch [134/1000] | Train Loss: 143165938.0200 | Val Loss: 281160001.2800\n",
      "Epoch [135/1000] | Train Loss: 143194013.8800 | Val Loss: 281500305.9200\n",
      "Epoch [136/1000] | Train Loss: 143721178.1400 | Val Loss: 281780424.9600\n",
      "Epoch [137/1000] | Train Loss: 143028976.4800 | Val Loss: 281598603.5200\n",
      "Epoch [138/1000] | Train Loss: 143289882.9600 | Val Loss: 281299013.1200\n",
      "Epoch [139/1000] | Train Loss: 143216011.1200 | Val Loss: 281218693.1200\n",
      "Epoch [140/1000] | Train Loss: 143116296.9600 | Val Loss: 280992733.4400\n",
      "Epoch [141/1000] | Train Loss: 143348310.3200 | Val Loss: 280276725.7600\n",
      "Epoch [142/1000] | Train Loss: 142827231.1200 | Val Loss: 280307333.1200\n",
      "Epoch [143/1000] | Train Loss: 143373218.0800 | Val Loss: 280672330.2400\n",
      "Epoch [144/1000] | Train Loss: 142741294.7200 | Val Loss: 281037237.7600\n",
      "Epoch [145/1000] | Train Loss: 142960699.6400 | Val Loss: 281146823.6800\n",
      "Epoch [146/1000] | Train Loss: 143320659.6200 | Val Loss: 280930195.2000\n",
      "Epoch [147/1000] | Train Loss: 143231121.1200 | Val Loss: 280794465.2800\n",
      "Epoch [148/1000] | Train Loss: 143057151.4400 | Val Loss: 280635063.0400\n",
      "Epoch [149/1000] | Train Loss: 143281966.1600 | Val Loss: 280378023.6800\n",
      "Epoch [150/1000] | Train Loss: 142956306.8000 | Val Loss: 280129987.8400\n",
      "Epoch [151/1000] | Train Loss: 143520734.6000 | Val Loss: 280265592.3200\n",
      "Epoch [152/1000] | Train Loss: 143046008.8800 | Val Loss: 280461239.0400\n",
      "Epoch [153/1000] | Train Loss: 142856085.8000 | Val Loss: 280314321.9200\n",
      "Epoch [154/1000] | Train Loss: 142652898.7400 | Val Loss: 280680545.2800\n",
      "Epoch [155/1000] | Train Loss: 142901155.4400 | Val Loss: 280282581.7600\n",
      "Epoch [156/1000] | Train Loss: 142227687.1400 | Val Loss: 280349800.9600\n",
      "Epoch [157/1000] | Train Loss: 142335224.3200 | Val Loss: 279714131.2000\n",
      "Epoch [158/1000] | Train Loss: 143262024.2400 | Val Loss: 278569954.5600\n",
      "Epoch [159/1000] | Train Loss: 142165820.3600 | Val Loss: 278846027.5200\n",
      "Epoch [160/1000] | Train Loss: 142033778.4400 | Val Loss: 278971909.1200\n",
      "Epoch [161/1000] | Train Loss: 141968373.7600 | Val Loss: 279453676.8000\n",
      "Epoch [162/1000] | Train Loss: 142529776.3200 | Val Loss: 280055701.7600\n",
      "Epoch [163/1000] | Train Loss: 142449654.6400 | Val Loss: 279198282.2400\n",
      "Epoch [164/1000] | Train Loss: 142445622.2400 | Val Loss: 279129159.6800\n",
      "Epoch [165/1000] | Train Loss: 142128490.4000 | Val Loss: 279044545.2800\n",
      "Epoch [166/1000] | Train Loss: 142209113.6400 | Val Loss: 278463075.8400\n",
      "Epoch [167/1000] | Train Loss: 141986814.4000 | Val Loss: 278498346.2400\n",
      "Epoch [168/1000] | Train Loss: 142199570.4800 | Val Loss: 278342292.4800\n",
      "Epoch [169/1000] | Train Loss: 140665430.9200 | Val Loss: 277836935.6800\n",
      "Epoch [170/1000] | Train Loss: 141317702.6200 | Val Loss: 277293913.6000\n",
      "Epoch [171/1000] | Train Loss: 142348238.8800 | Val Loss: 277519633.9200\n",
      "Epoch [172/1000] | Train Loss: 141447712.6400 | Val Loss: 277042865.9200\n",
      "Epoch [173/1000] | Train Loss: 141077333.7600 | Val Loss: 275847275.5200\n",
      "Epoch [174/1000] | Train Loss: 141164482.0800 | Val Loss: 275625565.4400\n",
      "Epoch [175/1000] | Train Loss: 140481762.7200 | Val Loss: 275871845.1200\n",
      "Epoch [176/1000] | Train Loss: 140190167.1200 | Val Loss: 275853523.2000\n",
      "Epoch [177/1000] | Train Loss: 140500113.3200 | Val Loss: 275512113.9200\n",
      "Epoch [178/1000] | Train Loss: 141132618.7200 | Val Loss: 274893030.4000\n",
      "Epoch [179/1000] | Train Loss: 140120749.3600 | Val Loss: 274814538.2400\n",
      "Epoch [180/1000] | Train Loss: 140446977.4800 | Val Loss: 274973967.3600\n",
      "Epoch [181/1000] | Train Loss: 138765720.1600 | Val Loss: 274361361.9200\n",
      "Epoch [182/1000] | Train Loss: 139802529.1200 | Val Loss: 272078216.9600\n",
      "Epoch [183/1000] | Train Loss: 139932777.6000 | Val Loss: 272261523.2000\n",
      "Epoch [184/1000] | Train Loss: 138900557.4400 | Val Loss: 270716929.2800\n",
      "Epoch [185/1000] | Train Loss: 139969387.4400 | Val Loss: 270174417.9200\n",
      "Epoch [186/1000] | Train Loss: 138179570.5600 | Val Loss: 270317817.6000\n",
      "Epoch [187/1000] | Train Loss: 138084683.9200 | Val Loss: 270158103.0400\n",
      "Epoch [188/1000] | Train Loss: 137333264.1800 | Val Loss: 268818816.0000\n",
      "Epoch [189/1000] | Train Loss: 136565781.8000 | Val Loss: 268507586.5600\n",
      "Epoch [190/1000] | Train Loss: 135611716.6400 | Val Loss: 266649943.0400\n",
      "Epoch [191/1000] | Train Loss: 135904290.3200 | Val Loss: 265167631.3600\n",
      "Epoch [192/1000] | Train Loss: 135463208.1400 | Val Loss: 266218069.7600\n",
      "Epoch [193/1000] | Train Loss: 135603439.8800 | Val Loss: 263760499.2000\n",
      "Epoch [194/1000] | Train Loss: 134261475.2000 | Val Loss: 262478689.2800\n",
      "Epoch [195/1000] | Train Loss: 133614993.4000 | Val Loss: 263015476.4800\n",
      "Epoch [196/1000] | Train Loss: 133165822.2400 | Val Loss: 259989507.8400\n",
      "Epoch [197/1000] | Train Loss: 131327698.8400 | Val Loss: 257731374.0800\n",
      "Epoch [198/1000] | Train Loss: 130485371.1600 | Val Loss: 258094297.6000\n",
      "Epoch [199/1000] | Train Loss: 130381790.9800 | Val Loss: 254121571.8400\n",
      "Epoch [200/1000] | Train Loss: 130888866.8000 | Val Loss: 253767863.0400\n",
      "Epoch [201/1000] | Train Loss: 129011000.7400 | Val Loss: 250694195.2000\n",
      "Epoch [202/1000] | Train Loss: 128194848.0000 | Val Loss: 248536529.9200\n",
      "Epoch [203/1000] | Train Loss: 126829577.5000 | Val Loss: 246933103.3600\n",
      "Epoch [204/1000] | Train Loss: 125842881.3600 | Val Loss: 243298780.1600\n",
      "Epoch [205/1000] | Train Loss: 123333222.3200 | Val Loss: 243099312.6400\n",
      "Epoch [206/1000] | Train Loss: 124617446.0800 | Val Loss: 238195406.0800\n",
      "Epoch [207/1000] | Train Loss: 120588863.6000 | Val Loss: 235473492.4800\n",
      "Epoch [208/1000] | Train Loss: 122703915.0400 | Val Loss: 231864782.0800\n",
      "Epoch [209/1000] | Train Loss: 120706867.5200 | Val Loss: 233775741.4400\n",
      "Epoch [210/1000] | Train Loss: 120910541.0400 | Val Loss: 225805442.5600\n",
      "Epoch [211/1000] | Train Loss: 119141139.5200 | Val Loss: 224648052.4800\n",
      "Epoch [212/1000] | Train Loss: 113032714.7600 | Val Loss: 217274600.9600\n",
      "Epoch [213/1000] | Train Loss: 113035678.8800 | Val Loss: 212585569.2800\n",
      "Epoch [214/1000] | Train Loss: 108519139.8000 | Val Loss: 207936862.7200\n",
      "Epoch [215/1000] | Train Loss: 110019321.7600 | Val Loss: 204080759.0400\n",
      "Epoch [216/1000] | Train Loss: 111454069.2800 | Val Loss: 213507197.4400\n",
      "Epoch [217/1000] | Train Loss: 113283079.6000 | Val Loss: 198615270.4000\n",
      "Epoch [218/1000] | Train Loss: 113311675.8750 | Val Loss: 205253844.4800\n",
      "Epoch [219/1000] | Train Loss: 110645516.4800 | Val Loss: 207130206.7200\n",
      "Epoch [220/1000] | Train Loss: 106834726.0800 | Val Loss: 182313461.7600\n",
      "Epoch [221/1000] | Train Loss: 95675673.7600 | Val Loss: 186906383.3600\n",
      "Epoch [222/1000] | Train Loss: 99043736.8800 | Val Loss: 179003537.9200\n",
      "Epoch [223/1000] | Train Loss: 95650027.7000 | Val Loss: 167603856.0000\n",
      "Epoch [224/1000] | Train Loss: 95209227.1200 | Val Loss: 164883132.1600\n",
      "Epoch [225/1000] | Train Loss: 88046241.2000 | Val Loss: 161034069.7600\n",
      "Epoch [226/1000] | Train Loss: 91422818.7200 | Val Loss: 156006044.1600\n",
      "Epoch [227/1000] | Train Loss: 88738549.0000 | Val Loss: 166428427.5200\n",
      "Epoch [228/1000] | Train Loss: 86910588.8000 | Val Loss: 151633262.0800\n",
      "Epoch [229/1000] | Train Loss: 95347865.8800 | Val Loss: 169577477.1200\n",
      "Epoch [230/1000] | Train Loss: 88767638.5200 | Val Loss: 152219125.7600\n",
      "Epoch [231/1000] | Train Loss: 89591885.4050 | Val Loss: 142061808.0000\n",
      "Epoch [232/1000] | Train Loss: 87773765.2800 | Val Loss: 141653109.1200\n",
      "Epoch [233/1000] | Train Loss: 76772957.6000 | Val Loss: 134742004.4800\n",
      "Epoch [234/1000] | Train Loss: 90877393.9200 | Val Loss: 146738822.4000\n",
      "Epoch [235/1000] | Train Loss: 86820014.5600 | Val Loss: 129645127.6800\n",
      "Epoch [236/1000] | Train Loss: 81992962.6000 | Val Loss: 136892111.3600\n",
      "Epoch [237/1000] | Train Loss: 83631829.6800 | Val Loss: 119808241.9200\n",
      "Epoch [238/1000] | Train Loss: 75329743.2000 | Val Loss: 117867471.3600\n",
      "Epoch [239/1000] | Train Loss: 80119237.8600 | Val Loss: 135706094.0800\n",
      "Epoch [240/1000] | Train Loss: 98401315.0400 | Val Loss: 151336392.9600\n",
      "Epoch [241/1000] | Train Loss: 78564183.7600 | Val Loss: 113019261.4400\n",
      "Epoch [242/1000] | Train Loss: 87104545.7800 | Val Loss: 148411930.2400\n",
      "Epoch [243/1000] | Train Loss: 83054209.2400 | Val Loss: 123392100.4800\n",
      "Epoch [244/1000] | Train Loss: 74954506.2400 | Val Loss: 121208000.6400\n",
      "Epoch [245/1000] | Train Loss: 76745436.8000 | Val Loss: 106908474.8800\n",
      "Epoch [246/1000] | Train Loss: 74750633.2800 | Val Loss: 112469464.9600\n",
      "Epoch [247/1000] | Train Loss: 74279404.2400 | Val Loss: 106111398.4000\n",
      "Epoch [248/1000] | Train Loss: 75275778.5600 | Val Loss: 104510574.7200\n",
      "Epoch [249/1000] | Train Loss: 87084282.0000 | Val Loss: 117340215.6800\n",
      "Epoch [250/1000] | Train Loss: 81119047.6800 | Val Loss: 110062068.4800\n",
      "Epoch [251/1000] | Train Loss: 78614560.6000 | Val Loss: 105292642.5600\n",
      "Epoch [252/1000] | Train Loss: 74973798.8500 | Val Loss: 103715239.6800\n",
      "Epoch [253/1000] | Train Loss: 71792790.5800 | Val Loss: 91349559.3600\n",
      "Epoch [254/1000] | Train Loss: 75238198.4000 | Val Loss: 92132393.2800\n",
      "Epoch [255/1000] | Train Loss: 67986828.2000 | Val Loss: 96424406.7200\n",
      "Epoch [256/1000] | Train Loss: 88330837.0000 | Val Loss: 106909306.2400\n",
      "Epoch [257/1000] | Train Loss: 68400698.5600 | Val Loss: 96957333.4400\n",
      "Epoch [258/1000] | Train Loss: 80426513.2800 | Val Loss: 130816402.5600\n",
      "Epoch [259/1000] | Train Loss: 85715642.8400 | Val Loss: 93767049.6000\n",
      "Epoch [260/1000] | Train Loss: 73043865.3563 | Val Loss: 95569888.9600\n",
      "Epoch [261/1000] | Train Loss: 77727833.2800 | Val Loss: 92595329.2800\n",
      "Epoch [262/1000] | Train Loss: 75409751.2800 | Val Loss: 89589035.2000\n",
      "Epoch [263/1000] | Train Loss: 78393342.3200 | Val Loss: 102277866.8800\n",
      "Epoch [264/1000] | Train Loss: 77554253.6800 | Val Loss: 85388202.5600\n",
      "Epoch [265/1000] | Train Loss: 69318214.6700 | Val Loss: 100492960.0000\n",
      "Epoch [266/1000] | Train Loss: 71568930.8800 | Val Loss: 87485285.4400\n",
      "Epoch [267/1000] | Train Loss: 73999532.3200 | Val Loss: 111214369.9200\n",
      "Epoch [268/1000] | Train Loss: 71256605.4400 | Val Loss: 91090290.2400\n",
      "Epoch [269/1000] | Train Loss: 77624233.5100 | Val Loss: 94576776.9600\n",
      "Epoch [270/1000] | Train Loss: 78408521.7600 | Val Loss: 90897496.6400\n",
      "Epoch [271/1000] | Train Loss: 65645841.0000 | Val Loss: 81539085.7600\n",
      "Epoch [272/1000] | Train Loss: 74311877.9500 | Val Loss: 86182325.1200\n",
      "Epoch [273/1000] | Train Loss: 73210803.2800 | Val Loss: 89193527.0400\n",
      "Epoch [274/1000] | Train Loss: 71071250.4800 | Val Loss: 92830011.8400\n",
      "Epoch [275/1000] | Train Loss: 78041951.8400 | Val Loss: 93678677.7600\n",
      "Epoch [276/1000] | Train Loss: 67591970.2400 | Val Loss: 91061488.3200\n",
      "Epoch [277/1000] | Train Loss: 70360177.4300 | Val Loss: 80222444.4800\n",
      "Epoch [278/1000] | Train Loss: 71285740.4800 | Val Loss: 85198568.0000\n",
      "Epoch [279/1000] | Train Loss: 74281390.5600 | Val Loss: 105447465.6000\n",
      "Epoch [280/1000] | Train Loss: 82123004.9600 | Val Loss: 119065472.6400\n",
      "Epoch [281/1000] | Train Loss: 75260210.3200 | Val Loss: 109703372.8000\n",
      "Epoch [282/1000] | Train Loss: 75305704.4800 | Val Loss: 101349836.4800\n",
      "Epoch [283/1000] | Train Loss: 78391482.8900 | Val Loss: 112992150.4000\n",
      "Epoch [284/1000] | Train Loss: 79575032.2400 | Val Loss: 82691012.4800\n",
      "Epoch [285/1000] | Train Loss: 73069551.5200 | Val Loss: 87686039.0400\n",
      "Epoch [286/1000] | Train Loss: 73013284.6600 | Val Loss: 110328585.6000\n",
      "Epoch [287/1000] | Train Loss: 71675432.2400 | Val Loss: 86328664.3200\n",
      "Epoch [288/1000] | Train Loss: 66230166.7200 | Val Loss: 78579196.1600\n",
      "Epoch [289/1000] | Train Loss: 76365239.7300 | Val Loss: 76539880.6400\n",
      "Epoch [290/1000] | Train Loss: 75606854.0750 | Val Loss: 80221597.1200\n",
      "Epoch [291/1000] | Train Loss: 72216251.8600 | Val Loss: 77511378.8800\n",
      "Epoch [292/1000] | Train Loss: 65603438.8000 | Val Loss: 81727670.0800\n",
      "Epoch [293/1000] | Train Loss: 75717315.9300 | Val Loss: 87647944.0000\n",
      "Epoch [294/1000] | Train Loss: 76828229.6800 | Val Loss: 86401280.0000\n",
      "Epoch [295/1000] | Train Loss: 71373317.5900 | Val Loss: 107111973.7600\n",
      "Epoch [296/1000] | Train Loss: 71908979.3600 | Val Loss: 92693603.5200\n",
      "Epoch [297/1000] | Train Loss: 70218570.0800 | Val Loss: 91197377.6000\n",
      "Epoch [298/1000] | Train Loss: 82469322.2400 | Val Loss: 101907830.0800\n",
      "Epoch [299/1000] | Train Loss: 71371563.7600 | Val Loss: 81701050.8800\n",
      "Epoch [300/1000] | Train Loss: 76179934.2400 | Val Loss: 103670434.5600\n",
      "Epoch [301/1000] | Train Loss: 76627903.0100 | Val Loss: 101032273.9200\n",
      "Epoch [302/1000] | Train Loss: 74125970.5200 | Val Loss: 80423338.5600\n",
      "Epoch [303/1000] | Train Loss: 63206269.1600 | Val Loss: 80301703.3600\n",
      "Epoch [304/1000] | Train Loss: 70514091.1300 | Val Loss: 78130096.3200\n",
      "Epoch [305/1000] | Train Loss: 74919965.8400 | Val Loss: 78144722.8800\n",
      "Epoch [306/1000] | Train Loss: 75252857.8675 | Val Loss: 95582532.8000\n",
      "Epoch [307/1000] | Train Loss: 70135069.4800 | Val Loss: 82550223.0400\n",
      "Epoch [308/1000] | Train Loss: 71508804.4000 | Val Loss: 87827052.4800\n",
      "Epoch [309/1000] | Train Loss: 81623043.4400 | Val Loss: 156633219.2000\n",
      "Epoch [310/1000] | Train Loss: 81182532.4800 | Val Loss: 113644617.6000\n",
      "Epoch [311/1000] | Train Loss: 74938092.8400 | Val Loss: 79807262.4000\n",
      "Epoch [312/1000] | Train Loss: 75624562.4800 | Val Loss: 84238379.2000\n",
      "Epoch [313/1000] | Train Loss: 62677623.3000 | Val Loss: 72973618.8800\n",
      "Epoch [314/1000] | Train Loss: 67378828.2600 | Val Loss: 77659982.0800\n",
      "Epoch [315/1000] | Train Loss: 73764367.6400 | Val Loss: 83876599.0400\n",
      "Epoch [316/1000] | Train Loss: 73544718.9200 | Val Loss: 76382698.2400\n",
      "Epoch [317/1000] | Train Loss: 73929633.7000 | Val Loss: 102058143.0400\n",
      "Epoch [318/1000] | Train Loss: 75025495.1550 | Val Loss: 87700154.5600\n",
      "Epoch [319/1000] | Train Loss: 72142675.6900 | Val Loss: 94991111.3600\n",
      "Epoch [320/1000] | Train Loss: 70724213.3200 | Val Loss: 79456026.8800\n",
      "Epoch [321/1000] | Train Loss: 83961712.6400 | Val Loss: 76133557.1200\n",
      "Epoch [322/1000] | Train Loss: 81730546.2400 | Val Loss: 107697832.0000\n",
      "Epoch [323/1000] | Train Loss: 72606208.4800 | Val Loss: 68744225.2800\n",
      "Epoch [324/1000] | Train Loss: 64423344.5000 | Val Loss: 67629783.6800\n",
      "Epoch [325/1000] | Train Loss: 75211339.0400 | Val Loss: 86636217.2800\n",
      "Epoch [326/1000] | Train Loss: 66493983.8200 | Val Loss: 82287582.0800\n",
      "Epoch [327/1000] | Train Loss: 70857294.3200 | Val Loss: 85146013.4400\n",
      "Epoch [328/1000] | Train Loss: 71020301.1200 | Val Loss: 81820616.9600\n",
      "Epoch [329/1000] | Train Loss: 71524022.5600 | Val Loss: 89635980.1600\n",
      "Epoch [330/1000] | Train Loss: 67961767.4800 | Val Loss: 78430364.4800\n",
      "Epoch [331/1000] | Train Loss: 65043369.3400 | Val Loss: 81297311.0400\n",
      "Epoch [332/1000] | Train Loss: 68432319.2000 | Val Loss: 74054075.2000\n",
      "Epoch [333/1000] | Train Loss: 72202280.3200 | Val Loss: 84910746.2400\n",
      "Epoch [334/1000] | Train Loss: 80670105.1200 | Val Loss: 71550028.8000\n",
      "Epoch [335/1000] | Train Loss: 70875230.4400 | Val Loss: 81032581.7600\n",
      "Epoch [336/1000] | Train Loss: 72960133.6000 | Val Loss: 82265999.0400\n",
      "Epoch [337/1000] | Train Loss: 72109250.8800 | Val Loss: 69215330.5600\n",
      "Epoch [338/1000] | Train Loss: 77846912.3400 | Val Loss: 87316465.6000\n",
      "Epoch [339/1000] | Train Loss: 62401548.4800 | Val Loss: 79607138.8800\n",
      "Epoch [340/1000] | Train Loss: 73564192.8800 | Val Loss: 87249524.4800\n",
      "Epoch [341/1000] | Train Loss: 61188679.1200 | Val Loss: 74815094.7200\n",
      "Epoch [342/1000] | Train Loss: 72769737.0400 | Val Loss: 77547505.9200\n",
      "Epoch [343/1000] | Train Loss: 66547494.8800 | Val Loss: 107547828.1600\n",
      "Epoch [344/1000] | Train Loss: 85921831.2800 | Val Loss: 127367594.2400\n",
      "Epoch [345/1000] | Train Loss: 77867680.4000 | Val Loss: 80487305.2800\n",
      "Epoch [346/1000] | Train Loss: 75180479.0000 | Val Loss: 72435823.6800\n",
      "Epoch [347/1000] | Train Loss: 67590170.4000 | Val Loss: 77796531.8400\n",
      "Epoch [348/1000] | Train Loss: 62720461.3750 | Val Loss: 76175285.1200\n",
      "Epoch [349/1000] | Train Loss: 74759517.5200 | Val Loss: 73658934.7200\n",
      "Epoch [350/1000] | Train Loss: 73539436.9600 | Val Loss: 79084386.8800\n",
      "Epoch [351/1000] | Train Loss: 79139865.9200 | Val Loss: 131537902.0800\n",
      "Epoch [352/1000] | Train Loss: 78563568.4800 | Val Loss: 93649600.0000\n",
      "Epoch [353/1000] | Train Loss: 66548613.1200 | Val Loss: 76398152.9600\n",
      "Epoch [354/1000] | Train Loss: 75903645.4400 | Val Loss: 97333201.9200\n",
      "Epoch [355/1000] | Train Loss: 79304301.8000 | Val Loss: 78287661.4400\n",
      "Epoch [356/1000] | Train Loss: 66220787.0400 | Val Loss: 79650201.9200\n",
      "Epoch [357/1000] | Train Loss: 62491916.2100 | Val Loss: 72959362.2400\n",
      "Epoch [358/1000] | Train Loss: 70852030.2400 | Val Loss: 84723012.4800\n",
      "Epoch [359/1000] | Train Loss: 65267725.5700 | Val Loss: 87644941.4400\n",
      "Epoch [360/1000] | Train Loss: 74503782.1600 | Val Loss: 77688670.7200\n",
      "Epoch [361/1000] | Train Loss: 73523082.0800 | Val Loss: 85980883.2000\n",
      "Epoch [362/1000] | Train Loss: 67220753.0400 | Val Loss: 88308473.6000\n",
      "Epoch [363/1000] | Train Loss: 68075487.0400 | Val Loss: 83766498.2400\n",
      "Epoch [364/1000] | Train Loss: 70655316.2400 | Val Loss: 77144092.4800\n",
      "Epoch [365/1000] | Train Loss: 67547731.9475 | Val Loss: 76853960.0000\n",
      "Epoch [366/1000] | Train Loss: 73190496.2400 | Val Loss: 76160846.0800\n",
      "Epoch [367/1000] | Train Loss: 64295727.6800 | Val Loss: 74356168.0000\n",
      "Epoch [368/1000] | Train Loss: 66905324.0600 | Val Loss: 77604884.4800\n",
      "Epoch [369/1000] | Train Loss: 69496599.5600 | Val Loss: 77554819.8400\n",
      "Epoch [370/1000] | Train Loss: 70553400.4912 | Val Loss: 82642691.5200\n",
      "Epoch [371/1000] | Train Loss: 72199044.1600 | Val Loss: 79560451.8400\n",
      "Epoch [372/1000] | Train Loss: 77489429.7200 | Val Loss: 76511480.9600\n",
      "Epoch [373/1000] | Train Loss: 65997906.4000 | Val Loss: 73993588.4800\n",
      "Epoch [374/1000] | Train Loss: 70442028.1600 | Val Loss: 78794443.2000\n",
      "Epoch [375/1000] | Train Loss: 68108119.2000 | Val Loss: 102360252.8000\n",
      "Epoch [376/1000] | Train Loss: 75564182.2400 | Val Loss: 73566723.8400\n",
      "Epoch [377/1000] | Train Loss: 71470146.3700 | Val Loss: 77523669.7600\n",
      "Epoch [378/1000] | Train Loss: 68507204.7200 | Val Loss: 79440491.2000\n",
      "Epoch [379/1000] | Train Loss: 63334256.2900 | Val Loss: 68591196.4800\n",
      "Epoch [380/1000] | Train Loss: 65341080.8800 | Val Loss: 71929759.3600\n",
      "Epoch [381/1000] | Train Loss: 62804040.0000 | Val Loss: 67461916.4800\n",
      "Epoch [382/1000] | Train Loss: 64572178.5800 | Val Loss: 62738750.7200\n",
      "Epoch [383/1000] | Train Loss: 69341615.1500 | Val Loss: 75024255.0400\n",
      "Epoch [384/1000] | Train Loss: 67004204.4000 | Val Loss: 77711033.2800\n",
      "Epoch [385/1000] | Train Loss: 69587431.9200 | Val Loss: 68909207.6800\n",
      "Epoch [386/1000] | Train Loss: 68801199.1900 | Val Loss: 75100524.1600\n",
      "Epoch [387/1000] | Train Loss: 64344754.8800 | Val Loss: 78990664.6400\n",
      "Epoch [388/1000] | Train Loss: 72481733.4400 | Val Loss: 68740944.6400\n",
      "Epoch [389/1000] | Train Loss: 73917185.9200 | Val Loss: 91271748.1600\n",
      "Epoch [390/1000] | Train Loss: 68098577.3950 | Val Loss: 66307986.2400\n",
      "Epoch [391/1000] | Train Loss: 66657708.1600 | Val Loss: 63547689.9200\n",
      "Epoch [392/1000] | Train Loss: 72067020.2650 | Val Loss: 68812657.9200\n",
      "Epoch [393/1000] | Train Loss: 73381553.6300 | Val Loss: 70010929.2800\n",
      "Epoch [394/1000] | Train Loss: 59860822.4000 | Val Loss: 70186629.1200\n",
      "Epoch [395/1000] | Train Loss: 80619395.2000 | Val Loss: 62249547.8400\n",
      "Epoch [396/1000] | Train Loss: 79969535.6800 | Val Loss: 120277084.8000\n",
      "Epoch [397/1000] | Train Loss: 74800578.5600 | Val Loss: 62575487.6800\n",
      "Epoch [398/1000] | Train Loss: 69077697.1600 | Val Loss: 64285868.4800\n",
      "Epoch [399/1000] | Train Loss: 65785571.7400 | Val Loss: 78099955.5200\n",
      "Epoch [400/1000] | Train Loss: 69337947.3000 | Val Loss: 76275600.9600\n",
      "Epoch [401/1000] | Train Loss: 58987326.3800 | Val Loss: 70309543.0400\n",
      "Epoch [402/1000] | Train Loss: 65134998.1000 | Val Loss: 70268115.2000\n",
      "Epoch [403/1000] | Train Loss: 74794166.9112 | Val Loss: 75045876.8000\n",
      "Epoch [404/1000] | Train Loss: 68868391.3700 | Val Loss: 70767577.6000\n",
      "Epoch [405/1000] | Train Loss: 64368097.7600 | Val Loss: 67674555.2000\n",
      "Epoch [406/1000] | Train Loss: 71309229.1800 | Val Loss: 55955640.0000\n",
      "Epoch [407/1000] | Train Loss: 86781816.8000 | Val Loss: 66004904.6400\n",
      "Epoch [408/1000] | Train Loss: 67665302.6000 | Val Loss: 55006850.5600\n",
      "Epoch [409/1000] | Train Loss: 69377872.4800 | Val Loss: 71861756.4800\n",
      "Epoch [410/1000] | Train Loss: 70284329.6800 | Val Loss: 71345011.5200\n",
      "Epoch [411/1000] | Train Loss: 65375069.6000 | Val Loss: 74368864.6400\n",
      "Epoch [412/1000] | Train Loss: 67691969.2000 | Val Loss: 58500740.8000\n",
      "Epoch [413/1000] | Train Loss: 66563966.8200 | Val Loss: 67401987.2000\n",
      "Epoch [414/1000] | Train Loss: 64188306.7200 | Val Loss: 72565023.0400\n",
      "Epoch [415/1000] | Train Loss: 67815651.2000 | Val Loss: 71889255.6800\n",
      "Epoch [416/1000] | Train Loss: 65976935.6400 | Val Loss: 70272534.4000\n",
      "Epoch [417/1000] | Train Loss: 60858860.8000 | Val Loss: 72907378.8800\n",
      "Epoch [418/1000] | Train Loss: 62941302.8000 | Val Loss: 62468236.8000\n",
      "Epoch [419/1000] | Train Loss: 70503048.7600 | Val Loss: 66404523.2000\n",
      "Epoch [420/1000] | Train Loss: 70119598.0000 | Val Loss: 64121352.9600\n",
      "Epoch [421/1000] | Train Loss: 60422857.4400 | Val Loss: 69498439.3600\n",
      "Epoch [422/1000] | Train Loss: 64030714.0000 | Val Loss: 81954183.6800\n",
      "Epoch [423/1000] | Train Loss: 70441782.2800 | Val Loss: 79946604.8000\n",
      "Epoch [424/1000] | Train Loss: 64190583.5200 | Val Loss: 70074777.9200\n",
      "Epoch [425/1000] | Train Loss: 60461416.8100 | Val Loss: 76244151.0400\n",
      "Epoch [426/1000] | Train Loss: 64345130.6400 | Val Loss: 84044641.9200\n",
      "Epoch [427/1000] | Train Loss: 71398507.4200 | Val Loss: 72185100.4800\n",
      "Epoch [428/1000] | Train Loss: 72162176.5300 | Val Loss: 71178026.2400\n",
      "Epoch [429/1000] | Train Loss: 66520071.2000 | Val Loss: 79089046.7200\n",
      "Epoch [430/1000] | Train Loss: 64890506.0000 | Val Loss: 69914245.7600\n",
      "Epoch [431/1000] | Train Loss: 63106258.4200 | Val Loss: 70399727.0400\n",
      "Epoch [432/1000] | Train Loss: 63891473.8000 | Val Loss: 67220295.6800\n",
      "Epoch [433/1000] | Train Loss: 64057954.7200 | Val Loss: 63967104.9600\n",
      "Epoch [434/1000] | Train Loss: 69884811.8400 | Val Loss: 53564730.7200\n",
      "Epoch [435/1000] | Train Loss: 65535494.7200 | Val Loss: 109360860.8000\n",
      "Epoch [436/1000] | Train Loss: 73501855.6800 | Val Loss: 94878695.0400\n",
      "Epoch [437/1000] | Train Loss: 69621771.6000 | Val Loss: 60786542.4000\n",
      "Epoch [438/1000] | Train Loss: 66020761.2000 | Val Loss: 64234693.7600\n",
      "Epoch [439/1000] | Train Loss: 64304558.2400 | Val Loss: 62070423.3600\n",
      "Epoch [440/1000] | Train Loss: 65934233.8400 | Val Loss: 72677561.6000\n",
      "Epoch [441/1000] | Train Loss: 68065676.7200 | Val Loss: 71648061.7600\n",
      "Epoch [442/1000] | Train Loss: 65900259.9600 | Val Loss: 70591363.2000\n",
      "Epoch [443/1000] | Train Loss: 64605024.1600 | Val Loss: 76803407.0400\n",
      "Epoch [444/1000] | Train Loss: 71345758.2800 | Val Loss: 64383905.2800\n",
      "Epoch [445/1000] | Train Loss: 63953218.4000 | Val Loss: 72796872.6400\n",
      "Epoch [446/1000] | Train Loss: 69106389.8200 | Val Loss: 67200549.1200\n",
      "Epoch [447/1000] | Train Loss: 71448062.5600 | Val Loss: 69162977.2800\n",
      "Epoch [448/1000] | Train Loss: 71460647.9900 | Val Loss: 137705233.9200\n",
      "Epoch [449/1000] | Train Loss: 84253581.0400 | Val Loss: 100968922.8800\n",
      "Epoch [450/1000] | Train Loss: 88582291.6800 | Val Loss: 70308049.6000\n",
      "Epoch [451/1000] | Train Loss: 66149253.6950 | Val Loss: 71521854.4000\n",
      "Epoch [452/1000] | Train Loss: 61954581.3400 | Val Loss: 69674895.3600\n",
      "Epoch [453/1000] | Train Loss: 63503543.6800 | Val Loss: 62859122.2400\n",
      "Epoch [454/1000] | Train Loss: 66163606.2400 | Val Loss: 84384321.2800\n",
      "Epoch [455/1000] | Train Loss: 66343825.9400 | Val Loss: 75886373.1200\n",
      "Epoch [456/1000] | Train Loss: 60652260.9600 | Val Loss: 72729844.4800\n",
      "Epoch [457/1000] | Train Loss: 61222739.3600 | Val Loss: 66846050.5600\n",
      "Epoch [458/1000] | Train Loss: 65703589.5400 | Val Loss: 64929108.8000\n",
      "Epoch [459/1000] | Train Loss: 67420319.3600 | Val Loss: 64288423.6800\n",
      "Epoch [460/1000] | Train Loss: 77407328.3200 | Val Loss: 93337793.2800\n",
      "Epoch [461/1000] | Train Loss: 70670659.2000 | Val Loss: 76112869.1200\n",
      "Epoch [462/1000] | Train Loss: 66818651.3600 | Val Loss: 63202610.8800\n",
      "Epoch [463/1000] | Train Loss: 62776295.0400 | Val Loss: 76948539.8400\n",
      "Epoch [464/1000] | Train Loss: 85535038.4000 | Val Loss: 125632841.6000\n",
      "Epoch [465/1000] | Train Loss: 69152091.9400 | Val Loss: 62491694.7200\n",
      "Epoch [466/1000] | Train Loss: 77215839.0600 | Val Loss: 60420212.4800\n",
      "Epoch [467/1000] | Train Loss: 60987890.8600 | Val Loss: 74309360.3200\n",
      "Epoch [468/1000] | Train Loss: 73146076.3200 | Val Loss: 67388866.8800\n",
      "Epoch [469/1000] | Train Loss: 69019567.6800 | Val Loss: 65842748.4800\n",
      "Epoch [470/1000] | Train Loss: 67867528.9800 | Val Loss: 57785364.9600\n",
      "Epoch [471/1000] | Train Loss: 61487176.8500 | Val Loss: 66384029.1200\n",
      "Epoch [472/1000] | Train Loss: 63304733.5700 | Val Loss: 78897974.0800\n",
      "Epoch [473/1000] | Train Loss: 66191122.8800 | Val Loss: 72131655.3600\n",
      "Epoch [474/1000] | Train Loss: 63029164.4400 | Val Loss: 86429453.4400\n",
      "Epoch [475/1000] | Train Loss: 70874583.6800 | Val Loss: 76091473.9200\n",
      "Epoch [476/1000] | Train Loss: 64600248.6400 | Val Loss: 63883008.0000\n",
      "Epoch [477/1000] | Train Loss: 68983333.2800 | Val Loss: 71844647.0400\n",
      "Epoch [478/1000] | Train Loss: 58064563.1200 | Val Loss: 73481644.1600\n",
      "Epoch [479/1000] | Train Loss: 66442272.7150 | Val Loss: 71809079.0400\n",
      "Epoch [480/1000] | Train Loss: 74072087.0400 | Val Loss: 77229726.7200\n",
      "Epoch [481/1000] | Train Loss: 56598322.0000 | Val Loss: 65201440.6400\n",
      "Epoch [482/1000] | Train Loss: 61978122.5600 | Val Loss: 63528616.6400\n",
      "Epoch [483/1000] | Train Loss: 63387068.7200 | Val Loss: 63493421.4400\n",
      "Epoch [484/1000] | Train Loss: 67305329.2000 | Val Loss: 57063457.6000\n",
      "Epoch [485/1000] | Train Loss: 67450748.3800 | Val Loss: 61355673.6000\n",
      "Epoch [486/1000] | Train Loss: 63895449.0400 | Val Loss: 76583250.5600\n",
      "Epoch [487/1000] | Train Loss: 70084516.4800 | Val Loss: 85653869.7600\n",
      "Epoch [488/1000] | Train Loss: 66846752.9350 | Val Loss: 67794994.8800\n",
      "Epoch [489/1000] | Train Loss: 71049394.9650 | Val Loss: 78285203.8400\n",
      "Epoch [490/1000] | Train Loss: 62207624.0300 | Val Loss: 67763260.1600\n",
      "Epoch [491/1000] | Train Loss: 61353104.5600 | Val Loss: 63462183.6800\n",
      "Epoch [492/1000] | Train Loss: 63579749.8600 | Val Loss: 59367887.8400\n",
      "Epoch [493/1000] | Train Loss: 62526385.7600 | Val Loss: 66299011.5200\n",
      "Epoch [494/1000] | Train Loss: 80152934.1600 | Val Loss: 101175925.1200\n",
      "Epoch [495/1000] | Train Loss: 76459608.2700 | Val Loss: 54484424.1600\n",
      "Epoch [496/1000] | Train Loss: 73696187.4800 | Val Loss: 54669657.2800\n",
      "Epoch [497/1000] | Train Loss: 62142454.0200 | Val Loss: 62705912.6400\n",
      "Epoch [498/1000] | Train Loss: 74321509.4000 | Val Loss: 73463078.0800\n",
      "Epoch [499/1000] | Train Loss: 71262124.5800 | Val Loss: 75083567.0400\n",
      "Epoch [500/1000] | Train Loss: 72459560.6400 | Val Loss: 74200357.4400\n",
      "Epoch [501/1000] | Train Loss: 63289445.2300 | Val Loss: 75956099.8400\n",
      "Epoch [502/1000] | Train Loss: 62850072.0000 | Val Loss: 69343361.9200\n",
      "Epoch [503/1000] | Train Loss: 60391022.0800 | Val Loss: 88787613.1200\n",
      "Epoch [504/1000] | Train Loss: 66382713.6800 | Val Loss: 73146470.7200\n",
      "Epoch [505/1000] | Train Loss: 72174720.8800 | Val Loss: 73216032.6400\n",
      "Epoch [506/1000] | Train Loss: 65378992.3200 | Val Loss: 68892656.3200\n",
      "Epoch [507/1000] | Train Loss: 69588624.5600 | Val Loss: 97086995.2000\n",
      "Epoch [508/1000] | Train Loss: 64258569.7600 | Val Loss: 65451941.4400\n",
      "Epoch [509/1000] | Train Loss: 67471704.4000 | Val Loss: 52724523.8400\n",
      "Epoch [510/1000] | Train Loss: 66083586.9100 | Val Loss: 62865795.0400\n",
      "Epoch [511/1000] | Train Loss: 64635255.5200 | Val Loss: 73195912.6400\n",
      "Epoch [512/1000] | Train Loss: 60308055.1200 | Val Loss: 60802678.2400\n",
      "Epoch [513/1000] | Train Loss: 62481783.8800 | Val Loss: 76268650.8800\n",
      "Epoch [514/1000] | Train Loss: 76256610.0400 | Val Loss: 86585276.8000\n",
      "Epoch [515/1000] | Train Loss: 55027756.0400 | Val Loss: 70007927.0400\n",
      "Epoch [516/1000] | Train Loss: 68972023.1400 | Val Loss: 63591773.4400\n",
      "Epoch [517/1000] | Train Loss: 72729652.0100 | Val Loss: 60256184.6400\n",
      "Epoch [518/1000] | Train Loss: 66276328.3200 | Val Loss: 63144402.5600\n",
      "Epoch [519/1000] | Train Loss: 76719091.9200 | Val Loss: 91287699.2000\n",
      "Epoch [520/1000] | Train Loss: 61834391.5600 | Val Loss: 60540096.4800\n",
      "Epoch [521/1000] | Train Loss: 57360649.2800 | Val Loss: 65716301.4400\n",
      "Epoch [522/1000] | Train Loss: 62490377.0700 | Val Loss: 57078036.6400\n",
      "Epoch [523/1000] | Train Loss: 70850960.0000 | Val Loss: 60090933.7600\n",
      "Epoch [524/1000] | Train Loss: 60860418.8000 | Val Loss: 68093138.5600\n",
      "Epoch [525/1000] | Train Loss: 63136529.4400 | Val Loss: 65299840.0000\n",
      "Epoch [526/1000] | Train Loss: 67273920.0000 | Val Loss: 55357691.0400\n",
      "Epoch [527/1000] | Train Loss: 72687820.2400 | Val Loss: 51624337.2800\n",
      "Epoch [528/1000] | Train Loss: 65376884.3200 | Val Loss: 102705760.9600\n",
      "Epoch [529/1000] | Train Loss: 75993472.5800 | Val Loss: 94957863.6800\n",
      "Epoch [530/1000] | Train Loss: 59417958.1600 | Val Loss: 62154065.4400\n",
      "Epoch [531/1000] | Train Loss: 54805000.6400 | Val Loss: 52672138.0800\n",
      "Epoch [532/1000] | Train Loss: 59289660.1600 | Val Loss: 52892474.4000\n",
      "Epoch [533/1000] | Train Loss: 61980098.8000 | Val Loss: 60148331.6800\n",
      "Epoch [534/1000] | Train Loss: 54829919.0800 | Val Loss: 58206623.0400\n",
      "Epoch [535/1000] | Train Loss: 74937637.5200 | Val Loss: 72062532.8000\n",
      "Epoch [536/1000] | Train Loss: 63094951.5200 | Val Loss: 71584156.1600\n",
      "Epoch [537/1000] | Train Loss: 81240002.4000 | Val Loss: 49687173.9200\n",
      "Epoch [538/1000] | Train Loss: 75102358.0800 | Val Loss: 85091597.7600\n",
      "Epoch [539/1000] | Train Loss: 71534312.3200 | Val Loss: 94115920.3200\n",
      "Epoch [540/1000] | Train Loss: 69779861.9600 | Val Loss: 68804122.5600\n",
      "Epoch [541/1000] | Train Loss: 69077556.4600 | Val Loss: 55022040.1600\n",
      "Epoch [542/1000] | Train Loss: 68970008.8000 | Val Loss: 63485456.0000\n",
      "Epoch [543/1000] | Train Loss: 56235884.4800 | Val Loss: 69518275.5200\n",
      "Epoch [544/1000] | Train Loss: 68875141.2800 | Val Loss: 67129209.6000\n",
      "Epoch [545/1000] | Train Loss: 65475510.7200 | Val Loss: 64846755.8400\n",
      "Epoch [546/1000] | Train Loss: 63236307.3600 | Val Loss: 95831650.2400\n",
      "Epoch [547/1000] | Train Loss: 65557017.3600 | Val Loss: 89219432.0000\n",
      "Epoch [548/1000] | Train Loss: 58019227.3200 | Val Loss: 67664048.9600\n",
      "Epoch [549/1000] | Train Loss: 62577565.4800 | Val Loss: 66317600.6400\n",
      "Epoch [550/1000] | Train Loss: 60780953.4400 | Val Loss: 63958641.2800\n",
      "Epoch [551/1000] | Train Loss: 63312279.3600 | Val Loss: 46800233.6000\n",
      "Epoch [552/1000] | Train Loss: 71448538.0000 | Val Loss: 65662419.5200\n",
      "Epoch [553/1000] | Train Loss: 68402558.1600 | Val Loss: 97254009.9200\n",
      "Epoch [554/1000] | Train Loss: 68030955.3250 | Val Loss: 71346420.4800\n",
      "Epoch [555/1000] | Train Loss: 67595058.4000 | Val Loss: 66466822.0800\n",
      "Epoch [556/1000] | Train Loss: 78883895.4400 | Val Loss: 50351506.0800\n",
      "Epoch [557/1000] | Train Loss: 63329181.3250 | Val Loss: 58500875.2000\n",
      "Epoch [558/1000] | Train Loss: 65901055.9400 | Val Loss: 66789061.7600\n",
      "Epoch [559/1000] | Train Loss: 57080221.5200 | Val Loss: 70554862.7200\n",
      "Epoch [560/1000] | Train Loss: 64740580.8800 | Val Loss: 56249452.1600\n",
      "Epoch [561/1000] | Train Loss: 66855430.1200 | Val Loss: 54900787.3600\n",
      "Epoch [562/1000] | Train Loss: 66788059.5200 | Val Loss: 71161856.0000\n",
      "Epoch [563/1000] | Train Loss: 65169220.1600 | Val Loss: 60896894.0800\n",
      "Epoch [564/1000] | Train Loss: 66429490.4000 | Val Loss: 47792726.5600\n",
      "Epoch [565/1000] | Train Loss: 59758311.8400 | Val Loss: 71290066.2400\n",
      "Epoch [566/1000] | Train Loss: 59089973.5500 | Val Loss: 64258907.5200\n",
      "Epoch [567/1000] | Train Loss: 58437294.0800 | Val Loss: 54851583.0400\n",
      "Epoch [568/1000] | Train Loss: 60733324.5200 | Val Loss: 44757850.2400\n",
      "Epoch [569/1000] | Train Loss: 68280280.1600 | Val Loss: 63680578.2400\n",
      "Epoch [570/1000] | Train Loss: 58791319.8800 | Val Loss: 103807144.6400\n",
      "Epoch [571/1000] | Train Loss: 64458184.8000 | Val Loss: 83932700.1600\n",
      "Epoch [572/1000] | Train Loss: 61281560.3200 | Val Loss: 58431613.2800\n",
      "Epoch [573/1000] | Train Loss: 59604449.8400 | Val Loss: 81413864.6400\n",
      "Epoch [574/1000] | Train Loss: 82858464.7200 | Val Loss: 127976778.2400\n",
      "Epoch [575/1000] | Train Loss: 80757632.0000 | Val Loss: 72547915.5200\n",
      "Epoch [576/1000] | Train Loss: 71844320.3200 | Val Loss: 87099240.6400\n",
      "Epoch [577/1000] | Train Loss: 61162961.3200 | Val Loss: 73054156.4800\n",
      "Epoch [578/1000] | Train Loss: 66444595.5600 | Val Loss: 57613755.0400\n",
      "Epoch [579/1000] | Train Loss: 54714815.6000 | Val Loss: 64328362.8800\n",
      "Epoch [580/1000] | Train Loss: 59393098.8000 | Val Loss: 86561686.7200\n",
      "Epoch [581/1000] | Train Loss: 63161366.8800 | Val Loss: 89952484.1600\n",
      "Epoch [582/1000] | Train Loss: 66176850.5600 | Val Loss: 64484770.2400\n",
      "Epoch [583/1000] | Train Loss: 57841164.2800 | Val Loss: 48708848.0000\n",
      "Epoch [584/1000] | Train Loss: 58001506.4000 | Val Loss: 57555651.8400\n",
      "Epoch [585/1000] | Train Loss: 65577322.2400 | Val Loss: 44945809.7600\n",
      "Epoch [586/1000] | Train Loss: 69943407.1200 | Val Loss: 50080665.6000\n",
      "Epoch [587/1000] | Train Loss: 59461063.4400 | Val Loss: 81552209.2800\n",
      "Epoch [588/1000] | Train Loss: 68376816.6400 | Val Loss: 72152390.4000\n",
      "Epoch [589/1000] | Train Loss: 69558375.6800 | Val Loss: 74923915.5200\n",
      "Epoch [590/1000] | Train Loss: 53506133.4400 | Val Loss: 69659255.3600\n",
      "Epoch [591/1000] | Train Loss: 62873402.0800 | Val Loss: 52052826.4000\n",
      "Epoch [592/1000] | Train Loss: 62005888.7200 | Val Loss: 75088937.6000\n",
      "Epoch [593/1000] | Train Loss: 71268556.0100 | Val Loss: 76354587.2000\n",
      "Epoch [594/1000] | Train Loss: 68363591.3500 | Val Loss: 50923669.2800\n",
      "Epoch [595/1000] | Train Loss: 64852015.3550 | Val Loss: 59524233.7600\n",
      "Epoch [596/1000] | Train Loss: 66485335.6800 | Val Loss: 69464251.8400\n",
      "Epoch [597/1000] | Train Loss: 68583517.3000 | Val Loss: 77541641.6000\n",
      "Epoch [598/1000] | Train Loss: 63653076.2800 | Val Loss: 57124132.8000\n",
      "Epoch [599/1000] | Train Loss: 74719437.1200 | Val Loss: 62084390.5600\n",
      "Epoch [600/1000] | Train Loss: 58665924.1800 | Val Loss: 50104787.3600\n",
      "Epoch [601/1000] | Train Loss: 66831825.6000 | Val Loss: 60429639.2000\n",
      "Epoch [602/1000] | Train Loss: 66824869.0500 | Val Loss: 51579007.3600\n",
      "Epoch [603/1000] | Train Loss: 62898907.0400 | Val Loss: 57886601.2800\n",
      "Epoch [604/1000] | Train Loss: 62438011.9200 | Val Loss: 40302731.3600\n",
      "Epoch [605/1000] | Train Loss: 63323359.2000 | Val Loss: 51717418.8800\n",
      "Epoch [606/1000] | Train Loss: 78563075.2800 | Val Loss: 116364935.6800\n",
      "Epoch [607/1000] | Train Loss: 75091004.8000 | Val Loss: 76019898.2400\n",
      "Epoch [608/1000] | Train Loss: 80754774.5200 | Val Loss: 47329348.0000\n",
      "Epoch [609/1000] | Train Loss: 59413884.4800 | Val Loss: 70788522.8800\n",
      "Epoch [610/1000] | Train Loss: 71982920.0000 | Val Loss: 83761989.1200\n",
      "Epoch [611/1000] | Train Loss: 68649945.7600 | Val Loss: 49582730.4000\n",
      "Epoch [612/1000] | Train Loss: 56749387.1200 | Val Loss: 61500700.1600\n",
      "Epoch [613/1000] | Train Loss: 60007045.6000 | Val Loss: 82537748.8000\n",
      "Epoch [614/1000] | Train Loss: 50336590.8800 | Val Loss: 52716900.8000\n",
      "Epoch [615/1000] | Train Loss: 68811219.2000 | Val Loss: 42291871.0400\n",
      "Epoch [616/1000] | Train Loss: 62959987.3525 | Val Loss: 46059707.5200\n",
      "Epoch [617/1000] | Train Loss: 77419395.5600 | Val Loss: 76156557.7600\n",
      "Epoch [618/1000] | Train Loss: 68402399.7800 | Val Loss: 43103874.7200\n",
      "Epoch [619/1000] | Train Loss: 63607991.6400 | Val Loss: 50309836.1600\n",
      "Epoch [620/1000] | Train Loss: 54678057.4400 | Val Loss: 83096743.6800\n",
      "Epoch [621/1000] | Train Loss: 58546681.2400 | Val Loss: 71939518.7200\n",
      "Epoch [622/1000] | Train Loss: 56648265.7200 | Val Loss: 59520442.5600\n",
      "Epoch [623/1000] | Train Loss: 64268257.2000 | Val Loss: 57546962.4000\n",
      "Epoch [624/1000] | Train Loss: 59964535.0400 | Val Loss: 72343869.4400\n",
      "Epoch [625/1000] | Train Loss: 60838708.7000 | Val Loss: 51563965.1200\n",
      "Epoch [626/1000] | Train Loss: 66197916.9200 | Val Loss: 61407121.2800\n",
      "Epoch [627/1000] | Train Loss: 60182435.5900 | Val Loss: 65314576.9600\n",
      "Epoch [628/1000] | Train Loss: 67454314.2400 | Val Loss: 61294182.8800\n",
      "Epoch [629/1000] | Train Loss: 63587072.0200 | Val Loss: 45019248.3200\n",
      "Epoch [630/1000] | Train Loss: 70178515.2800 | Val Loss: 63166251.0400\n",
      "Epoch [631/1000] | Train Loss: 65398185.7600 | Val Loss: 45834355.2000\n",
      "Epoch [632/1000] | Train Loss: 66464286.5600 | Val Loss: 57362277.6000\n",
      "Epoch [633/1000] | Train Loss: 65037169.4400 | Val Loss: 53279502.0800\n",
      "Epoch [634/1000] | Train Loss: 53790216.1600 | Val Loss: 47028319.6800\n",
      "Epoch [635/1000] | Train Loss: 58960325.2000 | Val Loss: 77346906.5600\n",
      "Epoch [636/1000] | Train Loss: 65173573.8800 | Val Loss: 67861067.5200\n",
      "Epoch [637/1000] | Train Loss: 52164142.0900 | Val Loss: 53674607.6800\n",
      "Epoch [638/1000] | Train Loss: 57302163.8400 | Val Loss: 56574276.8000\n",
      "Epoch [639/1000] | Train Loss: 61557697.9600 | Val Loss: 51784544.3200\n",
      "Epoch [640/1000] | Train Loss: 55010047.5200 | Val Loss: 50979021.1200\n",
      "Epoch [641/1000] | Train Loss: 58136288.8800 | Val Loss: 50247293.6000\n",
      "Epoch [642/1000] | Train Loss: 64095804.9600 | Val Loss: 45059354.0800\n",
      "Epoch [643/1000] | Train Loss: 54725954.2000 | Val Loss: 43384672.6400\n",
      "Epoch [644/1000] | Train Loss: 65584597.5600 | Val Loss: 65419033.7600\n",
      "Epoch [645/1000] | Train Loss: 54675616.7100 | Val Loss: 68697416.9600\n",
      "Epoch [646/1000] | Train Loss: 57692592.9600 | Val Loss: 65353775.3600\n",
      "Epoch [647/1000] | Train Loss: 54684129.5200 | Val Loss: 51017863.6800\n",
      "Epoch [648/1000] | Train Loss: 59883468.9600 | Val Loss: 52040410.8800\n",
      "Epoch [649/1000] | Train Loss: 79034102.0800 | Val Loss: 51625997.9200\n",
      "Epoch [650/1000] | Train Loss: 59289887.2100 | Val Loss: 73156328.3200\n",
      "Epoch [651/1000] | Train Loss: 59652905.6000 | Val Loss: 72248865.2800\n",
      "Epoch [652/1000] | Train Loss: 66959442.4000 | Val Loss: 55399361.9200\n",
      "Epoch [653/1000] | Train Loss: 67230915.1200 | Val Loss: 63092961.2800\n",
      "Epoch [654/1000] | Train Loss: 60207803.1600 | Val Loss: 63895462.5600\n",
      "Epoch [655/1000] | Train Loss: 59705794.8800 | Val Loss: 62935792.8000\n",
      "Epoch [656/1000] | Train Loss: 56369133.3600 | Val Loss: 66109557.7600\n",
      "Epoch [657/1000] | Train Loss: 53413691.8400 | Val Loss: 61205618.5600\n",
      "Epoch [658/1000] | Train Loss: 68386558.7200 | Val Loss: 84484050.2400\n",
      "Epoch [659/1000] | Train Loss: 57858340.0800 | Val Loss: 48682504.1600\n",
      "Epoch [660/1000] | Train Loss: 78599278.1600 | Val Loss: 41327810.2400\n",
      "Epoch [661/1000] | Train Loss: 63874148.6400 | Val Loss: 72574642.2400\n",
      "Epoch [662/1000] | Train Loss: 73553625.2800 | Val Loss: 88744765.1200\n",
      "Epoch [663/1000] | Train Loss: 66730347.9200 | Val Loss: 56767262.0800\n",
      "Epoch [664/1000] | Train Loss: 60288019.0400 | Val Loss: 51789562.2400\n",
      "Epoch [665/1000] | Train Loss: 64803026.7600 | Val Loss: 42633452.1600\n",
      "Epoch [666/1000] | Train Loss: 62976869.4400 | Val Loss: 62375713.7600\n",
      "Epoch [667/1000] | Train Loss: 55977281.3800 | Val Loss: 63301225.4400\n",
      "Epoch [668/1000] | Train Loss: 63115806.5600 | Val Loss: 61431907.3600\n",
      "Epoch [669/1000] | Train Loss: 58438220.9400 | Val Loss: 69060488.3200\n",
      "Epoch [670/1000] | Train Loss: 61129581.6000 | Val Loss: 68661205.1200\n",
      "Epoch [671/1000] | Train Loss: 55340677.9400 | Val Loss: 54987934.5600\n",
      "Epoch [672/1000] | Train Loss: 65411769.0200 | Val Loss: 61448148.8000\n",
      "Epoch [673/1000] | Train Loss: 54251668.9600 | Val Loss: 56643042.5600\n",
      "Epoch [674/1000] | Train Loss: 69978368.3200 | Val Loss: 38007399.2000\n",
      "Epoch [675/1000] | Train Loss: 62382747.4400 | Val Loss: 56328913.4400\n",
      "Epoch [676/1000] | Train Loss: 53547321.2800 | Val Loss: 96517182.7200\n",
      "Epoch [677/1000] | Train Loss: 63894979.0400 | Val Loss: 102061341.7600\n",
      "Epoch [678/1000] | Train Loss: 71868797.7600 | Val Loss: 90487536.9600\n",
      "Epoch [679/1000] | Train Loss: 70586679.6800 | Val Loss: 113912350.0800\n",
      "Epoch [680/1000] | Train Loss: 71904347.8200 | Val Loss: 79745600.6400\n",
      "Epoch [681/1000] | Train Loss: 62640108.1200 | Val Loss: 65564613.4400\n",
      "Epoch [682/1000] | Train Loss: 62612097.2000 | Val Loss: 51569452.0000\n",
      "Epoch [683/1000] | Train Loss: 60348829.9600 | Val Loss: 58034969.6000\n",
      "Epoch [684/1000] | Train Loss: 60894854.2800 | Val Loss: 65827346.2400\n",
      "Epoch [685/1000] | Train Loss: 57035529.2400 | Val Loss: 56670808.8000\n",
      "Epoch [686/1000] | Train Loss: 60943749.9200 | Val Loss: 56625737.9200\n",
      "Epoch [687/1000] | Train Loss: 62406456.7200 | Val Loss: 45867368.8000\n",
      "Epoch [688/1000] | Train Loss: 64344719.5200 | Val Loss: 76347521.2800\n",
      "Epoch [689/1000] | Train Loss: 58199187.5150 | Val Loss: 70712786.8800\n",
      "Epoch [690/1000] | Train Loss: 65212291.3600 | Val Loss: 53699949.1200\n",
      "Epoch [691/1000] | Train Loss: 65610923.9500 | Val Loss: 65176801.1200\n",
      "Epoch [692/1000] | Train Loss: 68636053.9200 | Val Loss: 72324965.1200\n",
      "Epoch [693/1000] | Train Loss: 67599349.0400 | Val Loss: 47847990.5600\n",
      "Epoch [694/1000] | Train Loss: 62900665.2800 | Val Loss: 97857135.0400\n",
      "Epoch [695/1000] | Train Loss: 64876469.1200 | Val Loss: 47134443.2000\n",
      "Epoch [696/1000] | Train Loss: 55168371.3600 | Val Loss: 46744568.6400\n",
      "Epoch [697/1000] | Train Loss: 72349207.2800 | Val Loss: 47660936.3200\n",
      "Epoch [698/1000] | Train Loss: 68455940.6400 | Val Loss: 112808150.0800\n",
      "Epoch [699/1000] | Train Loss: 71076429.9600 | Val Loss: 96613457.9200\n",
      "Epoch [700/1000] | Train Loss: 70950891.6800 | Val Loss: 58837358.4000\n",
      "Epoch [701/1000] | Train Loss: 61907254.6400 | Val Loss: 58841525.4400\n",
      "Epoch [702/1000] | Train Loss: 52845202.7200 | Val Loss: 67914125.1200\n",
      "Epoch [703/1000] | Train Loss: 59729635.3600 | Val Loss: 63894094.7200\n",
      "Epoch [704/1000] | Train Loss: 62447661.2900 | Val Loss: 40521028.0000\n",
      "Epoch [705/1000] | Train Loss: 62109283.3700 | Val Loss: 50211896.1600\n",
      "Epoch [706/1000] | Train Loss: 61035556.0000 | Val Loss: 72524264.0000\n",
      "Epoch [707/1000] | Train Loss: 57613005.6000 | Val Loss: 59123065.9200\n",
      "Epoch [708/1000] | Train Loss: 77569753.7600 | Val Loss: 40371246.8800\n",
      "Epoch [709/1000] | Train Loss: 60047169.6000 | Val Loss: 57710951.2000\n",
      "Epoch [710/1000] | Train Loss: 67963434.9600 | Val Loss: 89105794.5600\n",
      "Epoch [711/1000] | Train Loss: 57824535.4750 | Val Loss: 66318263.8400\n",
      "Epoch [712/1000] | Train Loss: 54947070.1800 | Val Loss: 46712118.8800\n",
      "Epoch [713/1000] | Train Loss: 57701090.8800 | Val Loss: 49942919.8400\n",
      "Epoch [714/1000] | Train Loss: 55571869.6900 | Val Loss: 54204927.6800\n",
      "Epoch [715/1000] | Train Loss: 61334694.8000 | Val Loss: 78677510.7200\n",
      "Epoch [716/1000] | Train Loss: 54117727.5200 | Val Loss: 57168844.1600\n",
      "Epoch [717/1000] | Train Loss: 55115835.0800 | Val Loss: 62336091.5200\n",
      "Epoch [718/1000] | Train Loss: 65207145.9200 | Val Loss: 69213639.3600\n",
      "Epoch [719/1000] | Train Loss: 51290087.3600 | Val Loss: 56717192.0000\n",
      "Epoch [720/1000] | Train Loss: 54676182.1600 | Val Loss: 63143286.2400\n",
      "Epoch [721/1000] | Train Loss: 58866886.0600 | Val Loss: 76175789.7600\n",
      "Epoch [722/1000] | Train Loss: 57699294.6250 | Val Loss: 62212782.0800\n",
      "Epoch [723/1000] | Train Loss: 55162110.8800 | Val Loss: 45764397.9200\n",
      "Epoch [724/1000] | Train Loss: 72521159.0400 | Val Loss: 58908834.8800\n",
      "Epoch [725/1000] | Train Loss: 63553486.7600 | Val Loss: 60753876.4800\n",
      "Epoch [726/1000] | Train Loss: 60422446.7300 | Val Loss: 47724998.5600\n",
      "Epoch [727/1000] | Train Loss: 51324735.7600 | Val Loss: 60361733.4400\n",
      "Epoch [728/1000] | Train Loss: 60050002.8500 | Val Loss: 73575555.5200\n",
      "Epoch [729/1000] | Train Loss: 63055939.0100 | Val Loss: 71482804.4800\n",
      "Epoch [730/1000] | Train Loss: 63196221.2000 | Val Loss: 66668468.4800\n",
      "Epoch [731/1000] | Train Loss: 47807680.1600 | Val Loss: 48294349.2800\n",
      "Epoch [732/1000] | Train Loss: 58345287.6000 | Val Loss: 38959860.8000\n",
      "Epoch [733/1000] | Train Loss: 64449398.4200 | Val Loss: 63365987.8400\n",
      "Epoch [734/1000] | Train Loss: 55504671.9900 | Val Loss: 64187893.1200\n",
      "Epoch [735/1000] | Train Loss: 54104252.3200 | Val Loss: 51856972.1600\n",
      "Epoch [736/1000] | Train Loss: 70727675.0400 | Val Loss: 65423157.6000\n",
      "Epoch [737/1000] | Train Loss: 65224236.2800 | Val Loss: 64508577.1200\n",
      "Epoch [738/1000] | Train Loss: 76449312.6400 | Val Loss: 37397090.2400\n",
      "Epoch [739/1000] | Train Loss: 58662281.2800 | Val Loss: 81541595.2000\n",
      "Epoch [740/1000] | Train Loss: 72480608.7200 | Val Loss: 101778070.0800\n",
      "Epoch [741/1000] | Train Loss: 72892736.4800 | Val Loss: 66229741.4400\n",
      "Epoch [742/1000] | Train Loss: 59207697.2800 | Val Loss: 84045287.3600\n",
      "Epoch [743/1000] | Train Loss: 67822630.3200 | Val Loss: 74731883.5200\n",
      "Epoch [744/1000] | Train Loss: 53162526.7200 | Val Loss: 61452905.4400\n",
      "Epoch [745/1000] | Train Loss: 55316241.7600 | Val Loss: 68365922.5600\n",
      "Epoch [746/1000] | Train Loss: 56292749.1000 | Val Loss: 65368699.2000\n",
      "Epoch [747/1000] | Train Loss: 53566741.7000 | Val Loss: 52278078.0800\n",
      "Epoch [748/1000] | Train Loss: 53005662.7200 | Val Loss: 57829752.6400\n",
      "Epoch [749/1000] | Train Loss: 65245736.5000 | Val Loss: 58556230.5600\n",
      "Epoch [750/1000] | Train Loss: 61022668.5600 | Val Loss: 69522279.6800\n",
      "Epoch [751/1000] | Train Loss: 56305312.0000 | Val Loss: 68157097.2800\n",
      "Epoch [752/1000] | Train Loss: 55330460.8000 | Val Loss: 61849915.3600\n",
      "Epoch [753/1000] | Train Loss: 53293764.3200 | Val Loss: 66934854.0800\n",
      "Epoch [754/1000] | Train Loss: 58568175.2000 | Val Loss: 54345776.4800\n",
      "Epoch [755/1000] | Train Loss: 65240773.6400 | Val Loss: 58260925.6000\n",
      "Epoch [756/1000] | Train Loss: 52111511.1600 | Val Loss: 52485329.9200\n",
      "Epoch [757/1000] | Train Loss: 48699173.9200 | Val Loss: 50144890.2400\n",
      "Epoch [758/1000] | Train Loss: 59718653.0400 | Val Loss: 38619220.8000\n",
      "Epoch [759/1000] | Train Loss: 74399394.0000 | Val Loss: 35714558.2400\n",
      "Epoch [760/1000] | Train Loss: 52792909.2000 | Val Loss: 61905584.6400\n",
      "Epoch [761/1000] | Train Loss: 51225607.5300 | Val Loss: 74451618.8800\n",
      "Epoch [762/1000] | Train Loss: 57810285.4400 | Val Loss: 59169653.4400\n",
      "Epoch [763/1000] | Train Loss: 54516426.8800 | Val Loss: 53875314.4000\n",
      "Epoch [764/1000] | Train Loss: 58094799.3600 | Val Loss: 48074546.5600\n",
      "Epoch [765/1000] | Train Loss: 49571666.1600 | Val Loss: 64710297.9200\n",
      "Epoch [766/1000] | Train Loss: 52936073.1825 | Val Loss: 55248451.2000\n",
      "Epoch [767/1000] | Train Loss: 55044943.0800 | Val Loss: 60399609.4400\n",
      "Epoch [768/1000] | Train Loss: 55653543.4100 | Val Loss: 59911382.8800\n",
      "Epoch [769/1000] | Train Loss: 60562874.9750 | Val Loss: 55311588.1600\n",
      "Epoch [770/1000] | Train Loss: 51521229.4200 | Val Loss: 58214987.0400\n",
      "Epoch [771/1000] | Train Loss: 54355504.5600 | Val Loss: 53954502.7200\n",
      "Epoch [772/1000] | Train Loss: 52094256.8800 | Val Loss: 46712936.6400\n",
      "Epoch [773/1000] | Train Loss: 56031273.5200 | Val Loss: 42846979.0400\n",
      "Epoch [774/1000] | Train Loss: 60886404.5150 | Val Loss: 76829950.0800\n",
      "Epoch [775/1000] | Train Loss: 63227799.3900 | Val Loss: 65086527.5200\n",
      "Epoch [776/1000] | Train Loss: 53985268.0200 | Val Loss: 38380416.0000\n",
      "Epoch [777/1000] | Train Loss: 55288178.2400 | Val Loss: 57918827.8400\n",
      "Epoch [778/1000] | Train Loss: 58574861.6000 | Val Loss: 131002751.3600\n",
      "Epoch [779/1000] | Train Loss: 72018422.2400 | Val Loss: 93420384.6400\n",
      "Epoch [780/1000] | Train Loss: 63810097.8600 | Val Loss: 38487922.5600\n",
      "Epoch [781/1000] | Train Loss: 58900752.9900 | Val Loss: 42033836.1600\n",
      "Epoch [782/1000] | Train Loss: 57726317.1600 | Val Loss: 84811619.5200\n",
      "Epoch [783/1000] | Train Loss: 50893649.6400 | Val Loss: 39556385.4400\n",
      "Epoch [784/1000] | Train Loss: 60743419.2800 | Val Loss: 38854759.3600\n",
      "Epoch [785/1000] | Train Loss: 55628387.3500 | Val Loss: 57577331.8400\n",
      "Epoch [786/1000] | Train Loss: 50258802.5500 | Val Loss: 66675686.2400\n",
      "Epoch [787/1000] | Train Loss: 57161568.6400 | Val Loss: 74206135.3600\n",
      "Epoch [788/1000] | Train Loss: 56239175.9200 | Val Loss: 57965122.0800\n",
      "Epoch [789/1000] | Train Loss: 63809929.5000 | Val Loss: 59327362.7200\n",
      "Epoch [790/1000] | Train Loss: 55416076.8900 | Val Loss: 43727071.3600\n",
      "Epoch [791/1000] | Train Loss: 58426959.4400 | Val Loss: 41360737.4400\n",
      "Epoch [792/1000] | Train Loss: 54850628.1500 | Val Loss: 49111287.3600\n",
      "Epoch [793/1000] | Train Loss: 53331830.4000 | Val Loss: 70373504.9600\n",
      "Epoch [794/1000] | Train Loss: 71513236.4800 | Val Loss: 64255644.3200\n",
      "Epoch [795/1000] | Train Loss: 53276641.2000 | Val Loss: 56319669.1200\n",
      "Epoch [796/1000] | Train Loss: 55800926.8200 | Val Loss: 77910422.4000\n",
      "Epoch [797/1000] | Train Loss: 55844530.9600 | Val Loss: 58101481.9200\n",
      "Epoch [798/1000] | Train Loss: 53893244.0000 | Val Loss: 48502605.9200\n",
      "Epoch [799/1000] | Train Loss: 59319693.7600 | Val Loss: 50730971.6800\n",
      "Epoch [800/1000] | Train Loss: 42078569.5200 | Val Loss: 55006743.6800\n",
      "Epoch [801/1000] | Train Loss: 49149979.6400 | Val Loss: 61706376.8000\n",
      "Epoch [802/1000] | Train Loss: 61949523.6800 | Val Loss: 53227705.9200\n",
      "Epoch [803/1000] | Train Loss: 47501537.7200 | Val Loss: 83573208.9600\n",
      "Epoch [804/1000] | Train Loss: 58933391.3600 | Val Loss: 63492263.0400\n",
      "Epoch [805/1000] | Train Loss: 52957881.8400 | Val Loss: 88070784.6400\n",
      "Epoch [806/1000] | Train Loss: 49510615.7000 | Val Loss: 68388064.6400\n",
      "Epoch [807/1000] | Train Loss: 60918841.1400 | Val Loss: 60109856.1600\n",
      "Epoch [808/1000] | Train Loss: 52005420.8000 | Val Loss: 48154213.4400\n",
      "Epoch [809/1000] | Train Loss: 62067807.2700 | Val Loss: 36011099.2000\n",
      "Epoch [810/1000] | Train Loss: 59590241.8400 | Val Loss: 48003592.3200\n",
      "Epoch [811/1000] | Train Loss: 54798506.7200 | Val Loss: 39876033.2800\n",
      "Epoch [812/1000] | Train Loss: 63049411.4000 | Val Loss: 46001861.6000\n",
      "Epoch [813/1000] | Train Loss: 61800369.0800 | Val Loss: 62460307.3600\n",
      "Epoch [814/1000] | Train Loss: 55068718.4600 | Val Loss: 93339707.2000\n",
      "Epoch [815/1000] | Train Loss: 65602794.5400 | Val Loss: 58927837.4400\n",
      "Epoch [816/1000] | Train Loss: 60222656.6400 | Val Loss: 55515265.2800\n",
      "Epoch [817/1000] | Train Loss: 52574155.8400 | Val Loss: 47745746.5600\n",
      "Epoch [818/1000] | Train Loss: 47261474.7200 | Val Loss: 33503051.8400\n",
      "Epoch [819/1000] | Train Loss: 56042387.3600 | Val Loss: 44030856.6400\n",
      "Epoch [820/1000] | Train Loss: 51437953.1200 | Val Loss: 49633309.9200\n",
      "Epoch [821/1000] | Train Loss: 56220326.7200 | Val Loss: 51119041.7600\n",
      "Epoch [822/1000] | Train Loss: 48009534.3300 | Val Loss: 42327658.4000\n",
      "Epoch [823/1000] | Train Loss: 65150155.3600 | Val Loss: 48851168.0000\n",
      "Epoch [824/1000] | Train Loss: 60229747.2800 | Val Loss: 108936769.6000\n",
      "Epoch [825/1000] | Train Loss: 66908243.1000 | Val Loss: 57489122.8800\n",
      "Epoch [826/1000] | Train Loss: 59932288.5600 | Val Loss: 36501600.2400\n",
      "Epoch [827/1000] | Train Loss: 53062485.0000 | Val Loss: 43165884.9600\n",
      "Epoch [828/1000] | Train Loss: 50296214.7200 | Val Loss: 52479084.9600\n",
      "Epoch [829/1000] | Train Loss: 49625074.4200 | Val Loss: 42756162.4000\n",
      "Epoch [830/1000] | Train Loss: 68652179.2800 | Val Loss: 55445593.1200\n",
      "Epoch [831/1000] | Train Loss: 54596234.0900 | Val Loss: 97494607.6800\n",
      "Epoch [832/1000] | Train Loss: 58483376.2400 | Val Loss: 49868916.6400\n",
      "Epoch [833/1000] | Train Loss: 58143326.1200 | Val Loss: 36264798.6400\n",
      "Epoch [834/1000] | Train Loss: 55554423.6600 | Val Loss: 60123295.8400\n",
      "Epoch [835/1000] | Train Loss: 53835888.0000 | Val Loss: 63023651.6800\n",
      "Epoch [836/1000] | Train Loss: 64368471.8400 | Val Loss: 103546466.5600\n",
      "Epoch [837/1000] | Train Loss: 67712427.8400 | Val Loss: 35495396.0000\n",
      "Epoch [838/1000] | Train Loss: 66964965.2800 | Val Loss: 52669191.8400\n",
      "Epoch [839/1000] | Train Loss: 46513438.2400 | Val Loss: 36267410.0800\n",
      "Epoch [840/1000] | Train Loss: 56969883.0400 | Val Loss: 42587349.9200\n",
      "Epoch [841/1000] | Train Loss: 55334136.4800 | Val Loss: 92244400.0000\n",
      "Epoch [842/1000] | Train Loss: 61052946.2400 | Val Loss: 65726514.8800\n",
      "Epoch [843/1000] | Train Loss: 65581480.1600 | Val Loss: 46352642.0800\n",
      "Epoch [844/1000] | Train Loss: 52901775.8000 | Val Loss: 51035958.4000\n",
      "Epoch [845/1000] | Train Loss: 61156940.4000 | Val Loss: 42104265.9200\n",
      "Epoch [846/1000] | Train Loss: 53732225.2000 | Val Loss: 86832026.8800\n",
      "Epoch [847/1000] | Train Loss: 64272991.7800 | Val Loss: 106516167.0400\n",
      "Epoch [848/1000] | Train Loss: 63618082.7200 | Val Loss: 46917299.5200\n",
      "Epoch [849/1000] | Train Loss: 48866882.4000 | Val Loss: 35720152.0800\n",
      "Epoch [850/1000] | Train Loss: 58375898.0200 | Val Loss: 56937562.8800\n",
      "Epoch [851/1000] | Train Loss: 59270171.8400 | Val Loss: 46361106.5600\n",
      "Epoch [852/1000] | Train Loss: 42457245.3800 | Val Loss: 37473631.7600\n",
      "Epoch [853/1000] | Train Loss: 52949535.9200 | Val Loss: 42435705.7600\n",
      "Epoch [854/1000] | Train Loss: 57015537.7000 | Val Loss: 33905867.3600\n",
      "Epoch [855/1000] | Train Loss: 49093657.0875 | Val Loss: 79622559.0400\n",
      "Epoch [856/1000] | Train Loss: 54071779.5200 | Val Loss: 82347297.6000\n",
      "Epoch [857/1000] | Train Loss: 49353876.1600 | Val Loss: 60689475.2000\n",
      "Epoch [858/1000] | Train Loss: 47201688.9000 | Val Loss: 44399912.4800\n",
      "Epoch [859/1000] | Train Loss: 50738964.7600 | Val Loss: 52878072.1600\n",
      "Epoch [860/1000] | Train Loss: 53046575.2400 | Val Loss: 65715717.6000\n",
      "Epoch [861/1000] | Train Loss: 47399025.3000 | Val Loss: 52050790.2400\n",
      "Epoch [862/1000] | Train Loss: 52750627.0550 | Val Loss: 46923017.1200\n",
      "Epoch [863/1000] | Train Loss: 52624247.6000 | Val Loss: 48236063.8400\n",
      "Epoch [864/1000] | Train Loss: 45807275.7600 | Val Loss: 49524601.7600\n",
      "Epoch [865/1000] | Train Loss: 57317135.4400 | Val Loss: 66403740.6400\n",
      "Epoch [866/1000] | Train Loss: 49854738.8800 | Val Loss: 47837574.8800\n",
      "Epoch [867/1000] | Train Loss: 59784687.3600 | Val Loss: 45596385.9200\n",
      "Epoch [868/1000] | Train Loss: 51692567.5800 | Val Loss: 45570835.6800\n",
      "Epoch [869/1000] | Train Loss: 55279324.0000 | Val Loss: 59466535.0400\n",
      "Epoch [870/1000] | Train Loss: 64928973.1600 | Val Loss: 111740865.6000\n",
      "Epoch [871/1000] | Train Loss: 55995050.3800 | Val Loss: 44464431.3600\n",
      "Epoch [872/1000] | Train Loss: 52548300.4800 | Val Loss: 46351900.1600\n",
      "Epoch [873/1000] | Train Loss: 53366069.1200 | Val Loss: 67302528.3200\n",
      "Epoch [874/1000] | Train Loss: 54369374.4000 | Val Loss: 55221537.6000\n",
      "Epoch [875/1000] | Train Loss: 58620372.1300 | Val Loss: 33316283.2000\n",
      "Epoch [876/1000] | Train Loss: 52874148.5600 | Val Loss: 51071020.6400\n",
      "Epoch [877/1000] | Train Loss: 54542548.9600 | Val Loss: 50436167.3600\n",
      "Epoch [878/1000] | Train Loss: 61381930.1600 | Val Loss: 58887625.4400\n",
      "Epoch [879/1000] | Train Loss: 49158650.8600 | Val Loss: 47005802.2400\n",
      "Epoch [880/1000] | Train Loss: 59348872.4800 | Val Loss: 55498856.8000\n",
      "Epoch [881/1000] | Train Loss: 66457089.2800 | Val Loss: 35308573.1200\n",
      "Epoch [882/1000] | Train Loss: 43212139.3600 | Val Loss: 56874126.7200\n",
      "Epoch [883/1000] | Train Loss: 50532556.8375 | Val Loss: 74681867.5200\n",
      "Epoch [884/1000] | Train Loss: 56606026.8800 | Val Loss: 49640090.2400\n",
      "Epoch [885/1000] | Train Loss: 56234534.8800 | Val Loss: 65471301.7600\n",
      "Epoch [886/1000] | Train Loss: 57443038.4000 | Val Loss: 29375483.2000\n",
      "Epoch [887/1000] | Train Loss: 44845137.2800 | Val Loss: 69721039.2000\n",
      "Epoch [888/1000] | Train Loss: 64624595.6800 | Val Loss: 102222864.6400\n",
      "Epoch [889/1000] | Train Loss: 57129489.4800 | Val Loss: 35627068.4800\n",
      "Epoch [890/1000] | Train Loss: 58630374.0700 | Val Loss: 38233913.6800\n",
      "Epoch [891/1000] | Train Loss: 59193906.0400 | Val Loss: 64579120.6400\n",
      "Epoch [892/1000] | Train Loss: 54244754.1600 | Val Loss: 52184561.9200\n",
      "Epoch [893/1000] | Train Loss: 55876762.8800 | Val Loss: 38095405.8400\n",
      "Epoch [894/1000] | Train Loss: 58360207.4000 | Val Loss: 28401874.8800\n",
      "Epoch [895/1000] | Train Loss: 60219055.7600 | Val Loss: 57402774.2400\n",
      "Epoch [896/1000] | Train Loss: 46631563.8400 | Val Loss: 80321024.0000\n",
      "Epoch [897/1000] | Train Loss: 49415154.7200 | Val Loss: 47274553.6000\n",
      "Epoch [898/1000] | Train Loss: 46555341.0000 | Val Loss: 48481361.1200\n",
      "Epoch [899/1000] | Train Loss: 50912418.2400 | Val Loss: 50517912.4800\n",
      "Epoch [900/1000] | Train Loss: 44436537.2600 | Val Loss: 56472089.7600\n",
      "Epoch [901/1000] | Train Loss: 55996736.4800 | Val Loss: 49710791.8400\n",
      "Epoch [902/1000] | Train Loss: 48185252.8700 | Val Loss: 42016598.9600\n",
      "Epoch [903/1000] | Train Loss: 39252273.2400 | Val Loss: 46289668.8000\n",
      "Epoch [904/1000] | Train Loss: 51737809.0800 | Val Loss: 48221386.8800\n",
      "Epoch [905/1000] | Train Loss: 46712564.7600 | Val Loss: 39511613.6000\n",
      "Epoch [906/1000] | Train Loss: 47040306.6000 | Val Loss: 42012939.2000\n",
      "Epoch [907/1000] | Train Loss: 53547905.4000 | Val Loss: 60709512.3200\n",
      "Epoch [908/1000] | Train Loss: 52532445.9200 | Val Loss: 54695958.0800\n",
      "Epoch [909/1000] | Train Loss: 56410504.7500 | Val Loss: 36986951.2000\n",
      "Epoch [910/1000] | Train Loss: 48571076.8800 | Val Loss: 40832841.9200\n",
      "Epoch [911/1000] | Train Loss: 53963865.9800 | Val Loss: 63055756.8000\n",
      "Epoch [912/1000] | Train Loss: 52501635.3600 | Val Loss: 50711526.4000\n",
      "Epoch [913/1000] | Train Loss: 49106617.2000 | Val Loss: 38246537.9200\n",
      "Epoch [914/1000] | Train Loss: 72883206.0800 | Val Loss: 32007914.4800\n",
      "Epoch [915/1000] | Train Loss: 52109045.1200 | Val Loss: 100101699.2000\n",
      "Epoch [916/1000] | Train Loss: 65495450.8300 | Val Loss: 35564214.8800\n",
      "Epoch [917/1000] | Train Loss: 75462634.2400 | Val Loss: 45305461.9200\n",
      "Epoch [918/1000] | Train Loss: 54915665.6900 | Val Loss: 80262252.8000\n",
      "Epoch [919/1000] | Train Loss: 47313757.5200 | Val Loss: 55922202.8800\n",
      "Epoch [920/1000] | Train Loss: 72077367.3600 | Val Loss: 34202324.9600\n",
      "Epoch [921/1000] | Train Loss: 54963026.8800 | Val Loss: 109285186.5600\n",
      "Epoch [922/1000] | Train Loss: 74581826.9600 | Val Loss: 111965173.4400\n",
      "Epoch [923/1000] | Train Loss: 59009588.3200 | Val Loss: 37196428.8000\n",
      "Epoch [924/1000] | Train Loss: 57752670.5600 | Val Loss: 42387603.0400\n",
      "Epoch [925/1000] | Train Loss: 50098822.4000 | Val Loss: 38249218.4800\n",
      "Epoch [926/1000] | Train Loss: 48921823.2000 | Val Loss: 54559948.1600\n",
      "Epoch [927/1000] | Train Loss: 60984095.7600 | Val Loss: 61978711.0400\n",
      "Epoch [928/1000] | Train Loss: 46954444.0000 | Val Loss: 68074313.7600\n",
      "Epoch [929/1000] | Train Loss: 42244440.8400 | Val Loss: 55569405.7600\n",
      "Epoch [930/1000] | Train Loss: 53321284.1800 | Val Loss: 48628013.1200\n",
      "Epoch [931/1000] | Train Loss: 47923930.0400 | Val Loss: 43885128.3200\n",
      "Epoch [932/1000] | Train Loss: 63778115.0000 | Val Loss: 30755533.0400\n",
      "Epoch [933/1000] | Train Loss: 48982651.2000 | Val Loss: 55244364.1600\n",
      "Epoch [934/1000] | Train Loss: 59616799.6800 | Val Loss: 77103939.8400\n",
      "Epoch [935/1000] | Train Loss: 56278562.2850 | Val Loss: 38435031.3600\n",
      "Epoch [936/1000] | Train Loss: 49550356.2800 | Val Loss: 44484711.3600\n",
      "Epoch [937/1000] | Train Loss: 48778067.5200 | Val Loss: 70084218.8800\n",
      "Epoch [938/1000] | Train Loss: 48435661.7600 | Val Loss: 38862762.1600\n",
      "Epoch [939/1000] | Train Loss: 47937607.0400 | Val Loss: 46846694.2400\n",
      "Epoch [940/1000] | Train Loss: 49342193.3400 | Val Loss: 44198025.6000\n",
      "Epoch [941/1000] | Train Loss: 63638731.2000 | Val Loss: 64042200.4800\n",
      "Epoch [942/1000] | Train Loss: 53493904.8400 | Val Loss: 78339143.6800\n",
      "Epoch [943/1000] | Train Loss: 52223002.4200 | Val Loss: 46140309.7600\n",
      "Epoch [944/1000] | Train Loss: 56317928.0800 | Val Loss: 60078307.2000\n",
      "Epoch [945/1000] | Train Loss: 54039682.3200 | Val Loss: 121318426.8800\n",
      "Epoch [946/1000] | Train Loss: 63964973.2400 | Val Loss: 68597861.7600\n",
      "Epoch [947/1000] | Train Loss: 46023844.9600 | Val Loss: 33026182.3200\n",
      "Epoch [948/1000] | Train Loss: 74332636.4800 | Val Loss: 27517519.2000\n",
      "Epoch [949/1000] | Train Loss: 53485934.1600 | Val Loss: 53017879.6800\n",
      "Epoch [950/1000] | Train Loss: 47439735.6800 | Val Loss: 60490782.0800\n",
      "Epoch [951/1000] | Train Loss: 54887576.5600 | Val Loss: 30064282.2400\n",
      "Epoch [952/1000] | Train Loss: 48501196.3200 | Val Loss: 45964286.5600\n",
      "Epoch [953/1000] | Train Loss: 47364552.0000 | Val Loss: 104306175.0400\n",
      "Epoch [954/1000] | Train Loss: 52358044.3200 | Val Loss: 59482025.7600\n",
      "Epoch [955/1000] | Train Loss: 78457684.1600 | Val Loss: 39882092.4800\n",
      "Epoch [956/1000] | Train Loss: 58161356.1600 | Val Loss: 51765083.8400\n",
      "Epoch [957/1000] | Train Loss: 54365387.1000 | Val Loss: 56497224.4800\n",
      "Epoch [958/1000] | Train Loss: 51967166.4000 | Val Loss: 56968204.1600\n",
      "Epoch [959/1000] | Train Loss: 48901039.0400 | Val Loss: 47232686.2400\n",
      "Epoch [960/1000] | Train Loss: 49198398.4000 | Val Loss: 48610743.3600\n",
      "Epoch [961/1000] | Train Loss: 51121944.1800 | Val Loss: 55678192.9600\n",
      "Epoch [962/1000] | Train Loss: 60815514.9100 | Val Loss: 50219977.6000\n",
      "Epoch [963/1000] | Train Loss: 51217711.2200 | Val Loss: 49842594.5600\n",
      "Epoch [964/1000] | Train Loss: 48034730.7600 | Val Loss: 43659115.2000\n",
      "Epoch [965/1000] | Train Loss: 53379521.9200 | Val Loss: 36008696.2400\n",
      "Epoch [966/1000] | Train Loss: 57734391.2000 | Val Loss: 30340582.0800\n",
      "Epoch [967/1000] | Train Loss: 50339317.6700 | Val Loss: 66068658.7200\n",
      "Epoch [968/1000] | Train Loss: 67282119.2800 | Val Loss: 73086585.2800\n",
      "Epoch [969/1000] | Train Loss: 48873632.0000 | Val Loss: 33075329.1200\n",
      "Epoch [970/1000] | Train Loss: 59739185.8400 | Val Loss: 36597812.0000\n",
      "Epoch [971/1000] | Train Loss: 50466897.7400 | Val Loss: 45103497.1200\n",
      "Epoch [972/1000] | Train Loss: 68861767.0400 | Val Loss: 72326991.3600\n",
      "Epoch [973/1000] | Train Loss: 62581949.2800 | Val Loss: 40702094.0000\n",
      "Epoch [974/1000] | Train Loss: 54367322.0000 | Val Loss: 116752696.6400\n",
      "Epoch [975/1000] | Train Loss: 54551305.2800 | Val Loss: 58595492.4800\n",
      "Epoch [976/1000] | Train Loss: 41219439.0800 | Val Loss: 34782499.4400\n",
      "Epoch [977/1000] | Train Loss: 56457128.1600 | Val Loss: 45041148.3200\n",
      "Epoch [978/1000] | Train Loss: 53233486.5600 | Val Loss: 71972547.2000\n",
      "Epoch [979/1000] | Train Loss: 48021095.4400 | Val Loss: 36810115.1200\n",
      "Epoch [980/1000] | Train Loss: 48570825.7600 | Val Loss: 36954437.0400\n",
      "Epoch [981/1000] | Train Loss: 47475774.7200 | Val Loss: 65399030.2400\n",
      "Epoch [982/1000] | Train Loss: 49117160.8075 | Val Loss: 44476283.0400\n",
      "Epoch [983/1000] | Train Loss: 45810115.3600 | Val Loss: 42423204.8800\n",
      "Epoch [984/1000] | Train Loss: 62331454.4900 | Val Loss: 31422102.4000\n",
      "Epoch [985/1000] | Train Loss: 52241627.3600 | Val Loss: 49419557.6000\n",
      "Epoch [986/1000] | Train Loss: 50033730.3200 | Val Loss: 57113592.9600\n",
      "Epoch [987/1000] | Train Loss: 41487732.3100 | Val Loss: 44908157.1200\n",
      "Epoch [988/1000] | Train Loss: 52838666.1200 | Val Loss: 40816602.6400\n",
      "Epoch [989/1000] | Train Loss: 46546708.2600 | Val Loss: 38661381.6000\n",
      "Epoch [990/1000] | Train Loss: 49185546.2400 | Val Loss: 53503209.6000\n",
      "Epoch [991/1000] | Train Loss: 39327041.2800 | Val Loss: 39438163.1200\n",
      "Epoch [992/1000] | Train Loss: 51789661.6000 | Val Loss: 43346314.2400\n",
      "Epoch [993/1000] | Train Loss: 55301732.0000 | Val Loss: 60279833.9200\n",
      "Epoch [994/1000] | Train Loss: 47878574.9600 | Val Loss: 32514839.6800\n",
      "Epoch [995/1000] | Train Loss: 52080708.9600 | Val Loss: 42435481.6000\n",
      "Epoch [996/1000] | Train Loss: 42517874.8800 | Val Loss: 83024834.8800\n",
      "Epoch [997/1000] | Train Loss: 49593894.0550 | Val Loss: 65095680.3200\n",
      "Epoch [998/1000] | Train Loss: 42590302.0800 | Val Loss: 43791832.9600\n",
      "Epoch [999/1000] | Train Loss: 43721754.1675 | Val Loss: 34705250.9600\n",
      "Epoch [1000/1000] | Train Loss: 42879771.3200 | Val Loss: 60537706.0800\n",
      "../models/best_fcn.pt\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FCN -> MAE: 2558.4258, RMSE: 7780.5983, R2: 0.7834\n"
     ]
    }
   ],
   "source": [
    "# Model: FCN\n",
    "fcn_params = config[\"model\"][\"fcn\"]\n",
    "fcn_model = FCNModel(fcn_params)\n",
    "fcn_model.train(X_train, y_train, X_val, y_val)\n",
    "\n",
    "fcn_save_path = config[\"deployment\"][\"fcn_model_path\"]\n",
    "fcn_model.save_model(fcn_save_path)\n",
    "\n",
    "predictor_fcn = Predictor(model_type=\"fcn\")\n",
    "preds_fcn = predictor_fcn.predict(X_val)\n",
    "\n",
    "metrics_fcn = evaluate_regression(y_val, preds_fcn, model=\"fcn\", plot=True)\n",
    "results.append([\"FCN\", metrics_fcn[\"MAE\"], metrics_fcn[\"RMSE\"], metrics_fcn[\"R2\"]])\n",
    "\n",
    "print(f\"FCN -> MAE: {metrics_fcn['MAE']:.4f}, RMSE: {metrics_fcn['RMSE']:.4f}, R2: {metrics_fcn['R2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb9f010c-bca9-48d3-a986-0ee66e8e27e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/best_xgboost.pkl\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost -> MAE: 4093.8741, RMSE: 12740.7058, R2: 0.4193\n"
     ]
    }
   ],
   "source": [
    "# Model: XGBoost\n",
    "xgb_params = config[\"model\"][\"xgboost\"]\n",
    "xgb_model = XGBoostModel(xgb_params)\n",
    "xgb_model.train(X_train, y_train, X_val, y_val)\n",
    "\n",
    "xgb_save_path = config[\"deployment\"][\"xgboost_model_path\"]\n",
    "xgb_model.save_model(xgb_save_path)\n",
    "\n",
    "predictor_xgb = Predictor(model_type=\"xgboost\")\n",
    "preds_xgb = predictor_xgb.predict(X_val)\n",
    "\n",
    "metrics_xgb = evaluate_regression(y_val, preds_xgb, model=\"xgboost\", plot=True)\n",
    "results.append([\"XGBoost\", metrics_xgb[\"MAE\"], metrics_xgb[\"RMSE\"], metrics_xgb[\"R2\"]])\n",
    "\n",
    "print(f\"XGBoost -> MAE: {metrics_xgb['MAE']:.4f}, RMSE: {metrics_xgb['RMSE']:.4f}, R2: {metrics_xgb['R2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db3d69bb-1702-4695-9b79-09b10657471e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000] | Train Loss: 153324655.6800 | Val Loss: 297061388.8000\n",
      "Epoch [2/1000] | Train Loss: 151083113.6800 | Val Loss: 291167390.7200\n",
      "Epoch [3/1000] | Train Loss: 146524732.4000 | Val Loss: 284104216.3200\n",
      "Epoch [4/1000] | Train Loss: 144112679.9600 | Val Loss: 281589392.6400\n",
      "Epoch [5/1000] | Train Loss: 144458894.6800 | Val Loss: 281713802.2400\n",
      "Epoch [6/1000] | Train Loss: 144317872.7800 | Val Loss: 282741999.3600\n",
      "Epoch [7/1000] | Train Loss: 144343330.7400 | Val Loss: 283412048.6400\n",
      "Epoch [8/1000] | Train Loss: 144304253.4000 | Val Loss: 282736930.5600\n",
      "Epoch [9/1000] | Train Loss: 144224821.9000 | Val Loss: 282854796.8000\n",
      "Epoch [10/1000] | Train Loss: 144247087.3200 | Val Loss: 282726260.4800\n",
      "Epoch [11/1000] | Train Loss: 144122074.4400 | Val Loss: 282315164.1600\n",
      "Epoch [12/1000] | Train Loss: 144005157.1200 | Val Loss: 282037806.0800\n",
      "Epoch [13/1000] | Train Loss: 143978752.4800 | Val Loss: 281894388.4800\n",
      "Epoch [14/1000] | Train Loss: 144032606.4800 | Val Loss: 281916407.0400\n",
      "Epoch [15/1000] | Train Loss: 144056983.7600 | Val Loss: 281887585.2800\n",
      "Epoch [16/1000] | Train Loss: 144333852.8800 | Val Loss: 280606608.6400\n",
      "Epoch [17/1000] | Train Loss: 144617910.7200 | Val Loss: 281729747.2000\n",
      "Epoch [18/1000] | Train Loss: 143975167.2600 | Val Loss: 280827600.6400\n",
      "Epoch [19/1000] | Train Loss: 143846995.1200 | Val Loss: 281177950.7200\n",
      "Epoch [20/1000] | Train Loss: 144098532.4000 | Val Loss: 282117304.3200\n",
      "Epoch [21/1000] | Train Loss: 143888739.2000 | Val Loss: 281424337.9200\n",
      "Epoch [22/1000] | Train Loss: 143780103.9600 | Val Loss: 281417387.5200\n",
      "Epoch [23/1000] | Train Loss: 143799114.7400 | Val Loss: 281071617.2800\n",
      "Epoch [24/1000] | Train Loss: 143682066.0800 | Val Loss: 281542754.5600\n",
      "Epoch [25/1000] | Train Loss: 143728588.4400 | Val Loss: 281352929.2800\n",
      "Epoch [26/1000] | Train Loss: 143660377.9200 | Val Loss: 281078519.0400\n",
      "Epoch [27/1000] | Train Loss: 143270540.4800 | Val Loss: 279649126.4000\n",
      "Epoch [28/1000] | Train Loss: 143869532.0000 | Val Loss: 279099502.0800\n",
      "Epoch [29/1000] | Train Loss: 144657721.5600 | Val Loss: 278926754.5600\n",
      "Epoch [30/1000] | Train Loss: 143807636.6600 | Val Loss: 280144861.4400\n",
      "Epoch [31/1000] | Train Loss: 143564565.4000 | Val Loss: 282004014.0800\n",
      "Epoch [32/1000] | Train Loss: 143885436.6400 | Val Loss: 281745635.8400\n",
      "Epoch [33/1000] | Train Loss: 143031867.8200 | Val Loss: 279474805.7600\n",
      "Epoch [34/1000] | Train Loss: 143274397.6000 | Val Loss: 279084682.2400\n",
      "Epoch [35/1000] | Train Loss: 143330971.8400 | Val Loss: 279298636.8000\n",
      "Epoch [36/1000] | Train Loss: 143300278.4400 | Val Loss: 278860906.2400\n",
      "Epoch [37/1000] | Train Loss: 143270600.8400 | Val Loss: 278803392.0000\n",
      "Epoch [38/1000] | Train Loss: 142920133.2000 | Val Loss: 279492160.0000\n",
      "Epoch [39/1000] | Train Loss: 143074091.6600 | Val Loss: 280579980.8000\n",
      "Epoch [40/1000] | Train Loss: 143229928.4800 | Val Loss: 279982222.0800\n",
      "Epoch [41/1000] | Train Loss: 143047956.2200 | Val Loss: 279303143.6800\n",
      "Epoch [42/1000] | Train Loss: 142842013.9000 | Val Loss: 278699960.3200\n",
      "Epoch [43/1000] | Train Loss: 142963840.8000 | Val Loss: 278616043.5200\n",
      "Epoch [44/1000] | Train Loss: 142762524.4800 | Val Loss: 278296403.2000\n",
      "Epoch [45/1000] | Train Loss: 142697610.4000 | Val Loss: 278472544.0000\n",
      "Epoch [46/1000] | Train Loss: 142658252.1200 | Val Loss: 278147101.4400\n",
      "Epoch [47/1000] | Train Loss: 142793812.2000 | Val Loss: 277684122.8800\n",
      "Epoch [48/1000] | Train Loss: 142464536.0000 | Val Loss: 278031376.6400\n",
      "Epoch [49/1000] | Train Loss: 142574720.6400 | Val Loss: 278066629.1200\n",
      "Epoch [50/1000] | Train Loss: 142413200.7600 | Val Loss: 276203472.6400\n",
      "Epoch [51/1000] | Train Loss: 142680896.4400 | Val Loss: 276253553.9200\n",
      "Epoch [52/1000] | Train Loss: 142184807.6000 | Val Loss: 277526693.1200\n",
      "Epoch [53/1000] | Train Loss: 142297884.6400 | Val Loss: 277619208.9600\n",
      "Epoch [54/1000] | Train Loss: 142258873.9200 | Val Loss: 276117507.8400\n",
      "Epoch [55/1000] | Train Loss: 141943928.8800 | Val Loss: 275900613.1200\n",
      "Epoch [56/1000] | Train Loss: 142292213.4200 | Val Loss: 276702562.5600\n",
      "Epoch [57/1000] | Train Loss: 141880579.8400 | Val Loss: 275243279.3600\n",
      "Epoch [58/1000] | Train Loss: 141948452.3200 | Val Loss: 273975088.6400\n",
      "Epoch [59/1000] | Train Loss: 142287646.1600 | Val Loss: 273926786.5600\n",
      "Epoch [60/1000] | Train Loss: 141420234.3400 | Val Loss: 276415690.2400\n",
      "Epoch [61/1000] | Train Loss: 142080087.1000 | Val Loss: 275832631.0400\n",
      "Epoch [62/1000] | Train Loss: 141575799.4400 | Val Loss: 275245530.8800\n",
      "Epoch [63/1000] | Train Loss: 141220377.2800 | Val Loss: 273550976.0000\n",
      "Epoch [64/1000] | Train Loss: 141357722.8800 | Val Loss: 272470373.1200\n",
      "Epoch [65/1000] | Train Loss: 141348718.3600 | Val Loss: 273757064.9600\n",
      "Epoch [66/1000] | Train Loss: 141048359.3200 | Val Loss: 274917637.1200\n",
      "Epoch [67/1000] | Train Loss: 141558850.7700 | Val Loss: 274744776.9600\n",
      "Epoch [68/1000] | Train Loss: 141034046.7200 | Val Loss: 272565776.6400\n",
      "Epoch [69/1000] | Train Loss: 140686171.4400 | Val Loss: 272572337.9200\n",
      "Epoch [70/1000] | Train Loss: 140673585.4200 | Val Loss: 272282807.0400\n",
      "Epoch [71/1000] | Train Loss: 140529368.4400 | Val Loss: 272678415.3600\n",
      "Epoch [72/1000] | Train Loss: 140806840.6200 | Val Loss: 273136624.6400\n",
      "Epoch [73/1000] | Train Loss: 140562246.6400 | Val Loss: 271932230.4000\n",
      "Epoch [74/1000] | Train Loss: 140379753.6000 | Val Loss: 271289584.6400\n",
      "Epoch [75/1000] | Train Loss: 140408055.7600 | Val Loss: 269739824.6400\n",
      "Epoch [76/1000] | Train Loss: 140349109.2800 | Val Loss: 271194188.8000\n",
      "Epoch [77/1000] | Train Loss: 140439376.2400 | Val Loss: 272449263.3600\n",
      "Epoch [78/1000] | Train Loss: 140509000.8800 | Val Loss: 271527929.6000\n",
      "Epoch [79/1000] | Train Loss: 139719962.7200 | Val Loss: 269584034.5600\n",
      "Epoch [80/1000] | Train Loss: 140366150.1600 | Val Loss: 268723430.4000\n",
      "Epoch [81/1000] | Train Loss: 139710916.6400 | Val Loss: 269324693.7600\n",
      "Epoch [82/1000] | Train Loss: 139744723.1000 | Val Loss: 271362645.7600\n",
      "Epoch [83/1000] | Train Loss: 140177470.4000 | Val Loss: 270506501.1200\n",
      "Epoch [84/1000] | Train Loss: 139112766.0800 | Val Loss: 267550199.0400\n",
      "Epoch [85/1000] | Train Loss: 139732310.1400 | Val Loss: 268256657.9200\n",
      "Epoch [86/1000] | Train Loss: 139251394.9400 | Val Loss: 268560613.1200\n",
      "Epoch [87/1000] | Train Loss: 139430327.0400 | Val Loss: 269121244.1600\n",
      "Epoch [88/1000] | Train Loss: 139166265.6000 | Val Loss: 268230183.6800\n",
      "Epoch [89/1000] | Train Loss: 138862028.0800 | Val Loss: 267416871.6800\n",
      "Epoch [90/1000] | Train Loss: 138760323.4400 | Val Loss: 267317054.7200\n",
      "Epoch [91/1000] | Train Loss: 138638296.9800 | Val Loss: 267304820.4800\n",
      "Epoch [92/1000] | Train Loss: 138530890.5800 | Val Loss: 267327093.7600\n",
      "Epoch [93/1000] | Train Loss: 138631294.2400 | Val Loss: 267703166.7200\n",
      "Epoch [94/1000] | Train Loss: 138711587.3600 | Val Loss: 264743252.4800\n",
      "Epoch [95/1000] | Train Loss: 138591659.4000 | Val Loss: 264519432.9600\n",
      "Epoch [96/1000] | Train Loss: 137878635.8800 | Val Loss: 266083046.4000\n",
      "Epoch [97/1000] | Train Loss: 137981967.9200 | Val Loss: 267305020.1600\n",
      "Epoch [98/1000] | Train Loss: 138436324.0000 | Val Loss: 266850850.5600\n",
      "Epoch [99/1000] | Train Loss: 137262208.1600 | Val Loss: 263617678.0800\n",
      "Epoch [100/1000] | Train Loss: 137798684.4400 | Val Loss: 263366918.4000\n",
      "Epoch [101/1000] | Train Loss: 137557039.2000 | Val Loss: 263109224.9600\n",
      "Epoch [102/1000] | Train Loss: 137646956.1600 | Val Loss: 261733400.3200\n",
      "Epoch [103/1000] | Train Loss: 138255200.6400 | Val Loss: 260704636.1600\n",
      "Epoch [104/1000] | Train Loss: 137169694.7200 | Val Loss: 261864428.8000\n",
      "Epoch [105/1000] | Train Loss: 137379026.5400 | Val Loss: 264757230.0800\n",
      "Epoch [106/1000] | Train Loss: 137146639.4400 | Val Loss: 262903566.0800\n",
      "Epoch [107/1000] | Train Loss: 136967003.8800 | Val Loss: 259974958.0800\n",
      "Epoch [108/1000] | Train Loss: 136382383.0400 | Val Loss: 260433231.3600\n",
      "Epoch [109/1000] | Train Loss: 136386784.2400 | Val Loss: 260094922.2400\n",
      "Epoch [110/1000] | Train Loss: 136150564.4800 | Val Loss: 259192463.3600\n",
      "Epoch [111/1000] | Train Loss: 135892373.6800 | Val Loss: 259828730.8800\n",
      "Epoch [112/1000] | Train Loss: 136145896.6400 | Val Loss: 260454867.2000\n",
      "Epoch [113/1000] | Train Loss: 135769284.7600 | Val Loss: 259107363.8400\n",
      "Epoch [114/1000] | Train Loss: 135642890.1600 | Val Loss: 258789241.6000\n",
      "Epoch [115/1000] | Train Loss: 135538618.0600 | Val Loss: 258457574.4000\n",
      "Epoch [116/1000] | Train Loss: 135706356.3200 | Val Loss: 258584997.1200\n",
      "Epoch [117/1000] | Train Loss: 135140087.3600 | Val Loss: 256670700.8000\n",
      "Epoch [118/1000] | Train Loss: 135896077.5200 | Val Loss: 258324085.7600\n",
      "Epoch [119/1000] | Train Loss: 135142422.4000 | Val Loss: 257026877.4400\n",
      "Epoch [120/1000] | Train Loss: 135020818.5600 | Val Loss: 255441986.5600\n",
      "Epoch [121/1000] | Train Loss: 135342191.4400 | Val Loss: 254621967.3600\n",
      "Epoch [122/1000] | Train Loss: 134551347.6400 | Val Loss: 256162767.3600\n",
      "Epoch [123/1000] | Train Loss: 134807272.6000 | Val Loss: 257548917.7600\n",
      "Epoch [124/1000] | Train Loss: 134493072.6400 | Val Loss: 254863916.8000\n",
      "Epoch [125/1000] | Train Loss: 134233168.0800 | Val Loss: 252901446.4000\n",
      "Epoch [126/1000] | Train Loss: 135024807.6000 | Val Loss: 253551605.7600\n",
      "Epoch [127/1000] | Train Loss: 133920447.2000 | Val Loss: 256135622.4000\n",
      "Epoch [128/1000] | Train Loss: 134179919.6000 | Val Loss: 255830109.4400\n",
      "Epoch [129/1000] | Train Loss: 133452418.7200 | Val Loss: 253239019.5200\n",
      "Epoch [130/1000] | Train Loss: 133658899.6400 | Val Loss: 251834289.9200\n",
      "Epoch [131/1000] | Train Loss: 133133326.0800 | Val Loss: 253311046.4000\n",
      "Epoch [132/1000] | Train Loss: 133522206.3600 | Val Loss: 253819863.0400\n",
      "Epoch [133/1000] | Train Loss: 133031377.0400 | Val Loss: 251612316.1600\n",
      "Epoch [134/1000] | Train Loss: 133084074.1600 | Val Loss: 250723014.4000\n",
      "Epoch [135/1000] | Train Loss: 132609939.0400 | Val Loss: 251713141.7600\n",
      "Epoch [136/1000] | Train Loss: 131900125.2400 | Val Loss: 249931016.9600\n",
      "Epoch [137/1000] | Train Loss: 132265168.8000 | Val Loss: 249682583.0400\n",
      "Epoch [138/1000] | Train Loss: 131999708.9600 | Val Loss: 249808880.6400\n",
      "Epoch [139/1000] | Train Loss: 131704922.6000 | Val Loss: 249518586.8800\n",
      "Epoch [140/1000] | Train Loss: 131734748.2400 | Val Loss: 250453621.7600\n",
      "Epoch [141/1000] | Train Loss: 131511414.7200 | Val Loss: 249321437.4400\n",
      "Epoch [142/1000] | Train Loss: 131722449.9200 | Val Loss: 247043997.4400\n",
      "Epoch [143/1000] | Train Loss: 132113839.9200 | Val Loss: 246381656.3200\n",
      "Epoch [144/1000] | Train Loss: 130712278.8000 | Val Loss: 248410908.1600\n",
      "Epoch [145/1000] | Train Loss: 130811648.1600 | Val Loss: 247431302.4000\n",
      "Epoch [146/1000] | Train Loss: 130027372.6800 | Val Loss: 246630328.3200\n",
      "Epoch [147/1000] | Train Loss: 129745851.4800 | Val Loss: 245887146.2400\n",
      "Epoch [148/1000] | Train Loss: 129436160.8400 | Val Loss: 245939645.4400\n",
      "Epoch [149/1000] | Train Loss: 129213468.0800 | Val Loss: 245590993.9200\n",
      "Epoch [150/1000] | Train Loss: 129162888.3700 | Val Loss: 245191404.8000\n",
      "Epoch [151/1000] | Train Loss: 128941144.8400 | Val Loss: 244438067.2000\n",
      "Epoch [152/1000] | Train Loss: 128571403.8800 | Val Loss: 243672709.1200\n",
      "Epoch [153/1000] | Train Loss: 128212250.0000 | Val Loss: 244086046.7200\n",
      "Epoch [154/1000] | Train Loss: 128185459.3200 | Val Loss: 243062502.4000\n",
      "Epoch [155/1000] | Train Loss: 127778869.0800 | Val Loss: 242928049.9200\n",
      "Epoch [156/1000] | Train Loss: 127442317.6400 | Val Loss: 242008160.0000\n",
      "Epoch [157/1000] | Train Loss: 127087232.7000 | Val Loss: 241794785.2800\n",
      "Epoch [158/1000] | Train Loss: 126820414.2400 | Val Loss: 241423176.9600\n",
      "Epoch [159/1000] | Train Loss: 126479032.8800 | Val Loss: 240592792.3200\n",
      "Epoch [160/1000] | Train Loss: 126274219.2000 | Val Loss: 239549030.4000\n",
      "Epoch [161/1000] | Train Loss: 125996436.8800 | Val Loss: 238670300.1600\n",
      "Epoch [162/1000] | Train Loss: 125620885.3000 | Val Loss: 239390301.4400\n",
      "Epoch [163/1000] | Train Loss: 125256276.6800 | Val Loss: 238213674.2400\n",
      "Epoch [164/1000] | Train Loss: 124725116.2000 | Val Loss: 237765889.2800\n",
      "Epoch [165/1000] | Train Loss: 124284134.1600 | Val Loss: 236999339.5200\n",
      "Epoch [166/1000] | Train Loss: 124176037.6800 | Val Loss: 235969511.6800\n",
      "Epoch [167/1000] | Train Loss: 123620123.7200 | Val Loss: 235704806.4000\n",
      "Epoch [168/1000] | Train Loss: 123171208.4000 | Val Loss: 235499120.6400\n",
      "Epoch [169/1000] | Train Loss: 122992214.9600 | Val Loss: 235267978.2400\n",
      "Epoch [170/1000] | Train Loss: 122816346.8000 | Val Loss: 233715772.1600\n",
      "Epoch [171/1000] | Train Loss: 121874915.8000 | Val Loss: 233009696.0000\n",
      "Epoch [172/1000] | Train Loss: 121497890.0200 | Val Loss: 232895979.5200\n",
      "Epoch [173/1000] | Train Loss: 121471213.0200 | Val Loss: 232262624.0000\n",
      "Epoch [174/1000] | Train Loss: 120825159.7600 | Val Loss: 230141103.3600\n",
      "Epoch [175/1000] | Train Loss: 120721946.3200 | Val Loss: 228206790.4000\n",
      "Epoch [176/1000] | Train Loss: 119788032.7000 | Val Loss: 227978595.8400\n",
      "Epoch [177/1000] | Train Loss: 119479725.0800 | Val Loss: 227530757.1200\n",
      "Epoch [178/1000] | Train Loss: 118818430.6800 | Val Loss: 226416673.2800\n",
      "Epoch [179/1000] | Train Loss: 118549404.1000 | Val Loss: 226205052.1600\n",
      "Epoch [180/1000] | Train Loss: 118056679.8400 | Val Loss: 224422908.1600\n",
      "Epoch [181/1000] | Train Loss: 117475985.2800 | Val Loss: 223213391.3600\n",
      "Epoch [182/1000] | Train Loss: 116898226.9200 | Val Loss: 222024062.7200\n",
      "Epoch [183/1000] | Train Loss: 116213133.6000 | Val Loss: 220741559.0400\n",
      "Epoch [184/1000] | Train Loss: 116009619.8200 | Val Loss: 221627692.8000\n",
      "Epoch [185/1000] | Train Loss: 115341908.5200 | Val Loss: 219212721.9200\n",
      "Epoch [186/1000] | Train Loss: 114939655.1600 | Val Loss: 217741546.2400\n",
      "Epoch [187/1000] | Train Loss: 114216151.6000 | Val Loss: 216940733.4400\n",
      "Epoch [188/1000] | Train Loss: 113579272.1600 | Val Loss: 215041158.4000\n",
      "Epoch [189/1000] | Train Loss: 113208336.2000 | Val Loss: 212866233.6000\n",
      "Epoch [190/1000] | Train Loss: 112363314.4800 | Val Loss: 212219377.9200\n",
      "Epoch [191/1000] | Train Loss: 111995682.9600 | Val Loss: 211985329.9200\n",
      "Epoch [192/1000] | Train Loss: 111716547.0600 | Val Loss: 212337178.8800\n",
      "Epoch [193/1000] | Train Loss: 110990525.2000 | Val Loss: 210231057.9200\n",
      "Epoch [194/1000] | Train Loss: 110578929.1200 | Val Loss: 209185244.1600\n",
      "Epoch [195/1000] | Train Loss: 109579147.8400 | Val Loss: 205909196.8000\n",
      "Epoch [196/1000] | Train Loss: 110125484.3200 | Val Loss: 202031164.1600\n",
      "Epoch [197/1000] | Train Loss: 109214508.9600 | Val Loss: 201892005.1200\n",
      "Epoch [198/1000] | Train Loss: 107763992.4400 | Val Loss: 203364700.1600\n",
      "Epoch [199/1000] | Train Loss: 108245353.6000 | Val Loss: 203844561.9200\n",
      "Epoch [200/1000] | Train Loss: 108150547.9200 | Val Loss: 197556430.0800\n",
      "Epoch [201/1000] | Train Loss: 106604461.2000 | Val Loss: 197690219.5200\n",
      "Epoch [202/1000] | Train Loss: 106146623.9600 | Val Loss: 196400450.5600\n",
      "Epoch [203/1000] | Train Loss: 105959523.5200 | Val Loss: 197129145.6000\n",
      "Epoch [204/1000] | Train Loss: 105133242.8000 | Val Loss: 193935261.4400\n",
      "Epoch [205/1000] | Train Loss: 104661400.9600 | Val Loss: 192909107.2000\n",
      "Epoch [206/1000] | Train Loss: 103997180.0000 | Val Loss: 193280618.2400\n",
      "Epoch [207/1000] | Train Loss: 103744963.5200 | Val Loss: 189562442.2400\n",
      "Epoch [208/1000] | Train Loss: 103482580.2400 | Val Loss: 189897766.4000\n",
      "Epoch [209/1000] | Train Loss: 103296772.4200 | Val Loss: 188453986.5600\n",
      "Epoch [210/1000] | Train Loss: 103067679.0000 | Val Loss: 186040512.0000\n",
      "Epoch [211/1000] | Train Loss: 101878272.6000 | Val Loss: 186230448.6400\n",
      "Epoch [212/1000] | Train Loss: 101414820.3000 | Val Loss: 186233308.1600\n",
      "Epoch [213/1000] | Train Loss: 101258368.8000 | Val Loss: 184813153.2800\n",
      "Epoch [214/1000] | Train Loss: 101111424.1600 | Val Loss: 181843272.9600\n",
      "Epoch [215/1000] | Train Loss: 100984571.5200 | Val Loss: 179372995.8400\n",
      "Epoch [216/1000] | Train Loss: 101356200.7200 | Val Loss: 175101102.7200\n",
      "Epoch [217/1000] | Train Loss: 100580510.2000 | Val Loss: 175486460.8000\n",
      "Epoch [218/1000] | Train Loss: 99738762.7200 | Val Loss: 180610270.7200\n",
      "Epoch [219/1000] | Train Loss: 99945460.7200 | Val Loss: 180030627.8400\n",
      "Epoch [220/1000] | Train Loss: 99167974.6000 | Val Loss: 175705953.2800\n",
      "Epoch [221/1000] | Train Loss: 98939490.0400 | Val Loss: 175259551.3600\n",
      "Epoch [222/1000] | Train Loss: 98213105.4400 | Val Loss: 173479404.8000\n",
      "Epoch [223/1000] | Train Loss: 97855892.5600 | Val Loss: 171967585.9200\n",
      "Epoch [224/1000] | Train Loss: 98272681.4600 | Val Loss: 169809592.9600\n",
      "Epoch [225/1000] | Train Loss: 97187042.2400 | Val Loss: 170297887.3600\n",
      "Epoch [226/1000] | Train Loss: 97190767.6800 | Val Loss: 166793329.9200\n",
      "Epoch [227/1000] | Train Loss: 97992484.0000 | Val Loss: 164033347.8400\n",
      "Epoch [228/1000] | Train Loss: 96426854.0000 | Val Loss: 166759587.2000\n",
      "Epoch [229/1000] | Train Loss: 97819416.4000 | Val Loss: 168468392.9600\n",
      "Epoch [230/1000] | Train Loss: 95660523.8400 | Val Loss: 163552476.8000\n",
      "Epoch [231/1000] | Train Loss: 95678069.7600 | Val Loss: 161646672.6400\n",
      "Epoch [232/1000] | Train Loss: 95387534.6000 | Val Loss: 159587585.9200\n",
      "Epoch [233/1000] | Train Loss: 95625063.6800 | Val Loss: 160207756.8000\n",
      "Epoch [234/1000] | Train Loss: 95784608.4800 | Val Loss: 163937986.5600\n",
      "Epoch [235/1000] | Train Loss: 94949718.7200 | Val Loss: 159438417.2800\n",
      "Epoch [236/1000] | Train Loss: 94410621.3600 | Val Loss: 160662124.8000\n",
      "Epoch [237/1000] | Train Loss: 94897337.6000 | Val Loss: 157465351.0400\n",
      "Epoch [238/1000] | Train Loss: 94144739.6000 | Val Loss: 159440476.8000\n",
      "Epoch [239/1000] | Train Loss: 94857166.7800 | Val Loss: 160572158.0800\n",
      "Epoch [240/1000] | Train Loss: 94663512.5200 | Val Loss: 157616263.6800\n",
      "Epoch [241/1000] | Train Loss: 94666663.6800 | Val Loss: 156494494.0800\n",
      "Epoch [242/1000] | Train Loss: 94073804.6400 | Val Loss: 159888177.2800\n",
      "Epoch [243/1000] | Train Loss: 94148033.9200 | Val Loss: 157463150.7200\n",
      "Epoch [244/1000] | Train Loss: 94143478.4800 | Val Loss: 154941357.4400\n",
      "Epoch [245/1000] | Train Loss: 93753246.8300 | Val Loss: 159044547.8400\n",
      "Epoch [246/1000] | Train Loss: 94094962.2800 | Val Loss: 155603354.2400\n",
      "Epoch [247/1000] | Train Loss: 93283210.8800 | Val Loss: 155964833.9200\n",
      "Epoch [248/1000] | Train Loss: 93190708.5600 | Val Loss: 157362899.8400\n",
      "Epoch [249/1000] | Train Loss: 93703345.3600 | Val Loss: 156296554.8800\n",
      "Epoch [250/1000] | Train Loss: 93005860.6400 | Val Loss: 152758750.7200\n",
      "Epoch [251/1000] | Train Loss: 94012348.4800 | Val Loss: 150895540.4800\n",
      "Epoch [252/1000] | Train Loss: 94111177.8400 | Val Loss: 154403107.2000\n",
      "Epoch [253/1000] | Train Loss: 92644945.2000 | Val Loss: 156263497.6000\n",
      "Epoch [254/1000] | Train Loss: 93308580.1600 | Val Loss: 156846167.0400\n",
      "Epoch [255/1000] | Train Loss: 93412463.5200 | Val Loss: 155385540.4800\n",
      "Epoch [256/1000] | Train Loss: 93206009.3600 | Val Loss: 153197221.1200\n",
      "Epoch [257/1000] | Train Loss: 92975560.4800 | Val Loss: 151745805.4400\n",
      "Epoch [258/1000] | Train Loss: 92419240.0000 | Val Loss: 151981780.4800\n",
      "Epoch [259/1000] | Train Loss: 92864892.5600 | Val Loss: 152814293.7600\n",
      "Epoch [260/1000] | Train Loss: 92423737.2800 | Val Loss: 152840852.4800\n",
      "Epoch [261/1000] | Train Loss: 92806468.2400 | Val Loss: 149894052.4800\n",
      "Epoch [262/1000] | Train Loss: 91704217.6000 | Val Loss: 148662549.1200\n",
      "Epoch [263/1000] | Train Loss: 92881323.9800 | Val Loss: 147591554.5600\n",
      "Epoch [264/1000] | Train Loss: 91776400.5600 | Val Loss: 148925588.4800\n",
      "Epoch [265/1000] | Train Loss: 92156297.3600 | Val Loss: 146587088.0000\n",
      "Epoch [266/1000] | Train Loss: 91225489.0400 | Val Loss: 146200051.2000\n",
      "Epoch [267/1000] | Train Loss: 91815284.9000 | Val Loss: 145762596.4800\n",
      "Epoch [268/1000] | Train Loss: 91253750.7200 | Val Loss: 145588201.6000\n",
      "Epoch [269/1000] | Train Loss: 91974378.6400 | Val Loss: 141170469.1200\n",
      "Epoch [270/1000] | Train Loss: 93099318.5600 | Val Loss: 141915055.3600\n",
      "Epoch [271/1000] | Train Loss: 91709601.2000 | Val Loss: 139296581.7600\n",
      "Epoch [272/1000] | Train Loss: 91527519.6800 | Val Loss: 142316459.5200\n",
      "Epoch [273/1000] | Train Loss: 91090498.0800 | Val Loss: 142664878.7200\n",
      "Epoch [274/1000] | Train Loss: 91215720.0000 | Val Loss: 141592106.2400\n",
      "Epoch [275/1000] | Train Loss: 91555873.8400 | Val Loss: 143426449.9200\n",
      "Epoch [276/1000] | Train Loss: 90675878.6400 | Val Loss: 141950231.0400\n",
      "Epoch [277/1000] | Train Loss: 90843012.4800 | Val Loss: 140917589.7600\n",
      "Epoch [278/1000] | Train Loss: 90732502.7200 | Val Loss: 140039485.4400\n",
      "Epoch [279/1000] | Train Loss: 90905379.9200 | Val Loss: 143628250.2400\n",
      "Epoch [280/1000] | Train Loss: 90613020.0400 | Val Loss: 141265878.4000\n",
      "Epoch [281/1000] | Train Loss: 90890037.9200 | Val Loss: 140777618.5600\n",
      "Epoch [282/1000] | Train Loss: 91480084.4200 | Val Loss: 138066958.0800\n",
      "Epoch [283/1000] | Train Loss: 91716724.3200 | Val Loss: 139121036.8000\n",
      "Epoch [284/1000] | Train Loss: 90322442.5600 | Val Loss: 139880610.5600\n",
      "Epoch [285/1000] | Train Loss: 91027449.9200 | Val Loss: 137489015.0400\n",
      "Epoch [286/1000] | Train Loss: 90208071.8400 | Val Loss: 138888319.3600\n",
      "Epoch [287/1000] | Train Loss: 90313155.5200 | Val Loss: 137106270.7200\n",
      "Epoch [288/1000] | Train Loss: 90579650.5200 | Val Loss: 136666359.0400\n",
      "Epoch [289/1000] | Train Loss: 90634290.2400 | Val Loss: 136515733.7600\n",
      "Epoch [290/1000] | Train Loss: 90827743.1200 | Val Loss: 134928219.5200\n",
      "Epoch [291/1000] | Train Loss: 89936906.0800 | Val Loss: 137563874.5600\n",
      "Epoch [292/1000] | Train Loss: 90562237.4400 | Val Loss: 136928803.2000\n",
      "Epoch [293/1000] | Train Loss: 91425929.1200 | Val Loss: 137997500.1600\n",
      "Epoch [294/1000] | Train Loss: 90249379.0400 | Val Loss: 134282832.0000\n",
      "Epoch [295/1000] | Train Loss: 90804204.2800 | Val Loss: 135278501.7600\n",
      "Epoch [296/1000] | Train Loss: 89926891.2000 | Val Loss: 135385293.4400\n",
      "Epoch [297/1000] | Train Loss: 89863993.6000 | Val Loss: 133474894.7200\n",
      "Epoch [298/1000] | Train Loss: 90971063.5200 | Val Loss: 133894377.6000\n",
      "Epoch [299/1000] | Train Loss: 91422057.2800 | Val Loss: 132014588.1600\n",
      "Epoch [300/1000] | Train Loss: 89903866.8600 | Val Loss: 132689920.0000\n",
      "Epoch [301/1000] | Train Loss: 89774245.4400 | Val Loss: 134027702.4000\n",
      "Epoch [302/1000] | Train Loss: 88614990.6400 | Val Loss: 132547494.4000\n",
      "Epoch [303/1000] | Train Loss: 90642496.8000 | Val Loss: 134091989.1200\n",
      "Epoch [304/1000] | Train Loss: 90939245.9200 | Val Loss: 131755472.6400\n",
      "Epoch [305/1000] | Train Loss: 89701086.2400 | Val Loss: 136810977.9200\n",
      "Epoch [306/1000] | Train Loss: 90784407.6800 | Val Loss: 133416846.7200\n",
      "Epoch [307/1000] | Train Loss: 89843231.1600 | Val Loss: 133404448.0000\n",
      "Epoch [308/1000] | Train Loss: 91407727.0400 | Val Loss: 135750391.6800\n",
      "Epoch [309/1000] | Train Loss: 89698160.3200 | Val Loss: 129662617.6000\n",
      "Epoch [310/1000] | Train Loss: 90113776.3200 | Val Loss: 132054074.8800\n",
      "Epoch [311/1000] | Train Loss: 89412994.7200 | Val Loss: 131689504.0000\n",
      "Epoch [312/1000] | Train Loss: 89540884.3200 | Val Loss: 133522897.9200\n",
      "Epoch [313/1000] | Train Loss: 90481730.7600 | Val Loss: 133295362.5600\n",
      "Epoch [314/1000] | Train Loss: 89609827.4400 | Val Loss: 131455912.9600\n",
      "Epoch [315/1000] | Train Loss: 89156943.0400 | Val Loss: 130834788.4800\n",
      "Epoch [316/1000] | Train Loss: 89216896.8000 | Val Loss: 129211050.2400\n",
      "Epoch [317/1000] | Train Loss: 90713523.0400 | Val Loss: 127725360.6400\n",
      "Epoch [318/1000] | Train Loss: 89858514.2400 | Val Loss: 132618266.2400\n",
      "Epoch [319/1000] | Train Loss: 89722596.4800 | Val Loss: 132371347.2000\n",
      "Epoch [320/1000] | Train Loss: 89369465.4000 | Val Loss: 132433200.6400\n",
      "Epoch [321/1000] | Train Loss: 89334232.2800 | Val Loss: 129613608.3200\n",
      "Epoch [322/1000] | Train Loss: 89666032.8000 | Val Loss: 129368984.9600\n",
      "Epoch [323/1000] | Train Loss: 89892656.7200 | Val Loss: 132148981.1200\n",
      "Epoch [324/1000] | Train Loss: 89404179.2400 | Val Loss: 131745370.8800\n",
      "Epoch [325/1000] | Train Loss: 89177704.0000 | Val Loss: 130452408.3200\n",
      "Epoch [326/1000] | Train Loss: 88987893.7600 | Val Loss: 131903242.8800\n",
      "Epoch [327/1000] | Train Loss: 89627722.6200 | Val Loss: 130078451.2000\n",
      "Epoch [328/1000] | Train Loss: 89758327.0400 | Val Loss: 130505985.9200\n",
      "Epoch [329/1000] | Train Loss: 89375255.7600 | Val Loss: 128938559.3600\n",
      "Epoch [330/1000] | Train Loss: 90532429.7600 | Val Loss: 130254951.0400\n",
      "Epoch [331/1000] | Train Loss: 88888397.6000 | Val Loss: 131329765.7600\n",
      "Epoch [332/1000] | Train Loss: 89570260.1600 | Val Loss: 133066327.6800\n",
      "Epoch [333/1000] | Train Loss: 89347803.3600 | Val Loss: 131707114.8800\n",
      "Epoch [334/1000] | Train Loss: 89795769.5800 | Val Loss: 132085784.9600\n",
      "Epoch [335/1000] | Train Loss: 89248730.7200 | Val Loss: 130912931.2000\n",
      "Epoch [336/1000] | Train Loss: 88295018.0800 | Val Loss: 129776673.9200\n",
      "Epoch [337/1000] | Train Loss: 88391118.4800 | Val Loss: 130108741.1200\n",
      "Epoch [338/1000] | Train Loss: 88631933.6400 | Val Loss: 131415706.8800\n",
      "Epoch [339/1000] | Train Loss: 89392750.4000 | Val Loss: 130547375.3600\n",
      "Epoch [340/1000] | Train Loss: 89604830.7600 | Val Loss: 127472686.7200\n",
      "Epoch [341/1000] | Train Loss: 88888033.1600 | Val Loss: 129171897.6000\n",
      "Epoch [342/1000] | Train Loss: 88715691.8400 | Val Loss: 131954720.0000\n",
      "Epoch [343/1000] | Train Loss: 88864013.2800 | Val Loss: 131069847.6800\n",
      "Epoch [344/1000] | Train Loss: 87807094.0800 | Val Loss: 127191847.6800\n",
      "Epoch [345/1000] | Train Loss: 89413793.2800 | Val Loss: 127736232.3200\n",
      "Epoch [346/1000] | Train Loss: 89202725.5000 | Val Loss: 125704350.0800\n",
      "Epoch [347/1000] | Train Loss: 88414767.6800 | Val Loss: 127504538.2400\n",
      "Epoch [348/1000] | Train Loss: 88997740.4000 | Val Loss: 128269237.1200\n",
      "Epoch [349/1000] | Train Loss: 88313319.1600 | Val Loss: 127116316.1600\n",
      "Epoch [350/1000] | Train Loss: 88727604.3200 | Val Loss: 126986391.0400\n",
      "Epoch [351/1000] | Train Loss: 88337312.3200 | Val Loss: 128837416.3200\n",
      "Epoch [352/1000] | Train Loss: 89136605.9600 | Val Loss: 130850375.6800\n",
      "Epoch [353/1000] | Train Loss: 89257803.2000 | Val Loss: 127691277.4400\n",
      "Epoch [354/1000] | Train Loss: 88135936.0000 | Val Loss: 128768882.5600\n",
      "Epoch [355/1000] | Train Loss: 88949656.3200 | Val Loss: 128173434.2400\n",
      "Epoch [356/1000] | Train Loss: 89184177.2800 | Val Loss: 127296538.8800\n",
      "Epoch [357/1000] | Train Loss: 88706078.7200 | Val Loss: 129330849.2800\n",
      "Epoch [358/1000] | Train Loss: 88801145.4400 | Val Loss: 130502563.8400\n",
      "Epoch [359/1000] | Train Loss: 88989390.5600 | Val Loss: 126675936.6400\n",
      "Epoch [360/1000] | Train Loss: 89608903.6800 | Val Loss: 129024497.2800\n",
      "Epoch [361/1000] | Train Loss: 88733320.0800 | Val Loss: 130373360.6400\n",
      "Epoch [362/1000] | Train Loss: 88150163.9000 | Val Loss: 129715110.4000\n",
      "Epoch [363/1000] | Train Loss: 88491775.3600 | Val Loss: 128985650.5600\n",
      "Epoch [364/1000] | Train Loss: 88955007.7600 | Val Loss: 132181113.6000\n",
      "Epoch [365/1000] | Train Loss: 88998911.6800 | Val Loss: 129360069.7600\n",
      "Epoch [366/1000] | Train Loss: 89136492.4800 | Val Loss: 128804060.1600\n",
      "Epoch [367/1000] | Train Loss: 88027199.3200 | Val Loss: 129437327.3600\n",
      "Epoch [368/1000] | Train Loss: 88373640.8000 | Val Loss: 129160573.4400\n",
      "Epoch [369/1000] | Train Loss: 88284300.8800 | Val Loss: 131206906.8800\n",
      "Epoch [370/1000] | Train Loss: 88659694.8800 | Val Loss: 129169072.6400\n",
      "Epoch [371/1000] | Train Loss: 89753051.0400 | Val Loss: 126569239.6800\n",
      "Epoch [372/1000] | Train Loss: 90732472.3200 | Val Loss: 126646880.0000\n",
      "Epoch [373/1000] | Train Loss: 89595249.2800 | Val Loss: 130409049.6000\n",
      "Epoch [374/1000] | Train Loss: 89433040.1600 | Val Loss: 128443030.4000\n",
      "Epoch [375/1000] | Train Loss: 88561057.6000 | Val Loss: 126968409.6000\n",
      "Epoch [376/1000] | Train Loss: 88821804.1600 | Val Loss: 128598128.0000\n",
      "Epoch [377/1000] | Train Loss: 89809507.8800 | Val Loss: 131847034.8800\n",
      "Epoch [378/1000] | Train Loss: 89704166.4800 | Val Loss: 128095234.5600\n",
      "Epoch [379/1000] | Train Loss: 88195297.6000 | Val Loss: 128796360.3200\n",
      "Epoch [380/1000] | Train Loss: 88876849.6000 | Val Loss: 131265441.2800\n",
      "Epoch [381/1000] | Train Loss: 87979572.6800 | Val Loss: 129925588.4800\n",
      "Epoch [382/1000] | Train Loss: 88566147.0400 | Val Loss: 129900558.0800\n",
      "Epoch [383/1000] | Train Loss: 88262118.0300 | Val Loss: 130055788.8000\n",
      "Epoch [384/1000] | Train Loss: 88106307.2000 | Val Loss: 131117370.8800\n",
      "Epoch [385/1000] | Train Loss: 88813164.1600 | Val Loss: 130611006.7200\n",
      "Epoch [386/1000] | Train Loss: 87759000.0400 | Val Loss: 129682575.3600\n",
      "Epoch [387/1000] | Train Loss: 88815954.1200 | Val Loss: 129662664.9600\n",
      "Epoch [388/1000] | Train Loss: 88200898.0800 | Val Loss: 129893562.8800\n",
      "Epoch [389/1000] | Train Loss: 88412475.8400 | Val Loss: 132067371.5200\n",
      "Epoch [390/1000] | Train Loss: 89037423.3400 | Val Loss: 129648637.4400\n",
      "Epoch [391/1000] | Train Loss: 87833304.8400 | Val Loss: 129271495.0400\n",
      "Epoch [392/1000] | Train Loss: 87055579.1200 | Val Loss: 130571832.9600\n",
      "Epoch [393/1000] | Train Loss: 88141437.4400 | Val Loss: 131050392.9600\n",
      "Epoch [394/1000] | Train Loss: 88303241.0000 | Val Loss: 127102968.9600\n",
      "Epoch [395/1000] | Train Loss: 88910159.6800 | Val Loss: 127770909.4400\n",
      "Epoch [396/1000] | Train Loss: 87418129.2800 | Val Loss: 125932662.4000\n",
      "Epoch [397/1000] | Train Loss: 89295361.0400 | Val Loss: 124788395.5200\n",
      "Epoch [398/1000] | Train Loss: 88086188.9800 | Val Loss: 127739777.2800\n",
      "Epoch [399/1000] | Train Loss: 88191994.8000 | Val Loss: 127011955.8400\n",
      "Epoch [400/1000] | Train Loss: 87590086.6000 | Val Loss: 124805886.0800\n",
      "Epoch [401/1000] | Train Loss: 87957867.6800 | Val Loss: 125152906.2400\n",
      "Epoch [402/1000] | Train Loss: 86934743.2700 | Val Loss: 126444428.8000\n",
      "Epoch [403/1000] | Train Loss: 87179667.8800 | Val Loss: 126576805.1200\n",
      "Epoch [404/1000] | Train Loss: 87064644.6400 | Val Loss: 128183282.5600\n",
      "Epoch [405/1000] | Train Loss: 87711214.7200 | Val Loss: 126603013.7600\n",
      "Epoch [406/1000] | Train Loss: 87621279.0400 | Val Loss: 123687053.4400\n",
      "Epoch [407/1000] | Train Loss: 88200336.8800 | Val Loss: 123689985.2800\n",
      "Epoch [408/1000] | Train Loss: 90925689.8400 | Val Loss: 129315335.6800\n",
      "Epoch [409/1000] | Train Loss: 88117595.9200 | Val Loss: 125525230.7200\n",
      "Epoch [410/1000] | Train Loss: 87355145.8000 | Val Loss: 123455812.4800\n",
      "Epoch [411/1000] | Train Loss: 87671516.9200 | Val Loss: 123963139.2000\n",
      "Epoch [412/1000] | Train Loss: 87601176.5600 | Val Loss: 125612918.4000\n",
      "Epoch [413/1000] | Train Loss: 86961831.8800 | Val Loss: 125312342.4000\n",
      "Epoch [414/1000] | Train Loss: 87420390.6600 | Val Loss: 123991066.8800\n",
      "Epoch [415/1000] | Train Loss: 87691252.1200 | Val Loss: 124858794.8800\n",
      "Epoch [416/1000] | Train Loss: 87373606.4000 | Val Loss: 125547734.4000\n",
      "Epoch [417/1000] | Train Loss: 87349412.1600 | Val Loss: 124935760.6400\n",
      "Epoch [418/1000] | Train Loss: 88243683.3600 | Val Loss: 121909491.2000\n",
      "Epoch [419/1000] | Train Loss: 87870293.9400 | Val Loss: 123808911.3600\n",
      "Epoch [420/1000] | Train Loss: 87690588.9600 | Val Loss: 124844652.8000\n",
      "Epoch [421/1000] | Train Loss: 87371764.4800 | Val Loss: 124279035.5200\n",
      "Epoch [422/1000] | Train Loss: 87243516.0000 | Val Loss: 123113315.8400\n",
      "Epoch [423/1000] | Train Loss: 87150724.8800 | Val Loss: 122925397.7600\n",
      "Epoch [424/1000] | Train Loss: 87667340.8400 | Val Loss: 123527756.1600\n",
      "Epoch [425/1000] | Train Loss: 86926239.0400 | Val Loss: 124241524.4800\n",
      "Epoch [426/1000] | Train Loss: 86969702.0800 | Val Loss: 123651207.6800\n",
      "Epoch [427/1000] | Train Loss: 87081594.5600 | Val Loss: 122904721.2800\n",
      "Epoch [428/1000] | Train Loss: 87168317.7600 | Val Loss: 122477104.6400\n",
      "Epoch [429/1000] | Train Loss: 88037255.6800 | Val Loss: 121227819.5200\n",
      "Epoch [430/1000] | Train Loss: 89024835.0400 | Val Loss: 121458371.8400\n",
      "Epoch [431/1000] | Train Loss: 87968315.6800 | Val Loss: 121703529.6000\n",
      "Epoch [432/1000] | Train Loss: 87358787.1200 | Val Loss: 120610510.7200\n",
      "Epoch [433/1000] | Train Loss: 87393006.6400 | Val Loss: 120568991.3600\n",
      "Epoch [434/1000] | Train Loss: 87616324.5600 | Val Loss: 120765567.3600\n",
      "Epoch [435/1000] | Train Loss: 87357990.0400 | Val Loss: 121353364.4800\n",
      "Epoch [436/1000] | Train Loss: 87437351.8400 | Val Loss: 121406727.6800\n",
      "Epoch [437/1000] | Train Loss: 87816968.1000 | Val Loss: 118775708.8000\n",
      "Epoch [438/1000] | Train Loss: 87869003.8400 | Val Loss: 119227849.6000\n",
      "Epoch [439/1000] | Train Loss: 87030473.6800 | Val Loss: 120253664.0000\n",
      "Epoch [440/1000] | Train Loss: 87382785.6000 | Val Loss: 120206362.2400\n",
      "Epoch [441/1000] | Train Loss: 87149683.7200 | Val Loss: 121372391.6800\n",
      "Epoch [442/1000] | Train Loss: 87460102.0400 | Val Loss: 121765911.0400\n",
      "Epoch [443/1000] | Train Loss: 86609824.7600 | Val Loss: 120480832.0000\n",
      "Epoch [444/1000] | Train Loss: 87324012.2800 | Val Loss: 121088179.2000\n",
      "Epoch [445/1000] | Train Loss: 87030024.8400 | Val Loss: 121927095.6800\n",
      "Epoch [446/1000] | Train Loss: 86911825.2800 | Val Loss: 121122762.2400\n",
      "Epoch [447/1000] | Train Loss: 87091875.8000 | Val Loss: 119737865.6000\n",
      "Epoch [448/1000] | Train Loss: 86756925.9200 | Val Loss: 119747923.8400\n",
      "Epoch [449/1000] | Train Loss: 87258168.5600 | Val Loss: 123390519.6800\n",
      "Epoch [450/1000] | Train Loss: 89378734.4000 | Val Loss: 123365951.3600\n",
      "Epoch [451/1000] | Train Loss: 86677539.5200 | Val Loss: 120613751.6800\n",
      "Epoch [452/1000] | Train Loss: 87362910.0800 | Val Loss: 120212330.8800\n",
      "Epoch [453/1000] | Train Loss: 86910033.5800 | Val Loss: 123660186.8800\n",
      "Epoch [454/1000] | Train Loss: 87080390.1600 | Val Loss: 122733628.8000\n",
      "Epoch [455/1000] | Train Loss: 87514064.6400 | Val Loss: 120563306.8800\n",
      "Epoch [456/1000] | Train Loss: 88210520.0000 | Val Loss: 122468375.0400\n",
      "Epoch [457/1000] | Train Loss: 87143504.0000 | Val Loss: 121499285.7600\n",
      "Epoch [458/1000] | Train Loss: 86968752.3200 | Val Loss: 125330128.6400\n",
      "Epoch [459/1000] | Train Loss: 90192323.2000 | Val Loss: 127808126.7200\n",
      "Epoch [460/1000] | Train Loss: 87506253.6800 | Val Loss: 123699342.7200\n",
      "Epoch [461/1000] | Train Loss: 87083225.1200 | Val Loss: 123880485.7600\n",
      "Epoch [462/1000] | Train Loss: 86789882.2400 | Val Loss: 124567047.6800\n",
      "Epoch [463/1000] | Train Loss: 86200258.1600 | Val Loss: 124289161.6000\n",
      "Epoch [464/1000] | Train Loss: 86913925.0000 | Val Loss: 123194030.0800\n",
      "Epoch [465/1000] | Train Loss: 86866167.2000 | Val Loss: 124195898.2400\n",
      "Epoch [466/1000] | Train Loss: 87028593.2800 | Val Loss: 125605280.6400\n",
      "Epoch [467/1000] | Train Loss: 87226917.9200 | Val Loss: 125007613.4400\n",
      "Epoch [468/1000] | Train Loss: 87410274.1600 | Val Loss: 125307299.8400\n",
      "Epoch [469/1000] | Train Loss: 86719902.4000 | Val Loss: 124833516.8000\n",
      "Epoch [470/1000] | Train Loss: 86934458.8000 | Val Loss: 122461937.2800\n",
      "Epoch [471/1000] | Train Loss: 87045805.7600 | Val Loss: 122067034.2400\n",
      "Epoch [472/1000] | Train Loss: 87815575.6800 | Val Loss: 127710913.2800\n",
      "Epoch [473/1000] | Train Loss: 88206292.6200 | Val Loss: 127449845.7600\n",
      "Epoch [474/1000] | Train Loss: 88029053.1200 | Val Loss: 126434649.6000\n",
      "Epoch [475/1000] | Train Loss: 88714527.6800 | Val Loss: 127121827.2000\n",
      "Epoch [476/1000] | Train Loss: 86857294.7200 | Val Loss: 123410994.5600\n",
      "Epoch [477/1000] | Train Loss: 86940683.7600 | Val Loss: 125894312.3200\n",
      "Epoch [478/1000] | Train Loss: 86592754.2400 | Val Loss: 124232256.0000\n",
      "Epoch [479/1000] | Train Loss: 87500259.0800 | Val Loss: 125730173.4400\n",
      "Epoch [480/1000] | Train Loss: 87396596.4800 | Val Loss: 125147029.1200\n",
      "Epoch [481/1000] | Train Loss: 87265515.5200 | Val Loss: 123332988.8000\n",
      "Epoch [482/1000] | Train Loss: 86783061.0400 | Val Loss: 123269804.8000\n",
      "Epoch [483/1000] | Train Loss: 86330267.6600 | Val Loss: 123870583.6800\n",
      "Epoch [484/1000] | Train Loss: 86536124.4000 | Val Loss: 123805447.6800\n",
      "Epoch [485/1000] | Train Loss: 86890563.8400 | Val Loss: 122791883.5200\n",
      "Epoch [486/1000] | Train Loss: 86630444.9600 | Val Loss: 121591555.2000\n",
      "Epoch [487/1000] | Train Loss: 86494370.1600 | Val Loss: 122994805.1200\n",
      "Epoch [488/1000] | Train Loss: 86425706.6200 | Val Loss: 124268938.8800\n",
      "Epoch [489/1000] | Train Loss: 86498128.9600 | Val Loss: 123174187.5200\n",
      "Epoch [490/1000] | Train Loss: 86245043.5200 | Val Loss: 121532208.6400\n",
      "Epoch [491/1000] | Train Loss: 87069398.9800 | Val Loss: 121000126.0800\n",
      "Epoch [492/1000] | Train Loss: 86188457.1200 | Val Loss: 122161114.8800\n",
      "Epoch [493/1000] | Train Loss: 86771789.8600 | Val Loss: 123572114.5600\n",
      "Epoch [494/1000] | Train Loss: 86851378.0800 | Val Loss: 122262979.8400\n",
      "Epoch [495/1000] | Train Loss: 87170494.0800 | Val Loss: 123004367.3600\n",
      "Epoch [496/1000] | Train Loss: 86676677.6000 | Val Loss: 123911616.0000\n",
      "Epoch [497/1000] | Train Loss: 87061431.2000 | Val Loss: 124048670.7200\n",
      "Epoch [498/1000] | Train Loss: 86726835.6200 | Val Loss: 122757834.2400\n",
      "Epoch [499/1000] | Train Loss: 86792499.1200 | Val Loss: 122603904.0000\n",
      "Epoch [500/1000] | Train Loss: 86594127.2000 | Val Loss: 122817507.8400\n",
      "Epoch [501/1000] | Train Loss: 86334499.8400 | Val Loss: 125514023.0400\n",
      "Epoch [502/1000] | Train Loss: 87077250.9200 | Val Loss: 125063687.0400\n",
      "Epoch [503/1000] | Train Loss: 86918005.9200 | Val Loss: 123623719.6800\n",
      "Epoch [504/1000] | Train Loss: 87249237.5800 | Val Loss: 121882478.0800\n",
      "Epoch [505/1000] | Train Loss: 86572124.3200 | Val Loss: 123068950.4000\n",
      "Epoch [506/1000] | Train Loss: 86430597.4200 | Val Loss: 123816672.0000\n",
      "Epoch [507/1000] | Train Loss: 86557084.6400 | Val Loss: 122437009.9200\n",
      "Epoch [508/1000] | Train Loss: 86200676.1200 | Val Loss: 122628909.4400\n",
      "Epoch [509/1000] | Train Loss: 86324268.7600 | Val Loss: 123348328.3200\n",
      "Epoch [510/1000] | Train Loss: 86990334.9600 | Val Loss: 122805284.4800\n",
      "Epoch [511/1000] | Train Loss: 86363883.2000 | Val Loss: 121444960.6400\n",
      "Epoch [512/1000] | Train Loss: 86081762.0800 | Val Loss: 123540602.8800\n",
      "Epoch [513/1000] | Train Loss: 86886047.8400 | Val Loss: 125217988.4800\n",
      "Epoch [514/1000] | Train Loss: 88028470.8000 | Val Loss: 126887096.3200\n",
      "Epoch [515/1000] | Train Loss: 88005178.8800 | Val Loss: 124884726.4000\n",
      "Epoch [516/1000] | Train Loss: 87458727.8800 | Val Loss: 127035129.6000\n",
      "Epoch [517/1000] | Train Loss: 87176214.8800 | Val Loss: 125007107.2000\n",
      "Epoch [518/1000] | Train Loss: 86945938.7200 | Val Loss: 124363447.0400\n",
      "Epoch [519/1000] | Train Loss: 86691485.6000 | Val Loss: 125599872.0000\n",
      "Epoch [520/1000] | Train Loss: 87812304.6400 | Val Loss: 126273171.8400\n",
      "Epoch [521/1000] | Train Loss: 87081621.8400 | Val Loss: 124888839.0400\n",
      "Epoch [522/1000] | Train Loss: 86710684.4800 | Val Loss: 122429477.7600\n",
      "Epoch [523/1000] | Train Loss: 87834095.6800 | Val Loss: 121051399.6800\n",
      "Epoch [524/1000] | Train Loss: 88363710.7400 | Val Loss: 124243965.4400\n",
      "Epoch [525/1000] | Train Loss: 86866874.4000 | Val Loss: 124237607.6800\n",
      "Epoch [526/1000] | Train Loss: 85702729.2800 | Val Loss: 122131375.3600\n",
      "Epoch [527/1000] | Train Loss: 86114176.9600 | Val Loss: 123959668.4800\n",
      "Epoch [528/1000] | Train Loss: 86992559.9200 | Val Loss: 122571818.8800\n",
      "Epoch [529/1000] | Train Loss: 86875013.4400 | Val Loss: 122547089.9200\n",
      "Epoch [530/1000] | Train Loss: 86420571.5200 | Val Loss: 121348053.7600\n",
      "Epoch [531/1000] | Train Loss: 86513682.8800 | Val Loss: 121953871.3600\n",
      "Epoch [532/1000] | Train Loss: 86385261.9400 | Val Loss: 122487864.3200\n",
      "Epoch [533/1000] | Train Loss: 86613512.4800 | Val Loss: 121249658.8800\n",
      "Epoch [534/1000] | Train Loss: 86200341.4800 | Val Loss: 120729121.2800\n",
      "Epoch [535/1000] | Train Loss: 86658336.9600 | Val Loss: 121649205.1200\n",
      "Epoch [536/1000] | Train Loss: 86227164.2600 | Val Loss: 121351969.9200\n",
      "Epoch [537/1000] | Train Loss: 85911802.9600 | Val Loss: 121136094.7200\n",
      "Epoch [538/1000] | Train Loss: 85523443.2800 | Val Loss: 121934666.2400\n",
      "Epoch [539/1000] | Train Loss: 87272809.8400 | Val Loss: 121109840.6400\n",
      "Epoch [540/1000] | Train Loss: 86563356.6400 | Val Loss: 120901967.3600\n",
      "Epoch [541/1000] | Train Loss: 87133399.4400 | Val Loss: 122917443.2000\n",
      "Epoch [542/1000] | Train Loss: 85875385.8400 | Val Loss: 121365441.9200\n",
      "Epoch [543/1000] | Train Loss: 86745282.3200 | Val Loss: 120104782.7200\n",
      "Epoch [544/1000] | Train Loss: 86059388.4000 | Val Loss: 121029669.7600\n",
      "Epoch [545/1000] | Train Loss: 86818188.3600 | Val Loss: 122207505.9200\n",
      "Epoch [546/1000] | Train Loss: 86492204.1600 | Val Loss: 120078367.3600\n",
      "Epoch [547/1000] | Train Loss: 86536074.0800 | Val Loss: 121369831.6800\n",
      "Epoch [548/1000] | Train Loss: 86039472.1600 | Val Loss: 121149713.9200\n",
      "Epoch [549/1000] | Train Loss: 86180409.2800 | Val Loss: 120489534.7200\n",
      "Epoch [550/1000] | Train Loss: 86069892.9600 | Val Loss: 120182821.7600\n",
      "Epoch [551/1000] | Train Loss: 86756667.0400 | Val Loss: 121698646.4000\n",
      "Epoch [552/1000] | Train Loss: 86868992.3200 | Val Loss: 118397937.9200\n",
      "Epoch [553/1000] | Train Loss: 86686761.4400 | Val Loss: 119680286.0800\n",
      "Epoch [554/1000] | Train Loss: 86563620.3600 | Val Loss: 120571937.9200\n",
      "Epoch [555/1000] | Train Loss: 85832509.2000 | Val Loss: 119221391.3600\n",
      "Epoch [556/1000] | Train Loss: 85880040.6400 | Val Loss: 119457217.9200\n",
      "Epoch [557/1000] | Train Loss: 86061758.2400 | Val Loss: 120076836.4800\n",
      "Epoch [558/1000] | Train Loss: 86499960.9600 | Val Loss: 120023385.6000\n",
      "Epoch [559/1000] | Train Loss: 86372253.9600 | Val Loss: 122076138.8800\n",
      "Epoch [560/1000] | Train Loss: 87150737.3900 | Val Loss: 119886142.7200\n",
      "Epoch [561/1000] | Train Loss: 86726002.3200 | Val Loss: 120147831.0400\n",
      "Epoch [562/1000] | Train Loss: 85827723.9200 | Val Loss: 121949825.9200\n",
      "Epoch [563/1000] | Train Loss: 86297391.0400 | Val Loss: 120025021.4400\n",
      "Epoch [564/1000] | Train Loss: 85962016.6400 | Val Loss: 121758332.1600\n",
      "Epoch [565/1000] | Train Loss: 85776546.2400 | Val Loss: 121756533.1200\n",
      "Epoch [566/1000] | Train Loss: 85678841.2800 | Val Loss: 120167845.7600\n",
      "Epoch [567/1000] | Train Loss: 86703726.3200 | Val Loss: 119097922.5600\n",
      "Epoch [568/1000] | Train Loss: 86952209.7800 | Val Loss: 121594954.2400\n",
      "Epoch [569/1000] | Train Loss: 85679810.6000 | Val Loss: 120893653.7600\n",
      "Epoch [570/1000] | Train Loss: 86018412.6000 | Val Loss: 119036513.9200\n",
      "Epoch [571/1000] | Train Loss: 85592968.9600 | Val Loss: 119427936.0000\n",
      "Epoch [572/1000] | Train Loss: 85329167.6800 | Val Loss: 121907116.8000\n",
      "Epoch [573/1000] | Train Loss: 86111388.3600 | Val Loss: 119354895.3600\n",
      "Epoch [574/1000] | Train Loss: 86138034.2400 | Val Loss: 119472366.7200\n",
      "Epoch [575/1000] | Train Loss: 85404731.2000 | Val Loss: 120530744.9600\n",
      "Epoch [576/1000] | Train Loss: 85355058.0800 | Val Loss: 121704064.6400\n",
      "Epoch [577/1000] | Train Loss: 85888627.1600 | Val Loss: 121367374.7200\n",
      "Epoch [578/1000] | Train Loss: 85674270.7200 | Val Loss: 119766678.4000\n",
      "Epoch [579/1000] | Train Loss: 86917715.5200 | Val Loss: 119644675.8400\n",
      "Epoch [580/1000] | Train Loss: 87764099.0800 | Val Loss: 124199824.0000\n",
      "Epoch [581/1000] | Train Loss: 88872956.9600 | Val Loss: 122264172.8000\n",
      "Epoch [582/1000] | Train Loss: 86827017.7600 | Val Loss: 119412141.4400\n",
      "Epoch [583/1000] | Train Loss: 86789788.5400 | Val Loss: 119602115.8400\n",
      "Epoch [584/1000] | Train Loss: 85405681.3200 | Val Loss: 119667216.0000\n",
      "Epoch [585/1000] | Train Loss: 86250910.7200 | Val Loss: 121092412.8000\n",
      "Epoch [586/1000] | Train Loss: 86405421.9200 | Val Loss: 120114569.6000\n",
      "Epoch [587/1000] | Train Loss: 85445836.1000 | Val Loss: 118127113.6000\n",
      "Epoch [588/1000] | Train Loss: 88480844.4800 | Val Loss: 117649194.2400\n",
      "Epoch [589/1000] | Train Loss: 86164481.2400 | Val Loss: 120378883.8400\n",
      "Epoch [590/1000] | Train Loss: 86184813.1600 | Val Loss: 118193176.3200\n",
      "Epoch [591/1000] | Train Loss: 85658194.7200 | Val Loss: 117341274.2400\n",
      "Epoch [592/1000] | Train Loss: 85867162.4000 | Val Loss: 116689498.8800\n",
      "Epoch [593/1000] | Train Loss: 85862692.3800 | Val Loss: 118745128.3200\n",
      "Epoch [594/1000] | Train Loss: 86174099.6800 | Val Loss: 120304979.8400\n",
      "Epoch [595/1000] | Train Loss: 85161455.2200 | Val Loss: 117569467.5200\n",
      "Epoch [596/1000] | Train Loss: 86640401.0800 | Val Loss: 116815905.9200\n",
      "Epoch [597/1000] | Train Loss: 85679713.5000 | Val Loss: 119265724.8000\n",
      "Epoch [598/1000] | Train Loss: 85554916.1600 | Val Loss: 119526765.4400\n",
      "Epoch [599/1000] | Train Loss: 85993996.9600 | Val Loss: 119531385.6000\n",
      "Epoch [600/1000] | Train Loss: 85775452.1200 | Val Loss: 118584540.8000\n",
      "Epoch [601/1000] | Train Loss: 85342794.2400 | Val Loss: 118283588.4800\n",
      "Epoch [602/1000] | Train Loss: 86611374.8800 | Val Loss: 120631631.3600\n",
      "Epoch [603/1000] | Train Loss: 85756567.6400 | Val Loss: 119411989.1200\n",
      "Epoch [604/1000] | Train Loss: 85388964.8000 | Val Loss: 118700592.0000\n",
      "Epoch [605/1000] | Train Loss: 85718197.4400 | Val Loss: 119201190.4000\n",
      "Epoch [606/1000] | Train Loss: 85980404.1200 | Val Loss: 120149581.4400\n",
      "Epoch [607/1000] | Train Loss: 85874502.0000 | Val Loss: 119388561.9200\n",
      "Epoch [608/1000] | Train Loss: 85781592.5600 | Val Loss: 119828032.0000\n",
      "Epoch [609/1000] | Train Loss: 86330003.6400 | Val Loss: 118297349.7600\n",
      "Epoch [610/1000] | Train Loss: 85538064.5600 | Val Loss: 117866584.9600\n",
      "Epoch [611/1000] | Train Loss: 87164591.6800 | Val Loss: 120302896.0000\n",
      "Epoch [612/1000] | Train Loss: 86124468.6400 | Val Loss: 118889289.6000\n",
      "Epoch [613/1000] | Train Loss: 86029078.3200 | Val Loss: 120462005.7600\n",
      "Epoch [614/1000] | Train Loss: 85551902.2200 | Val Loss: 119340289.9200\n",
      "Epoch [615/1000] | Train Loss: 85561036.3200 | Val Loss: 119249517.4400\n",
      "Epoch [616/1000] | Train Loss: 86091522.2400 | Val Loss: 121184519.0400\n",
      "Epoch [617/1000] | Train Loss: 85385459.4400 | Val Loss: 119655455.3600\n",
      "Epoch [618/1000] | Train Loss: 84949341.3600 | Val Loss: 119463613.4400\n",
      "Epoch [619/1000] | Train Loss: 85191990.1500 | Val Loss: 118617941.7600\n",
      "Epoch [620/1000] | Train Loss: 85384947.1200 | Val Loss: 119567688.3200\n",
      "Epoch [621/1000] | Train Loss: 84958526.3200 | Val Loss: 118944753.9200\n",
      "Epoch [622/1000] | Train Loss: 85156868.9600 | Val Loss: 117896263.0400\n",
      "Epoch [623/1000] | Train Loss: 85768399.6800 | Val Loss: 118540211.2000\n",
      "Epoch [624/1000] | Train Loss: 85324675.5200 | Val Loss: 117641790.0800\n",
      "Epoch [625/1000] | Train Loss: 86768858.5600 | Val Loss: 115979044.4800\n",
      "Epoch [626/1000] | Train Loss: 86473025.4000 | Val Loss: 116037995.5200\n",
      "Epoch [627/1000] | Train Loss: 85678422.2800 | Val Loss: 118464168.3200\n",
      "Epoch [628/1000] | Train Loss: 85370154.2400 | Val Loss: 117504616.3200\n",
      "Epoch [629/1000] | Train Loss: 86682887.4800 | Val Loss: 118621695.3600\n",
      "Epoch [630/1000] | Train Loss: 85678131.3600 | Val Loss: 116722382.0800\n",
      "Epoch [631/1000] | Train Loss: 84879418.6400 | Val Loss: 118804141.4400\n",
      "Epoch [632/1000] | Train Loss: 86051410.8800 | Val Loss: 119113431.0400\n",
      "Epoch [633/1000] | Train Loss: 86040437.0800 | Val Loss: 116636464.6400\n",
      "Epoch [634/1000] | Train Loss: 85477513.7600 | Val Loss: 117522250.2400\n",
      "Epoch [635/1000] | Train Loss: 84887778.4400 | Val Loss: 118112043.5200\n",
      "Epoch [636/1000] | Train Loss: 85138533.7600 | Val Loss: 117441779.2000\n",
      "Epoch [637/1000] | Train Loss: 86119079.0400 | Val Loss: 115659199.3600\n",
      "Epoch [638/1000] | Train Loss: 85446136.2600 | Val Loss: 118620792.9600\n",
      "Epoch [639/1000] | Train Loss: 85450737.1200 | Val Loss: 118213546.2400\n",
      "Epoch [640/1000] | Train Loss: 85111384.9600 | Val Loss: 116723148.8000\n",
      "Epoch [641/1000] | Train Loss: 86032805.3600 | Val Loss: 115922926.0800\n",
      "Epoch [642/1000] | Train Loss: 85547520.9600 | Val Loss: 118121717.1200\n",
      "Epoch [643/1000] | Train Loss: 85669864.9600 | Val Loss: 116923973.1200\n",
      "Epoch [644/1000] | Train Loss: 85533677.2800 | Val Loss: 116600922.2400\n",
      "Epoch [645/1000] | Train Loss: 84672534.8800 | Val Loss: 119911503.3600\n",
      "Epoch [646/1000] | Train Loss: 87554551.0400 | Val Loss: 123229992.9600\n",
      "Epoch [647/1000] | Train Loss: 86101013.9200 | Val Loss: 119150876.1600\n",
      "Epoch [648/1000] | Train Loss: 86315782.1600 | Val Loss: 117680854.4000\n",
      "Epoch [649/1000] | Train Loss: 85475135.1200 | Val Loss: 118514053.1200\n",
      "Epoch [650/1000] | Train Loss: 85323699.5200 | Val Loss: 118518679.6800\n",
      "Epoch [651/1000] | Train Loss: 85014445.0800 | Val Loss: 118072321.2800\n",
      "Epoch [652/1000] | Train Loss: 85553715.3600 | Val Loss: 117732966.4000\n",
      "Epoch [653/1000] | Train Loss: 84593444.4000 | Val Loss: 117111032.3200\n",
      "Epoch [654/1000] | Train Loss: 85025339.0900 | Val Loss: 118329507.2000\n",
      "Epoch [655/1000] | Train Loss: 85303367.9400 | Val Loss: 117581848.3200\n",
      "Epoch [656/1000] | Train Loss: 85652172.8000 | Val Loss: 115849226.2400\n",
      "Epoch [657/1000] | Train Loss: 86378352.3200 | Val Loss: 114914348.1600\n",
      "Epoch [658/1000] | Train Loss: 84366697.1200 | Val Loss: 116299118.7200\n",
      "Epoch [659/1000] | Train Loss: 85262311.0400 | Val Loss: 116899832.3200\n",
      "Epoch [660/1000] | Train Loss: 85668602.8800 | Val Loss: 115811458.5600\n",
      "Epoch [661/1000] | Train Loss: 85223216.6400 | Val Loss: 114486640.6400\n",
      "Epoch [662/1000] | Train Loss: 86211871.7200 | Val Loss: 119272522.2400\n",
      "Epoch [663/1000] | Train Loss: 85912003.6000 | Val Loss: 118713587.2000\n",
      "Epoch [664/1000] | Train Loss: 86197681.5200 | Val Loss: 115749598.7200\n",
      "Epoch [665/1000] | Train Loss: 85373941.4400 | Val Loss: 116976637.4400\n",
      "Epoch [666/1000] | Train Loss: 84670581.6000 | Val Loss: 117323505.9200\n",
      "Epoch [667/1000] | Train Loss: 85086623.8400 | Val Loss: 115413992.3200\n",
      "Epoch [668/1000] | Train Loss: 84694792.4800 | Val Loss: 116373515.5200\n",
      "Epoch [669/1000] | Train Loss: 85145750.3400 | Val Loss: 117090503.6800\n",
      "Epoch [670/1000] | Train Loss: 85258656.4800 | Val Loss: 115294190.7200\n",
      "Epoch [671/1000] | Train Loss: 84960196.6400 | Val Loss: 116172938.8800\n",
      "Epoch [672/1000] | Train Loss: 85557631.0400 | Val Loss: 116335344.6400\n",
      "Epoch [673/1000] | Train Loss: 85329476.0000 | Val Loss: 116859660.1600\n",
      "Epoch [674/1000] | Train Loss: 86712031.9200 | Val Loss: 119932362.2400\n",
      "Epoch [675/1000] | Train Loss: 86710454.2000 | Val Loss: 117674620.1600\n",
      "Epoch [676/1000] | Train Loss: 85633379.9200 | Val Loss: 116435262.7200\n",
      "Epoch [677/1000] | Train Loss: 85335618.0800 | Val Loss: 116841899.5200\n",
      "Epoch [678/1000] | Train Loss: 84903666.0800 | Val Loss: 114532218.8800\n",
      "Epoch [679/1000] | Train Loss: 85759054.4000 | Val Loss: 113123419.5200\n",
      "Epoch [680/1000] | Train Loss: 85646189.9200 | Val Loss: 114278944.6400\n",
      "Epoch [681/1000] | Train Loss: 84490249.8400 | Val Loss: 114957900.8000\n",
      "Epoch [682/1000] | Train Loss: 85699737.4600 | Val Loss: 115316261.7600\n",
      "Epoch [683/1000] | Train Loss: 84990752.6600 | Val Loss: 114394168.9600\n",
      "Epoch [684/1000] | Train Loss: 84849745.3600 | Val Loss: 113683142.4000\n",
      "Epoch [685/1000] | Train Loss: 85315823.6800 | Val Loss: 114404993.2800\n",
      "Epoch [686/1000] | Train Loss: 84868257.7600 | Val Loss: 114817578.8800\n",
      "Epoch [687/1000] | Train Loss: 84336028.3600 | Val Loss: 113805475.8400\n",
      "Epoch [688/1000] | Train Loss: 85102424.4000 | Val Loss: 114547160.3200\n",
      "Epoch [689/1000] | Train Loss: 85004698.4800 | Val Loss: 113276234.2400\n",
      "Epoch [690/1000] | Train Loss: 85252040.0000 | Val Loss: 114262973.4400\n",
      "Epoch [691/1000] | Train Loss: 85203085.2800 | Val Loss: 113491986.5600\n",
      "Epoch [692/1000] | Train Loss: 85172405.0600 | Val Loss: 114109826.5600\n",
      "Epoch [693/1000] | Train Loss: 84094652.3200 | Val Loss: 114458156.8000\n",
      "Epoch [694/1000] | Train Loss: 85659897.1200 | Val Loss: 116938972.8000\n",
      "Epoch [695/1000] | Train Loss: 84207825.4600 | Val Loss: 115527172.4800\n",
      "Epoch [696/1000] | Train Loss: 84409214.8800 | Val Loss: 113963700.4800\n",
      "Epoch [697/1000] | Train Loss: 85321513.6000 | Val Loss: 113261688.3200\n",
      "Epoch [698/1000] | Train Loss: 85339782.2400 | Val Loss: 112984848.0000\n",
      "Epoch [699/1000] | Train Loss: 84248443.2000 | Val Loss: 110723948.1600\n",
      "Epoch [700/1000] | Train Loss: 85652215.5600 | Val Loss: 109732316.1600\n",
      "Epoch [701/1000] | Train Loss: 85523573.1200 | Val Loss: 110352772.4800\n",
      "Epoch [702/1000] | Train Loss: 84810141.8400 | Val Loss: 110449278.0800\n",
      "Epoch [703/1000] | Train Loss: 85513741.2800 | Val Loss: 110279667.2000\n",
      "Epoch [704/1000] | Train Loss: 85852829.1200 | Val Loss: 110614476.1600\n",
      "Epoch [705/1000] | Train Loss: 83827615.8000 | Val Loss: 109522775.0400\n",
      "Epoch [706/1000] | Train Loss: 84646435.8400 | Val Loss: 109595779.2000\n",
      "Epoch [707/1000] | Train Loss: 85655567.1400 | Val Loss: 108737843.2000\n",
      "Epoch [708/1000] | Train Loss: 84681771.6800 | Val Loss: 110181052.1600\n",
      "Epoch [709/1000] | Train Loss: 86154844.6400 | Val Loss: 110344430.7200\n",
      "Epoch [710/1000] | Train Loss: 85111684.6000 | Val Loss: 110212839.6800\n",
      "Epoch [711/1000] | Train Loss: 84446664.2000 | Val Loss: 109477257.6000\n",
      "Epoch [712/1000] | Train Loss: 84664775.3600 | Val Loss: 109993253.7600\n",
      "Epoch [713/1000] | Train Loss: 84161267.2000 | Val Loss: 113239401.6000\n",
      "Epoch [714/1000] | Train Loss: 87217170.2400 | Val Loss: 115861020.1600\n",
      "Epoch [715/1000] | Train Loss: 85414286.8200 | Val Loss: 115127491.8400\n",
      "Epoch [716/1000] | Train Loss: 84775474.8000 | Val Loss: 113904924.1600\n",
      "Epoch [717/1000] | Train Loss: 84798714.4400 | Val Loss: 113388835.2000\n",
      "Epoch [718/1000] | Train Loss: 84491820.4400 | Val Loss: 114071662.0800\n",
      "Epoch [719/1000] | Train Loss: 84725781.2800 | Val Loss: 113808072.3200\n",
      "Epoch [720/1000] | Train Loss: 84881174.0800 | Val Loss: 112973804.8000\n",
      "Epoch [721/1000] | Train Loss: 84932970.8800 | Val Loss: 115977292.8000\n",
      "Epoch [722/1000] | Train Loss: 84448150.4000 | Val Loss: 114763489.2800\n",
      "Epoch [723/1000] | Train Loss: 86774099.2000 | Val Loss: 112852444.1600\n",
      "Epoch [724/1000] | Train Loss: 85034056.2800 | Val Loss: 113853096.9600\n",
      "Epoch [725/1000] | Train Loss: 84383066.9600 | Val Loss: 113863658.8800\n",
      "Epoch [726/1000] | Train Loss: 84502563.0400 | Val Loss: 112882954.8800\n",
      "Epoch [727/1000] | Train Loss: 84014587.5200 | Val Loss: 113215460.4800\n",
      "Epoch [728/1000] | Train Loss: 84225180.9600 | Val Loss: 111444730.8800\n",
      "Epoch [729/1000] | Train Loss: 84425737.9600 | Val Loss: 112679978.2400\n",
      "Epoch [730/1000] | Train Loss: 83641951.9200 | Val Loss: 112886608.6400\n",
      "Epoch [731/1000] | Train Loss: 85220548.6200 | Val Loss: 113817025.2800\n",
      "Epoch [732/1000] | Train Loss: 84653367.2000 | Val Loss: 112312549.1200\n",
      "Epoch [733/1000] | Train Loss: 83998545.2800 | Val Loss: 114725870.0800\n",
      "Epoch [734/1000] | Train Loss: 83953657.6800 | Val Loss: 115510468.4800\n",
      "Epoch [735/1000] | Train Loss: 84531694.0000 | Val Loss: 115657709.4400\n",
      "Epoch [736/1000] | Train Loss: 84128198.8600 | Val Loss: 113968193.9200\n",
      "Epoch [737/1000] | Train Loss: 84518394.7600 | Val Loss: 113311859.2000\n",
      "Epoch [738/1000] | Train Loss: 84658605.6600 | Val Loss: 114438512.6400\n",
      "Epoch [739/1000] | Train Loss: 84338969.8400 | Val Loss: 114472325.7600\n",
      "Epoch [740/1000] | Train Loss: 83867755.5400 | Val Loss: 114309646.0800\n",
      "Epoch [741/1000] | Train Loss: 85167479.7600 | Val Loss: 114173619.2000\n",
      "Epoch [742/1000] | Train Loss: 83768640.9600 | Val Loss: 113737187.8400\n",
      "Epoch [743/1000] | Train Loss: 84010134.7200 | Val Loss: 113101759.3600\n",
      "Epoch [744/1000] | Train Loss: 83947345.4000 | Val Loss: 114317790.7200\n",
      "Epoch [745/1000] | Train Loss: 83976096.4800 | Val Loss: 113349145.6000\n",
      "Epoch [746/1000] | Train Loss: 84846221.4400 | Val Loss: 113859285.1200\n",
      "Epoch [747/1000] | Train Loss: 84509601.6600 | Val Loss: 111925705.6000\n",
      "Epoch [748/1000] | Train Loss: 84598827.4400 | Val Loss: 112143811.8400\n",
      "Epoch [749/1000] | Train Loss: 84763830.8000 | Val Loss: 113662364.1600\n",
      "Epoch [750/1000] | Train Loss: 83736390.7200 | Val Loss: 112917829.7600\n",
      "Epoch [751/1000] | Train Loss: 84320788.4400 | Val Loss: 112001509.7600\n",
      "Epoch [752/1000] | Train Loss: 83761029.4400 | Val Loss: 112687532.8000\n",
      "Epoch [753/1000] | Train Loss: 83880000.7200 | Val Loss: 113031358.0800\n",
      "Epoch [754/1000] | Train Loss: 84885487.3600 | Val Loss: 112040906.8800\n",
      "Epoch [755/1000] | Train Loss: 83910388.7200 | Val Loss: 111950768.0000\n",
      "Epoch [756/1000] | Train Loss: 84422362.6400 | Val Loss: 112779491.8400\n",
      "Epoch [757/1000] | Train Loss: 85281367.7600 | Val Loss: 114529744.0000\n",
      "Epoch [758/1000] | Train Loss: 84360831.2800 | Val Loss: 111857589.7600\n",
      "Epoch [759/1000] | Train Loss: 84308336.5600 | Val Loss: 113014291.2000\n",
      "Epoch [760/1000] | Train Loss: 84182404.3200 | Val Loss: 113158921.6000\n",
      "Epoch [761/1000] | Train Loss: 84119197.1000 | Val Loss: 113216858.2400\n",
      "Epoch [762/1000] | Train Loss: 84082561.6800 | Val Loss: 113406905.6000\n",
      "Epoch [763/1000] | Train Loss: 83843330.6400 | Val Loss: 112323566.0800\n",
      "Epoch [764/1000] | Train Loss: 83931674.9100 | Val Loss: 113440192.6400\n",
      "Epoch [765/1000] | Train Loss: 84162613.0400 | Val Loss: 115257333.7600\n",
      "Epoch [766/1000] | Train Loss: 84273551.6600 | Val Loss: 113254505.6000\n",
      "Epoch [767/1000] | Train Loss: 84558278.5800 | Val Loss: 112726717.4400\n",
      "Epoch [768/1000] | Train Loss: 84243602.3600 | Val Loss: 114743232.0000\n",
      "Epoch [769/1000] | Train Loss: 84548760.3200 | Val Loss: 114145950.0800\n",
      "Epoch [770/1000] | Train Loss: 84852246.8000 | Val Loss: 115396839.6800\n",
      "Epoch [771/1000] | Train Loss: 83897999.6800 | Val Loss: 115044491.5200\n",
      "Epoch [772/1000] | Train Loss: 83842173.8400 | Val Loss: 113758798.7200\n",
      "Epoch [773/1000] | Train Loss: 84637769.8000 | Val Loss: 113800245.1200\n",
      "Epoch [774/1000] | Train Loss: 83942298.5200 | Val Loss: 114706547.2000\n",
      "Epoch [775/1000] | Train Loss: 84417755.8000 | Val Loss: 114719418.2400\n",
      "Epoch [776/1000] | Train Loss: 83379062.7200 | Val Loss: 113393163.5200\n",
      "Epoch [777/1000] | Train Loss: 84062819.1200 | Val Loss: 112445511.6800\n",
      "Epoch [778/1000] | Train Loss: 84187009.4800 | Val Loss: 112052267.5200\n",
      "Epoch [779/1000] | Train Loss: 84423837.9200 | Val Loss: 113293767.0400\n",
      "Epoch [780/1000] | Train Loss: 84214877.1200 | Val Loss: 114495520.0000\n",
      "Epoch [781/1000] | Train Loss: 83281449.1200 | Val Loss: 113165887.3600\n",
      "Epoch [782/1000] | Train Loss: 83202133.3600 | Val Loss: 113677558.4000\n",
      "Epoch [783/1000] | Train Loss: 83341222.7200 | Val Loss: 113286176.6400\n",
      "Epoch [784/1000] | Train Loss: 83401665.6000 | Val Loss: 111889804.8000\n",
      "Epoch [785/1000] | Train Loss: 85529466.2400 | Val Loss: 111102381.4400\n",
      "Epoch [786/1000] | Train Loss: 83617911.4400 | Val Loss: 113948784.0000\n",
      "Epoch [787/1000] | Train Loss: 85212878.6400 | Val Loss: 114207020.8000\n",
      "Epoch [788/1000] | Train Loss: 84442705.6800 | Val Loss: 111735836.8000\n",
      "Epoch [789/1000] | Train Loss: 84571733.1200 | Val Loss: 111974986.2400\n",
      "Epoch [790/1000] | Train Loss: 83940049.2800 | Val Loss: 114195267.2000\n",
      "Epoch [791/1000] | Train Loss: 84841214.0800 | Val Loss: 112598946.5600\n",
      "Epoch [792/1000] | Train Loss: 84107946.8800 | Val Loss: 112680750.0800\n",
      "Epoch [793/1000] | Train Loss: 84357385.5200 | Val Loss: 115658324.4800\n",
      "Epoch [794/1000] | Train Loss: 85177338.0800 | Val Loss: 115881095.6800\n",
      "Epoch [795/1000] | Train Loss: 84055750.4800 | Val Loss: 114196254.7200\n",
      "Epoch [796/1000] | Train Loss: 84474539.2000 | Val Loss: 114209167.3600\n",
      "Epoch [797/1000] | Train Loss: 83293534.7200 | Val Loss: 113246768.6400\n",
      "Epoch [798/1000] | Train Loss: 83904456.3200 | Val Loss: 112735625.6000\n",
      "Epoch [799/1000] | Train Loss: 83585955.3600 | Val Loss: 113718672.0000\n",
      "Epoch [800/1000] | Train Loss: 82807955.2000 | Val Loss: 113837777.9200\n",
      "Epoch [801/1000] | Train Loss: 84041154.3600 | Val Loss: 114572648.9600\n",
      "Epoch [802/1000] | Train Loss: 83508767.5200 | Val Loss: 114233463.6800\n",
      "Epoch [803/1000] | Train Loss: 84281652.5400 | Val Loss: 112965710.0800\n",
      "Epoch [804/1000] | Train Loss: 83220131.5200 | Val Loss: 113494479.3600\n",
      "Epoch [805/1000] | Train Loss: 83672884.7600 | Val Loss: 111534149.1200\n",
      "Epoch [806/1000] | Train Loss: 83417236.9400 | Val Loss: 111389605.7600\n",
      "Epoch [807/1000] | Train Loss: 83130687.2000 | Val Loss: 111462314.8800\n",
      "Epoch [808/1000] | Train Loss: 84950570.0800 | Val Loss: 115853283.8400\n",
      "Epoch [809/1000] | Train Loss: 84520541.2000 | Val Loss: 112962615.6800\n",
      "Epoch [810/1000] | Train Loss: 83811618.5600 | Val Loss: 112112444.1600\n",
      "Epoch [811/1000] | Train Loss: 83113263.1200 | Val Loss: 113703243.5200\n",
      "Epoch [812/1000] | Train Loss: 83999499.0400 | Val Loss: 114313746.5600\n",
      "Epoch [813/1000] | Train Loss: 83712723.9800 | Val Loss: 113118702.0800\n",
      "Epoch [814/1000] | Train Loss: 83556494.6800 | Val Loss: 111974939.5200\n",
      "Epoch [815/1000] | Train Loss: 83229558.2400 | Val Loss: 113375386.2400\n",
      "Epoch [816/1000] | Train Loss: 83642593.7600 | Val Loss: 112809471.3600\n",
      "Epoch [817/1000] | Train Loss: 83395425.6000 | Val Loss: 112431493.7600\n",
      "Epoch [818/1000] | Train Loss: 83005536.0800 | Val Loss: 112143049.6000\n",
      "Epoch [819/1000] | Train Loss: 83904760.1600 | Val Loss: 112147511.6800\n",
      "Epoch [820/1000] | Train Loss: 82716566.7200 | Val Loss: 116571650.5600\n",
      "Epoch [821/1000] | Train Loss: 84517236.3200 | Val Loss: 116826299.5200\n",
      "Epoch [822/1000] | Train Loss: 84721805.1200 | Val Loss: 114984958.0800\n",
      "Epoch [823/1000] | Train Loss: 84109143.4400 | Val Loss: 115356210.5600\n",
      "Epoch [824/1000] | Train Loss: 83378096.8800 | Val Loss: 114138877.4400\n",
      "Epoch [825/1000] | Train Loss: 83700455.4400 | Val Loss: 114156165.7600\n",
      "Epoch [826/1000] | Train Loss: 84085644.8800 | Val Loss: 114238679.0400\n",
      "Epoch [827/1000] | Train Loss: 83280170.0200 | Val Loss: 115114359.6800\n",
      "Epoch [828/1000] | Train Loss: 83071923.2000 | Val Loss: 114498389.1200\n",
      "Epoch [829/1000] | Train Loss: 83510223.0000 | Val Loss: 114095857.2800\n",
      "Epoch [830/1000] | Train Loss: 83670426.6200 | Val Loss: 113903273.6000\n",
      "Epoch [831/1000] | Train Loss: 83708927.0400 | Val Loss: 113525568.6400\n",
      "Epoch [832/1000] | Train Loss: 83938663.8200 | Val Loss: 111426848.6400\n",
      "Epoch [833/1000] | Train Loss: 83912469.7600 | Val Loss: 111669360.0000\n",
      "Epoch [834/1000] | Train Loss: 83547800.4800 | Val Loss: 114207936.0000\n",
      "Epoch [835/1000] | Train Loss: 85161364.6400 | Val Loss: 116339447.0400\n",
      "Epoch [836/1000] | Train Loss: 83985964.5600 | Val Loss: 112894260.4800\n",
      "Epoch [837/1000] | Train Loss: 83457454.2400 | Val Loss: 112786619.5200\n",
      "Epoch [838/1000] | Train Loss: 83191584.9200 | Val Loss: 112950093.4400\n",
      "Epoch [839/1000] | Train Loss: 83099491.8000 | Val Loss: 113743047.6800\n",
      "Epoch [840/1000] | Train Loss: 83100259.9200 | Val Loss: 113815064.9600\n",
      "Epoch [841/1000] | Train Loss: 83197637.9200 | Val Loss: 113351270.4000\n",
      "Epoch [842/1000] | Train Loss: 83207804.4000 | Val Loss: 112668117.7600\n",
      "Epoch [843/1000] | Train Loss: 83669537.2800 | Val Loss: 112444060.1600\n",
      "Epoch [844/1000] | Train Loss: 82981992.3600 | Val Loss: 110532021.7600\n",
      "Epoch [845/1000] | Train Loss: 83355973.1200 | Val Loss: 110913080.3200\n",
      "Epoch [846/1000] | Train Loss: 82707929.6000 | Val Loss: 115035942.4000\n",
      "Epoch [847/1000] | Train Loss: 83697281.0000 | Val Loss: 113838632.9600\n",
      "Epoch [848/1000] | Train Loss: 83874538.5600 | Val Loss: 112149754.8800\n",
      "Epoch [849/1000] | Train Loss: 83258900.8000 | Val Loss: 113341335.0400\n",
      "Epoch [850/1000] | Train Loss: 84134216.0000 | Val Loss: 118408757.1200\n",
      "Epoch [851/1000] | Train Loss: 83966366.6400 | Val Loss: 114414880.0000\n",
      "Epoch [852/1000] | Train Loss: 83157787.2000 | Val Loss: 114250000.0000\n",
      "Epoch [853/1000] | Train Loss: 84128213.4400 | Val Loss: 113152343.6800\n",
      "Epoch [854/1000] | Train Loss: 83703898.6400 | Val Loss: 114948679.6800\n",
      "Epoch [855/1000] | Train Loss: 83020929.8600 | Val Loss: 113812609.9200\n",
      "Epoch [856/1000] | Train Loss: 83560163.3600 | Val Loss: 114043502.0800\n",
      "Epoch [857/1000] | Train Loss: 83181693.4400 | Val Loss: 113702000.6400\n",
      "Epoch [858/1000] | Train Loss: 83166267.1800 | Val Loss: 113917068.8000\n",
      "Epoch [859/1000] | Train Loss: 82686454.0000 | Val Loss: 113497410.5600\n",
      "Epoch [860/1000] | Train Loss: 82712752.1600 | Val Loss: 112932649.6000\n",
      "Epoch [861/1000] | Train Loss: 83435635.3600 | Val Loss: 113019566.0800\n",
      "Epoch [862/1000] | Train Loss: 83357261.9200 | Val Loss: 112885635.8400\n",
      "Epoch [863/1000] | Train Loss: 82574129.8000 | Val Loss: 115287132.8000\n",
      "Epoch [864/1000] | Train Loss: 83141865.0400 | Val Loss: 115496860.1600\n",
      "Epoch [865/1000] | Train Loss: 83672871.3600 | Val Loss: 114796209.9200\n",
      "Epoch [866/1000] | Train Loss: 83294701.8400 | Val Loss: 115105740.1600\n",
      "Epoch [867/1000] | Train Loss: 83466176.9600 | Val Loss: 113178844.8000\n",
      "Epoch [868/1000] | Train Loss: 83226504.0800 | Val Loss: 112590549.1200\n",
      "Epoch [869/1000] | Train Loss: 83291698.8800 | Val Loss: 114622340.4800\n",
      "Epoch [870/1000] | Train Loss: 83005258.7200 | Val Loss: 113500026.2400\n",
      "Epoch [871/1000] | Train Loss: 83421119.9200 | Val Loss: 113318433.2800\n",
      "Epoch [872/1000] | Train Loss: 83130780.4500 | Val Loss: 111703948.8000\n",
      "Epoch [873/1000] | Train Loss: 82798129.6800 | Val Loss: 112258492.8000\n",
      "Epoch [874/1000] | Train Loss: 82887205.7400 | Val Loss: 112580425.6000\n",
      "Epoch [875/1000] | Train Loss: 82873468.7800 | Val Loss: 112507864.3200\n",
      "Epoch [876/1000] | Train Loss: 83237658.0000 | Val Loss: 111795889.9200\n",
      "Epoch [877/1000] | Train Loss: 83077952.1200 | Val Loss: 111646243.2000\n",
      "Epoch [878/1000] | Train Loss: 82537319.5200 | Val Loss: 111756844.8000\n",
      "Epoch [879/1000] | Train Loss: 83384858.2400 | Val Loss: 111982966.4000\n",
      "Epoch [880/1000] | Train Loss: 83295387.4400 | Val Loss: 112487529.6000\n",
      "Epoch [881/1000] | Train Loss: 82912995.9600 | Val Loss: 111874686.0800\n",
      "Epoch [882/1000] | Train Loss: 84607410.0200 | Val Loss: 111472114.5600\n",
      "Epoch [883/1000] | Train Loss: 82982124.1600 | Val Loss: 112224656.0000\n",
      "Epoch [884/1000] | Train Loss: 82882855.5600 | Val Loss: 110095173.7600\n",
      "Epoch [885/1000] | Train Loss: 83425161.6000 | Val Loss: 109982618.8800\n",
      "Epoch [886/1000] | Train Loss: 82208416.0000 | Val Loss: 110933549.4400\n",
      "Epoch [887/1000] | Train Loss: 82848230.2400 | Val Loss: 112944481.9200\n",
      "Epoch [888/1000] | Train Loss: 82741019.8400 | Val Loss: 111055464.3200\n",
      "Epoch [889/1000] | Train Loss: 82731434.4000 | Val Loss: 110205494.4000\n",
      "Epoch [890/1000] | Train Loss: 83100273.6000 | Val Loss: 108803683.8400\n",
      "Epoch [891/1000] | Train Loss: 83591264.0000 | Val Loss: 109945020.8000\n",
      "Epoch [892/1000] | Train Loss: 83372929.9200 | Val Loss: 108088272.6400\n",
      "Epoch [893/1000] | Train Loss: 84369157.4400 | Val Loss: 108129489.9200\n",
      "Epoch [894/1000] | Train Loss: 83207417.5200 | Val Loss: 110201427.8400\n",
      "Epoch [895/1000] | Train Loss: 84168557.2800 | Val Loss: 111111945.6000\n",
      "Epoch [896/1000] | Train Loss: 83267652.5200 | Val Loss: 108903221.1200\n",
      "Epoch [897/1000] | Train Loss: 83494020.1600 | Val Loss: 108993955.8400\n",
      "Epoch [898/1000] | Train Loss: 82934148.7200 | Val Loss: 109118238.0800\n",
      "Epoch [899/1000] | Train Loss: 83966336.7600 | Val Loss: 111943014.4000\n",
      "Epoch [900/1000] | Train Loss: 82897425.6000 | Val Loss: 110272945.9200\n",
      "Epoch [901/1000] | Train Loss: 83325441.6000 | Val Loss: 109220017.2800\n",
      "Epoch [902/1000] | Train Loss: 83653152.3200 | Val Loss: 110318880.6400\n",
      "Epoch [903/1000] | Train Loss: 82372250.4000 | Val Loss: 109480660.4800\n",
      "Epoch [904/1000] | Train Loss: 82462756.6400 | Val Loss: 109172072.9600\n",
      "Epoch [905/1000] | Train Loss: 83079674.2400 | Val Loss: 110289070.7200\n",
      "Epoch [906/1000] | Train Loss: 81965733.4400 | Val Loss: 108458949.1200\n",
      "Epoch [907/1000] | Train Loss: 83544966.1600 | Val Loss: 108960188.1600\n",
      "Epoch [908/1000] | Train Loss: 83658020.3200 | Val Loss: 108627248.0000\n",
      "Epoch [909/1000] | Train Loss: 81941090.4800 | Val Loss: 109647928.3200\n",
      "Epoch [910/1000] | Train Loss: 81534968.7200 | Val Loss: 109612955.5200\n",
      "Epoch [911/1000] | Train Loss: 83391159.5200 | Val Loss: 111369136.6400\n",
      "Epoch [912/1000] | Train Loss: 82283509.0400 | Val Loss: 110516995.2000\n",
      "Epoch [913/1000] | Train Loss: 82154770.2400 | Val Loss: 110167664.0000\n",
      "Epoch [914/1000] | Train Loss: 82952706.5600 | Val Loss: 111480549.1200\n",
      "Epoch [915/1000] | Train Loss: 82448081.9200 | Val Loss: 109980112.0000\n",
      "Epoch [916/1000] | Train Loss: 82557494.4000 | Val Loss: 110587778.5600\n",
      "Epoch [917/1000] | Train Loss: 83673422.8400 | Val Loss: 110580641.9200\n",
      "Epoch [918/1000] | Train Loss: 82327560.0000 | Val Loss: 110231976.3200\n",
      "Epoch [919/1000] | Train Loss: 82275143.5200 | Val Loss: 111250325.7600\n",
      "Epoch [920/1000] | Train Loss: 82749076.3400 | Val Loss: 110453950.0800\n",
      "Epoch [921/1000] | Train Loss: 82686881.9000 | Val Loss: 111481528.9600\n",
      "Epoch [922/1000] | Train Loss: 83479542.1200 | Val Loss: 109816577.9200\n",
      "Epoch [923/1000] | Train Loss: 82281988.8000 | Val Loss: 111150947.8400\n",
      "Epoch [924/1000] | Train Loss: 82103520.2200 | Val Loss: 110287430.4000\n",
      "Epoch [925/1000] | Train Loss: 82106698.8000 | Val Loss: 109721622.4000\n",
      "Epoch [926/1000] | Train Loss: 82130261.7600 | Val Loss: 110183016.9600\n",
      "Epoch [927/1000] | Train Loss: 82889880.7200 | Val Loss: 113243047.0400\n",
      "Epoch [928/1000] | Train Loss: 83238907.2000 | Val Loss: 111475592.9600\n",
      "Epoch [929/1000] | Train Loss: 82987427.5200 | Val Loss: 111225056.0000\n",
      "Epoch [930/1000] | Train Loss: 82505095.2400 | Val Loss: 113118067.8400\n",
      "Epoch [931/1000] | Train Loss: 83252184.8000 | Val Loss: 113165532.8000\n",
      "Epoch [932/1000] | Train Loss: 82131364.7400 | Val Loss: 113020277.1200\n",
      "Epoch [933/1000] | Train Loss: 82421943.2500 | Val Loss: 112848801.9200\n",
      "Epoch [934/1000] | Train Loss: 82787217.7200 | Val Loss: 111549262.7200\n",
      "Epoch [935/1000] | Train Loss: 82833328.6400 | Val Loss: 111562967.6800\n",
      "Epoch [936/1000] | Train Loss: 83227820.4000 | Val Loss: 108711205.7600\n",
      "Epoch [937/1000] | Train Loss: 82235229.6000 | Val Loss: 109313873.2800\n",
      "Epoch [938/1000] | Train Loss: 82635996.8000 | Val Loss: 110802317.4400\n",
      "Epoch [939/1000] | Train Loss: 82959531.0400 | Val Loss: 110550769.9200\n",
      "Epoch [940/1000] | Train Loss: 82518278.9600 | Val Loss: 108727534.0800\n",
      "Epoch [941/1000] | Train Loss: 82541428.1200 | Val Loss: 109129920.0000\n",
      "Epoch [942/1000] | Train Loss: 82715925.7600 | Val Loss: 110497016.3200\n",
      "Epoch [943/1000] | Train Loss: 82125431.2000 | Val Loss: 110188400.0000\n",
      "Epoch [944/1000] | Train Loss: 82824509.2200 | Val Loss: 110888258.5600\n",
      "Epoch [945/1000] | Train Loss: 82495129.0000 | Val Loss: 110044053.1200\n",
      "Epoch [946/1000] | Train Loss: 82501233.3600 | Val Loss: 109028153.6000\n",
      "Epoch [947/1000] | Train Loss: 82381788.3200 | Val Loss: 111165390.0800\n",
      "Epoch [948/1000] | Train Loss: 82534712.9400 | Val Loss: 111206131.2000\n",
      "Epoch [949/1000] | Train Loss: 82776344.9600 | Val Loss: 109636343.0400\n",
      "Epoch [950/1000] | Train Loss: 81678435.2000 | Val Loss: 110090684.8000\n",
      "Epoch [951/1000] | Train Loss: 82057324.6800 | Val Loss: 110078556.1600\n",
      "Epoch [952/1000] | Train Loss: 82360313.7200 | Val Loss: 111297464.3200\n",
      "Epoch [953/1000] | Train Loss: 82108983.2000 | Val Loss: 110286284.1600\n",
      "Epoch [954/1000] | Train Loss: 81623013.7600 | Val Loss: 111981180.8000\n",
      "Epoch [955/1000] | Train Loss: 82738971.3600 | Val Loss: 110515100.1600\n",
      "Epoch [956/1000] | Train Loss: 81832713.9200 | Val Loss: 109959564.8000\n",
      "Epoch [957/1000] | Train Loss: 82485556.1600 | Val Loss: 110455216.0000\n",
      "Epoch [958/1000] | Train Loss: 82210805.8400 | Val Loss: 111444693.7600\n",
      "Epoch [959/1000] | Train Loss: 82006213.1200 | Val Loss: 110514056.3200\n",
      "Epoch [960/1000] | Train Loss: 82640034.2400 | Val Loss: 109073564.1600\n",
      "Epoch [961/1000] | Train Loss: 82282489.1200 | Val Loss: 110203091.2000\n",
      "Epoch [962/1000] | Train Loss: 82234114.8800 | Val Loss: 111807025.9200\n",
      "Epoch [963/1000] | Train Loss: 82481320.5000 | Val Loss: 110886923.5200\n",
      "Epoch [964/1000] | Train Loss: 82566190.9600 | Val Loss: 109581303.6800\n",
      "Epoch [965/1000] | Train Loss: 81289365.9600 | Val Loss: 110425889.9200\n",
      "Epoch [966/1000] | Train Loss: 81864373.4400 | Val Loss: 110747267.2000\n",
      "Epoch [967/1000] | Train Loss: 82008228.5600 | Val Loss: 110820278.4000\n",
      "Epoch [968/1000] | Train Loss: 83003759.9600 | Val Loss: 111035769.6000\n",
      "Epoch [969/1000] | Train Loss: 81418414.0800 | Val Loss: 110912847.3600\n",
      "Epoch [970/1000] | Train Loss: 81929625.4400 | Val Loss: 110575576.9600\n",
      "Epoch [971/1000] | Train Loss: 82275031.0400 | Val Loss: 110777340.8000\n",
      "Epoch [972/1000] | Train Loss: 83349081.1200 | Val Loss: 108736648.9600\n",
      "Epoch [973/1000] | Train Loss: 82376990.3200 | Val Loss: 111077793.2800\n",
      "Epoch [974/1000] | Train Loss: 82526626.6400 | Val Loss: 110104947.8400\n",
      "Epoch [975/1000] | Train Loss: 82006877.6000 | Val Loss: 110030698.2400\n",
      "Epoch [976/1000] | Train Loss: 81047123.2000 | Val Loss: 109686623.3600\n",
      "Epoch [977/1000] | Train Loss: 82120981.1200 | Val Loss: 109498771.8400\n",
      "Epoch [978/1000] | Train Loss: 81862381.4400 | Val Loss: 109448257.9200\n",
      "Epoch [979/1000] | Train Loss: 81639189.2000 | Val Loss: 108001562.2400\n",
      "Epoch [980/1000] | Train Loss: 81779117.8400 | Val Loss: 107830505.6000\n",
      "Epoch [981/1000] | Train Loss: 81951640.8400 | Val Loss: 108816125.4400\n",
      "Epoch [982/1000] | Train Loss: 81512019.9100 | Val Loss: 109359171.2000\n",
      "Epoch [983/1000] | Train Loss: 81810435.6800 | Val Loss: 108914649.6000\n",
      "Epoch [984/1000] | Train Loss: 82315973.6000 | Val Loss: 110900830.0800\n",
      "Epoch [985/1000] | Train Loss: 82066822.4000 | Val Loss: 109337936.6400\n",
      "Epoch [986/1000] | Train Loss: 82008458.5600 | Val Loss: 110694991.3600\n",
      "Epoch [987/1000] | Train Loss: 81622081.2800 | Val Loss: 110005935.3600\n",
      "Epoch [988/1000] | Train Loss: 81367072.7200 | Val Loss: 112254352.6400\n",
      "Epoch [989/1000] | Train Loss: 83200155.9000 | Val Loss: 113706155.5200\n",
      "Epoch [990/1000] | Train Loss: 81728544.9600 | Val Loss: 111121723.5200\n",
      "Epoch [991/1000] | Train Loss: 82017777.6600 | Val Loss: 110412712.3200\n",
      "Epoch [992/1000] | Train Loss: 81930666.0800 | Val Loss: 111017536.0000\n",
      "Epoch [993/1000] | Train Loss: 82110296.6800 | Val Loss: 112156757.7600\n",
      "Epoch [994/1000] | Train Loss: 83001839.8400 | Val Loss: 111691679.3600\n",
      "Epoch [995/1000] | Train Loss: 81067505.4800 | Val Loss: 110364682.2400\n",
      "Epoch [996/1000] | Train Loss: 82393168.9600 | Val Loss: 109485802.2400\n",
      "Epoch [997/1000] | Train Loss: 81691835.0400 | Val Loss: 111186251.5200\n",
      "Epoch [998/1000] | Train Loss: 82421399.2000 | Val Loss: 112597767.6800\n",
      "Epoch [999/1000] | Train Loss: 82446677.1200 | Val Loss: 113024637.4400\n",
      "Epoch [1000/1000] | Train Loss: 81357212.1600 | Val Loss: 111341677.4400\n",
      "../models/best_cnn.pt\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CNNRegressor:\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([1, 57344]) from checkpoint, the shape in current model is torch.Size([1, 114688]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-35a1db75c294>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_save_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mpredictor_cnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cnn\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mpreds_cnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor_cnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/GitHub/Hyperspectral-Regression-Pipeline-for-DON-Prediction/src/deployment/predictor.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_type)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \"\"\"\n\u001b[1;32m     37\u001b[0m         \u001b[0mGenerate\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minput\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNumPy\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mFCN\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m                   \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bands\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mCNN\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mwith\u001b[0m \u001b[0ma\u001b[0m \u001b[0mreshape\u001b[0m \u001b[0mto\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                   \u001b[0;32mor\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mXGBoost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/GitHub/Hyperspectral-Regression-Pipeline-for-DON-Prediction/src/models/cnn_model.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model architecture not initialized. Cannot load state dict.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"state_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2153\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   2154\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   2155\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CNNRegressor:\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([1, 57344]) from checkpoint, the shape in current model is torch.Size([1, 114688])."
     ]
    }
   ],
   "source": [
    "# Model: CNN\n",
    "cnn_params = config[\"model\"][\"cnn\"]\n",
    "cnn_model = CNNModel(cnn_params)\n",
    "cnn_model.train(X_train, y_train, X_val, y_val)\n",
    "\n",
    "cnn_save_path = config[\"deployment\"][\"cnn_model_path\"]\n",
    "cnn_model.save_model(cnn_save_path)\n",
    "\n",
    "predictor_cnn = Predictor(model_type=\"cnn\")\n",
    "preds_cnn = predictor_cnn.predict(X_val)\n",
    "\n",
    "metrics_cnn = evaluate_regression(y_val, preds_cnn, model=\"cnn\", plot=True)\n",
    "results.append([\"CNN\", metrics_cnn[\"MAE\"], metrics_cnn[\"RMSE\"], metrics_cnn[\"R2\"]])\n",
    "\n",
    "print(f\"CNN -> MAE: {metrics_cnn['MAE']:.4f}, RMSE: {metrics_cnn['RMSE']:.4f}, R2: {metrics_cnn['R2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f5c3b1-be01-4795-b545-1d6a3611e2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model: Transformer (placeholder if you have a real training loop)\n",
    "# trans_params = config[\"model\"][\"transformer\"]\n",
    "# trans_model = TransformerModel(trans_params)\n",
    "# trans_model.train(X_train, y_train, X_val, y_val)\n",
    "\n",
    "# trans_save_path = config[\"deployment\"][\"transformer_model_path\"]\n",
    "# trans_model.save_model(trans_save_path)\n",
    "\n",
    "# predictor_trans = Predictor(model_type=\"transformer\", model_path=trans_save_path)\n",
    "# preds_trans = predictor_trans.predict(X_val)\n",
    "\n",
    "# metrics_trans = evaluate_regression(y_val, preds_trans, model=\"transformer\", plot=True)\n",
    "# results.append([\"Transformer\", metrics_trans[\"MAE\"], metrics_trans[\"RMSE\"], metrics_trans[\"R2\"]])\n",
    "\n",
    "# print(f\"Transformer -> MAE: {metrics_trans['MAE']:.4f}, RMSE: {metrics_trans['RMSE']:.4f}, R2: {metrics_trans['R2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ac7faa-661b-42d0-8c7c-95a9cb3585e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results, columns=[\"Model\", \"MAE\", \"RMSE\", \"R2\"])\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6853826b-6756-472d-b252-5ec5cf034811",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.title(\"Comparison of MAE on Validation Set\")\n",
    "\n",
    "bar_x = np.arange(len(results))\n",
    "mae_values = df_results[\"MAE\"].values\n",
    "\n",
    "plt.bar(bar_x, mae_values)\n",
    "plt.xticks(bar_x, df_results[\"Model\"])\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8c6239-4ee2-499d-9919-dec267a13321",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.title(\"Comparison of RMSE on Validation Set\")\n",
    "\n",
    "rmse_values = df_results[\"RMSE\"].values\n",
    "plt.bar(bar_x, rmse_values)\n",
    "plt.xticks(bar_x, df_results[\"Model\"])\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd62e09-c396-4691-b376-148522c33a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.title(\"Comparison of R2 on Validation Set\")\n",
    "\n",
    "r2_values = df_results[\"R2\"].values\n",
    "plt.bar(bar_x, r2_values)\n",
    "plt.xticks(bar_x, df_results[\"Model\"])\n",
    "plt.ylabel(\"R² Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15e75a0-b32f-4a43-9549-49d823e565d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
